\documentclass{article}
\usepackage{minx_math}
\usepackage[top=3.5cm, bottom=3.5cm, left=3.5cm, right=3.5cm]{geometry}
\begin{document}

\title{Recovering Clusters on the Nonparametric Stochastic Block Model}
\author{Min Xu, Varun Jog, Po-Ling Loh, Zongming Ma}
\maketitle

\begin{abstract}
\centering
\noindent 
A graph is known, edges and nodes, \\
communities hide within its folds. \\
Stochastic blocks the model is, \\
weighted edges a novel goal.
\end{abstract}

\tableofcontents


\newpage
\section{Preliminary}

Suppose we have a graph of $n$ nodes and each edge may take on any of $L$ colors. The graph is generated by a stochastic block model with $K$ clusters. We observe a matrix $A$ such that $A_{ij} \sim P$ for node $i,j$ in the same cluster and $A_{ij} \sim Q$ for node $i,j$ in different clusters, where $P,Q$ are some distributions. 

We first consider $P,Q$ discrete. Each within-cluster edge takes on color $l \in \{1,...,L\}$ with probability $P_l$ and each between-cluster edge with probability $Q_l$. We suppose that $P_l, Q_l \rightarrow 0$ so that the graph is sparse. Let $n_k$ be the size of cluster $k$, we will suppose that $\frac{n}{\beta K} \leq n_k \leq \frac{\beta n}{K}$ for some $\beta \geq 1$. \\

The goal is to recover the clustering. We want to estimate $\hat{\sigma} : \{1,...,n\} \rightarrow \{1,...,K\}$ such that $\min_{\tau \in S_K} d_H( \hat{\sigma}, \tau \circ \sigma)$ is small where $S_K$ is the permutation group over $K$ elements.\\

We suppose that the number of colors $L$ is finite and that the probabilities $P_l, Q_l$ are unknown to us. We refer to the setting where $P_l, Q_l$ are known as the \emph{oracle setting}. 

We define the Renyi divergence between $P,Q$ as
\[
I_{tot} = -2 \log \sum_l \sqrt{P_l Q_l}
\]

\textbf{Our goal is to show that all results (i.e., weak consistency rates, strong consistency thresholds) that hold under the oracle setting also hold when the distribution $\{P_l, Q_l\}$ is unknown. }

\section{Weak Recovery Under the Oracle Setting}

\cite{zhangminimax} characterizes the minimax rate of weak recovery for Bernoulli $P,Q$. Their results and proofs can be extended in a straightforward manner to general discrete $P,Q$ under the oracle setting. 

\begin{proposition} 
\label{prop:weak_recovery_oracle}
(Oracle Setting Upper bound)
Assume $\frac{n I_{tot}}{K \log K} \rightarrow \infty$. The maximum likelihood estimator $\hat{\sigma}$ in the oracle setting achieves:
\[
\sup_{\Theta(n, K, \beta, P, Q)} \E r(\hat{\sigma}, \sigma) \leq \left\{ 
    \begin{array}{cc} 
   \exp \left( - (1 + o(1)) \frac{nI_{tot}}{2} \right ), \, & K=2, \\
   \exp \left( - (1 + o(1)) \frac{nI_{tot}}{\beta K} \right ), \,& K\geq 3
  \end{array} \right. 
\]

\end{proposition}

\begin{proof}
Proof of this proposition follows that of Theorem 3.2 in~\cite{zhangminimax}. We describe only the parts that need to be modified. 

Because we have a different likelihood function, our $T(\sigma)$ takes on a different form from that of~\cite{zhangminimax} at the bottom of page 8:
\[
T(\sigma) = \sum_{i<j} \mathbf{1}_{\sigma(i) = \sigma(j)} \sum_{l=0}^L \log \frac{P_l}{Q_l} \mathbf{1}_{A_{ij} = l} 
\]

Let $\sigma_0$ denote the true community assignment. We make a mistake if for some other community assignment $\sigma$, we get $T(\sigma) > T(\sigma_0)$. The key part of the proof is to bound

$$
P_m \equiv P\left(\exists \sigma \,:\,  T(\sigma) > T(\sigma_0),\, d_H(\sigma, \sigma_0) = m \right)
$$

To that end, we bound the probability of error of a fixed $\sigma$ $m$-distant from $\sigma_0$ in Hamming distance. We will prove an analogue of Proposition 5.1 in~\cite{zhangminimax}:

Let $\sigma$ be an arbitrary assignment satisfying $d(\sigma, \sigma_0) = m$. Let $X_i, Y_i$ be random variables such that
\[
X_i = \log \frac{P_l}{Q_l} \trm{ w.p. $P_l$} \qquad 
Y_i = \log \frac{P_l}{Q_l} \trm{ w.p. $Q_l$} 
\]
and $\alpha, \gamma$ be integers where
\[
\alpha = | \{ (i,j) \,:\, \sigma_0(i) = \sigma_0(j) \wedge \sigma(i) \neq \sigma(j) \} | 
\quad
\gamma = | \{ (i,j) \,:\, \sigma_0(i) \neq \sigma_0(j) \wedge \sigma(i) = \sigma(j) \} | 
\]

Then
\begin{align}
P( T(\sigma) \geq T(\sigma_0) ) \leq 
  P\left( \sum_{i=1}^\alpha X_i - \sum_{i=1}^\gamma Y_i < 0 \right) \leq 
  \exp( - \frac{\gamma + \alpha}{2} I) \label{eqn:new_prop51}
\end{align}

Lemma 5.3 from~\cite{zhangminimax} bounds $\alpha, \gamma$ and Proposition 5.2 bounds the number of $\sigma$'s (up to equivalent classes) such that $d_H(\sigma, \sigma_0) = m$. These pieces together bounds $P_m$. The rest of the proof follows~\cite{zhangminimax} exactly starting from Page 16. \\

We devote the rest of the proof toward proving equation~\ref{eqn:new_prop51}.

\begin{align*}
T(\sigma_0) - T(\sigma') = &
   \sum_{i<j} \mathbf{1}_{\sigma_0(i) = \sigma_0(j) \wedge \sigma'(i) \neq \sigma'(j)} \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log \frac{P_l}{Q_l} \\
    &- \sum_{i<j} \mathbf{1}_{\sigma_0(i) \neq \sigma_0(j) \wedge \sigma'(i) = \sigma'(j)} \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log \frac{P_l}{Q_l} \\
 &=  \sum_{i=1}^\alpha X_i - \sum_{i=1}^\gamma Y_i 
\end{align*}


\begin{align*}
P( \sum_{i=1}^\gamma Y_i - \sum_{i=1}^\alpha X_i > 0 ) &\leq 
  \E \left( e^{- t \sum_{i=1}^\alpha X_i} e^{t \sum_{i=1}^\gamma Y_i} \right) \\
 & \leq  \E \left( e^{ - t X_1 \alpha} e^{ t Y_1 \gamma} \right ) \\
 &\leq \left( \E e^{ - t X_i} \E e^{t Y_1} \right)^{(1-w)\alpha + w \gamma} 
       \frac{ \left( \E e^{t Y_1} \right)^{(1-w)(\gamma - \alpha)} }
            { \left( \E e^{-t X_1} \right)^{w (\gamma - \alpha)} }
\end{align*}

We will show that when $t=1/2$ and $w=1/2$, the fraction term equals $1$ and the first term equals $\exp\left( - (1/2 \alpha + 1/2 \gamma) I \right)$. 

Note that 
\begin{align*}
\E e^{ -t X_1} &= \sum_l P_l e^{ - t \log \frac{P_l}{Q_l}} \\
   &= \sum_l P_l \left( \frac{Q_l}{P_l} \right)^t \\ 
   &= \sum_l \sqrt{P_l Q_l} \quad \trm{(if $t=1/2$)}
\end{align*}

\begin{align*}
\E e^{ t Y_1} &= \sum_l Q_l e^{ t \log \frac{P_l}{Q_l} } \\
   &= \sum_l Q_l \left( \frac{P_l}{Q_l} \right)^t \\
   &= \sum_l \sqrt{P_l Q_l} \quad \trm{if $t=1/2$}
\end{align*}

\begin{align*}
P( \sum_{i=1}^\gamma Y_i - \sum_{i=1}^\alpha X_i > 0) &\leq 
    \left( \sum_l \sqrt{P_lQ_l} \right)^{\alpha + \gamma} \\
   &\leq  \exp( - \frac{1}{2} I)^{\alpha + \gamma} \\
   &\leq \exp\left( - \frac{(\alpha+\gamma)}{2}  I \right)
\end{align*}
   

\end{proof}

\section{Weak Recovery in the General Setting}

\begin{proposition}
\label{prop:consistency_possible}
 If weak recovery (consistency) is possible under the oracle setting, then it is possible when $P,Q$ are unknown.
\end{proposition}

\begin{proof}
From proposition~\ref{prop:weak_recovery_oracle}, we know that weak recovery is possible iff $\frac{n I_{tot}}{K \log K} \rightarrow \infty$. 

From Lemma~\ref{lem:simplify_renyi}, we have
\begin{align*}
I_{tot} &= (1+o(1))\sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2 
\end{align*}


Since $I_{tot} = \omega( \frac{K \log K}{n} )$ by hypothesis, it must be that, for some $l$, $(\sqrt{P_l} - \sqrt{Q_l})^2 = \omega( \frac{K \log K}{n} )$. 

We choose such an $l$ and consider an estimator $\hat{\sigma}_l$ that uses only the information $\mathbf{1}_{A_{ij} = l}$. 

Since the Renyi-divergence $I_l$ of $Ber(P_l)$ and $Ber(Q_l)$ is
\[
I_l = (1+o(1)) \left( (\sqrt{P_l} - \sqrt{Q_l})^2 + (\sqrt{1-P_l} - \sqrt{1-Q_l})^2 \right)
\]
We have that $\frac{ n I_l}{K \log K} \rightarrow \infty$ and weak consistency is thus achievable with $\hat{\sigma}_l$. 

\end{proof}

Although weak consistency is achievable with the estimator $\hat{\sigma}_l$ that considers only $\mathbf{1}_{A_{ij} = l}$, the estimator converges at $\exp( - \frac{n I_l}{\beta K})$ and therefore does not converge at the same rate. It is easy to see that $I_l \leq I_{tot} \leq L I_l (1 - o(1))$ where the second inequality holds as equality under some cases. 

\section{Rate Optimal Recovery}

We propose the following algorithm to recover the clusters in the general setting. We proceed in two stages: first, we identify a color $l$ that can provide consistent recovery by itself (such $l$ must exist by proposition~\ref{prop:consistency_possible}), and second, we use $\sigma^l$ -- a clustering based on $l$ -- to estimate $\{ \hat{P}_l, \hat{Q}_l \}$ and then use the estimates to refine the clustering. The second stage closely follows the algorithm from \cite{gao2015achieving}. \\

\textbf{Stage 1. Identify a consistent color}
\begin{enumerate}
\item For each color $l$:
  \begin{enumerate}
   \item Perform spectral clustering on $\tilde{A}_{ij} \equiv \mathbf{1}(A_{ij} = l)$ to get $\sigma^l$.
   \item Estimate $\hat{P}_l, \hat{Q}_l$ via counts from $\sigma^l$. 
   \item Estimate $\sqrt{I_l}$ via 
  $\sqrt{ \hat{I}_l } \equiv \frac{| \hat{P}_l - \hat{Q}_l |}{\sqrt{ \hat{P}_l \vee \hat{Q}_l}}$. 
   \end{enumerate}
\item Output $l^*$ for which $\sqrt{\hat{I}}$ is maximized
\end{enumerate}

 \textbf{Stage 2. Refine clusters}
\begin{enumerate}
\item For each node $u$:
  \begin{enumerate}
  \item Use previously computed $l^*$ to perform spectral clustering on $\mathcal{G}_{-u}$, get $\hat{\sigma}_u$.
   \item Use $\hat{\sigma}_u$ to estimate $\hat{P}_l, \hat{Q}_l$.
   \item Assign $\hat{\sigma}_u(u) = \arg\max_k \sum_{v \,:\, \sigma_u(v) = k} \sum_l 
                  \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) $. 
   \end{enumerate}

\item \textbf{Run consensus.} 
  \subitem Define $\hat{\sigma}(1) = \hat{\sigma}_1(1)$. 
  \subitem For each node $u$, define 
 \[
\hat{\sigma}(u) = \argmax_k 
         \big |  \{ v \,:\, \hat{\sigma}_1(v) = k \} \cap \{ v \,:\, \hat{\sigma}_u(v) = \hat{\sigma}_u(u)  \} \big| 
\]
\item Output $\hat{\sigma}$. 

\end{enumerate}

\begin{proposition}
\label{prop:rate_optimal}
Suppose $K$ is fixed and that $\frac{n I}{L} \rightarrow \infty$ where $L$ is the number of colors. Suppose that $P_l \asymp Q_l$ for all $l$ and $P_l, Q_l = \Omega(1/n)$. Then the above procedure has error rate satisfying
\[
\lim_{n \rightarrow \infty}  \sup_{\sigma_0, \{P_l, Q_l\}} P \left( l(\hat{\sigma}, \sigma_0) \geq \exp\left( - (1 - o(1)) \frac{ n I }{\beta K} \right) \right) = 0
\]
\end{proposition}


In particular, this shows that the threshold behavior (for the symmetric $K=2$ case and finite colors $L$) that \cite{jog2015information} demonstrates hold even if the $P_l, Q_l$'s are not known. 

The rest of the write-up constitute the proof of proposition~\ref{prop:rate_optimal}. The proof proceeds in three steps. In the first step (section~\ref{sec:estimation}, we prove a general result that controls the estimation quality $| \hat{P}_l - P_l |$ and $|\hat{Q}_l - Q_l|$ where $\hat{P}_l, \hat{Q}_l$ are constructed from a consistent clustering algorithm. In the second step (section~\ref{sec:initial_clustering}), we provide guarantee for the first stage of the our proposed algorithm. In the third step (section~\ref{sec:misclassify}), we analyze the second stage of the proposed algorithm and bound the probability of misclassifying a single node.

Justification of the consensus step follows from Lemma 4 from \cite{gao2015achieving} and the proof of theorem~\ref{prop:rate_optimal} follows from the proof of theorem 2 in \cite{gao2015achieving}. 


\section{Estimation}
\label{sec:estimation}

Let $\sigma_0$ be the true clustering and $\hat{\sigma}$ be a clustering algorithm.


\begin{proposition}
\label{prop:estimation_consistency}
Let $\sigma = \hat{\sigma}(G)$ be a clustering of the graph with error rate $\gamma$. That is, $d_H(\sigma, \sigma_0) = \gamma n$. Let $\Delta_l = | P_l - Q_l |$. Suppose that $P_l \vee Q_l \geq \frac{c}{n}$ for all $l$ for some constant $c$. 


Let $\hat{P}_l = \frac{\sum_{u,v \,:\, \sigma(u)=\sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {\sum_{u,v \,:\, \sigma(u) = \sigma(v)} 1}$ and
    $\hat{Q}_l = \frac{\sum_{u,v \,:\, \sigma(u) \neq \sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {\sum_{u,v \,:\, \sigma(u) \neq \sigma(v)} 1}$ be the MLE of $P_l$ and $Q_l$ based on $\sigma$. 

Let $C_{thresh}$ be an absolute constant and let $\delta$ be a positive, fixed, and arbitrarily small real number. Then, for all $l \in L$:

\begin{itemize}
\item[Case 1] If $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq C_{thresh}$, then
\begin{align*}
 | \hat{P}_l - P_l | &\leq \eta \Delta_l \\
 | \hat{Q}_l - Q_l | &\leq \eta \Delta_l 
\end{align*}
\item[Case 2] If $\frac{n \Delta_l^2}{P_l \vee Q_l} \leq C_{thresh}$, then
\begin{align*}
 | \hat{P}_l - P_l | &\leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}} \\
 | \hat{Q}_l - Q_l | &\leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}
\end{align*}
\end{itemize}
with probability at least $1 - L n^{-(3 + \delta)}$, where $\eta = 4 \left( \sqrt{2 C_\delta k \gamma \log \frac{k}{\gamma} } + \frac{C_\delta}{c} k \gamma \log \frac{k}{\gamma} + \gamma k \right)$ is strictly  decreasing with respect to $\gamma k \log k$ and independent of the color $l$. 
\end{proposition}

We note that the sequence $\eta$ is a function of $\gamma k \log k$ and that it does not depend on the color $l$. In other words, so long as $\gamma k \log k \rightarrow 0$, $| \hat{P}_l - P_l |$ and $|\hat{Q}_l - Q_l|$ converge uniformly for all colors; this is important when $L$ is allowed to increase with $n$. 


\subsection{Proof} 

We use a rough clustering $\sigma$ to estimate the parameters. $\sigma$ itself is a random variable dependent on the edge variables but we will analyze a fixed $\sigma$ and then take the union bound.

Let $\sigma_0$ be the true clustering and $\sigma$ be a rough one. 
Suppose that $d_H(\sigma, \sigma_0) = \gamma n$.

There are at most $\binom{n}{\gamma n} k^{\gamma n}$ possible assignments $\sigma$'s that satisfy the distance constraint. 

\[
\log \binom{n}{\gamma n} k^{\gamma n} \leq 
  \log \left( \frac{ n(n-1) ...(n-\gamma n+1) }{(\gamma n)!} \right) + \gamma n \log k \leq
  \log \left( \frac{ n^{\gamma n} e^{\gamma n} }
     { (\gamma n)^{\gamma n} } \frac{1}{\sqrt{2\pi \gamma n}} \right) + \gamma n \log k 
 \leq C_1 \gamma n \log \frac{k}{\gamma} 
\]

for some absolute constant $C_1$. 

\subsubsection{Bias of $\hat{P}_l$}


Our estimator of $P_l$ is 
\[
\hat{P_l} = \frac{ \sum_{i \neq j \,:\, \sigma(i) = \sigma(j)} \mathbf{1}(A_{ij} = l) }{
                   \sum_{i \neq j \,:\, \sigma(i) = \sigma(j)} }
\]

Because $\sigma$ is an imperfect clustering, $\hat{P}_l$ will be biased. In fact, $\E \hat{P}_l$ is a convex combination of $P_l, Q_l$. 
\begin{align}
\E \hat{P_l} &= 
   \frac{ \sum_{i \neq j \,:\, \sigma(i) = \sigma(j)} 
             \mathbf{1}(\sigma_0(i) = \sigma_0(j) ) P_l + 
               \mathbf{1}(\sigma_0(i) \neq \sigma_0(j)) Q_l }{
                   \sum_{i \neq j \,:\, \sigma(i) = \sigma(j)} 1 } \nonumber \\
  &= (1 - \lambda) P_l + \lambda Q_l  = P_l + \lambda (Q_l - P_l) \label{eqn:bias_simple_bound}
\end{align}
for $\lambda = \frac{\sum_{i \neq j \,:\, \sigma(i) = \sigma(j)} 
     \mathbf{1}(\sigma_0(i) \neq \sigma_0(j)) }{\sum_{i \neq j \,:\, \sigma(i) = \sigma(j)} 1}$.

Thus, we have that 
\[
|\E \hat{P}_l - P_l | \leq \lambda |Q_l - P_l|
\]

We need to upper bound $\lambda$. observe that
\begin{align*}
\lambda 
  &= \frac{\sum_{i \neq j \,:\, \sigma(i) = \sigma(j) }\mathbf{1}(\sigma_0(i) \neq \sigma_0(j)) }{\sum_{i\neq j} \mathbf{1}(\sigma(i) = \sigma(j)) } 
      \\
  &= 
   \frac{\sum_k \sum_{i \neq j \,:\, \sigma(i)=\sigma(j)=k} \mathbf{1}(\sigma_0(i) \neq \sigma_0(j))}{\sum_k \hat{n}_k (\hat{n}_k-1)} 
      \\
  &\leq \frac{\sum_k \sum_{i \neq j \,:\, \sigma(i)=\sigma(j)=k} 
       \mathbf{1}( \neg (\sigma_0(i) = \sigma_0(j) =k) )}{\sum_k \hat{n}_k (\hat{n}_k-1)} 
      \\ 
  &\leq \frac{ \sum_k \sum_{i \neq j \,:\, \sigma(i)=\sigma(j)=k} \mathbf{1}(\sigma_0(j)) \neq k) + \mathbf{1}(\sigma_0(i) \neq k)}
            {\sum_k \hat{n}_k (\hat{n}_k - 1)} 
\end{align*}
Define $\gamma_k = \frac{1}{n} \sum_{i \,:\, \sigma(i)=k} \mathbf{1}(\sigma_0(i) \neq k)$ as the error rate within the estimated cluster $k$, and define $\hat{n}_k = \sum_i \mathbf{1}(\sigma(i) = k)$. Then, we have that $\sum_k \gamma_k = \gamma$ and also $\sum_{i \,:\, \sigma(i) = k} \sum_{j \,:\, \sigma(j) = k} \mathbf{1}(\sigma_0(j) \neq k) \leq \gamma_k n \hat{n}_k$. We continue the bound: 
\begin{align*}
\lambda  
  &\leq \frac{ \sum_k 2 \gamma_k n \hat{n}_k }{\sum_k \hat{n}_k(\hat{n}_k - 1)} 
     \\
  &= \frac{n}{\sum_k \hat{n}_k (\hat{n}_k - 1) } \sum_k 2 \gamma_k \hat{n}_k 
     \\
  &\leq \frac{k}{n-k} n \sum_k 2 \gamma_k \frac{\hat{n}_k}{n} \\
  &\leq 4 \gamma k
\end{align*}

In the first inequality, we used the fact that $\sum_k \frac{\hat{n}_k }{n} (\hat{n}_k - 1) = n \sum_k \left( \frac{\hat{n}_k}{n} \right)^2 - 1 \geq \frac{n}{k} - 1$ since $\sum_k \frac{\hat{n}_k}{n} = 1$. In the last inequality, we used the assumption that $k < \frac{n}{2}$. 

We then have an upper bound for $\lambda \leq 4 \gamma k$. Therefore, we have that

\[
|\E \hat{P}_l - P_l | \leq 4 \gamma k \Delta_l
\]

where $\Delta_l  = |Q_l - P_l|$. To simplify presentation, we define $\eta_1 = 4 \gamma k$ so that
\[
|\E \hat{P}_l - P_l | \leq \eta_1 \Delta_l
\]
From this it is clear that $\eta_1$ becomes arbitrarily small if $\gamma$ is made arbitrarily small. 

\subsubsection{Variance of $\hat{P}_l$}

Having handled the bias, we now bound the standard deviation.

Let $ \tilde{A}_{ij} = \mathbf{1}(A_{ij} = l)$. Then, by Bernstein's inequality,
\[
P\left( \left| \sum_{i,j\,:\, \sigma(i) = \sigma(j)} (\tilde{A}_{ij} - \E \tilde{A}_{ij} ) \right|  > t 
 \right) \leq 2 \exp\left( 
    - \frac{t^2}{ 2 \sum_{i,j \, \sigma(i) = \sigma(j)} \E \tilde{A}_{ij}  + \frac{2}{3}t } 
\right)
\]

We first bound $\sum_{i,j \, \sigma(i) = \sigma(j)} \E \tilde{A}_{ij}$:
\begin{align*}
\sum_{i,j \, \sigma(i) = \sigma(j)} \E \tilde{A}_{ij} &=
  \sum_k \hat{n}_k (\hat{n}_k - 1) \E \hat{P}_l \\
 &\leq (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) \quad 
  \trm{(by Equation~\ref{eqn:bias_simple_bound})}
\end{align*}

Therefore,
\[
P\left( \left| \sum_{i,j\,:\, \sigma(i) = \sigma(j)} (\tilde{A}_{ij} - \E \tilde{A}_{ij} ) \right|  > t 
 \right) \leq 2 \exp\left( 
    - \frac{t^2}{ 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1)  + \frac{2}{3}t } 
\right)
\]

We want to bound the probability by $\exp( - C_1 \gamma n \log \frac{k}{\gamma} - (3+\delta) \log n )$. Assuming that $\gamma \geq \frac{1}{n}$, we have that $\gamma \log \gamma^{-1} \geq \frac{1}{n} \log n$ and thus $C_1 \gamma n \log \frac{k}{\gamma} + (3 + \delta) \log n \leq C_\delta \gamma n \log \frac{k}{\gamma}$. 

We choose $t$ such that
\begin{align*}
t^2 &=4 \left\{  2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) \left( 
    C_1 \gamma n \log \frac{k}{\gamma} + (3+\delta) \log n \right) \vee 
   \left( C_1 \gamma n \log \frac{k}{\gamma} + (3 + \delta) \log n \right)^2 \right\} \\
 &\leq 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) C_{\delta} \gamma n \log \frac{k}{\gamma} \vee \left( C_\delta \gamma n \log \frac{k}{\gamma} \right)^2  \\
 &\leq \left( \sqrt{  2 (P_l \vee Q_l)  \sum_k \hat{n}_k (\hat{n}_k - 1) C_\delta \gamma n \log \frac{k}{\gamma} } + C_\delta \gamma n \log \frac{k}{\gamma} \right)^2
\end{align*}

We now verify that regardless of which term among $\{ 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) ,\, C_1 \gamma n \log \frac{k}{\gamma} + (3 + \delta)\log n \}$ is larger, the probability term is at most $2 \exp( - C_1 \gamma n \log \frac{k}{\gamma} - (3+\delta) \log n )$. Let $A = 2(P_l \vee Q_l)\sum_k \hat{n}_k (\hat{n}_k - 1)$ and $B = C_1 \gamma n \log \frac{k}{\gamma} + (3 + \delta) \log n$. 

\begin{itemize}
\item Suppose $A \geq B$, then $t^2 = 4 AB$ and the probability term is $2 \ exp \left( - \frac{4 AB}{A + \frac{4}{3} \sqrt{AB}} \right)  \leq 2 \exp( - B )$. 
\item Suppose $A \leq B$, then $t^2 = 4 B^2$ and the probability term is upper bounded by $2 \exp( - A ) \leq 2 \exp( - B)$. 
\end{itemize}

Thus, with probability at most $2\exp( - C_1 \gamma n \log \frac{k}{\gamma} - (3+\delta) \log n $:

\begin{align*}
| \hat{P}_l - \E \hat{P}_l | =
\frac{\sum_{i,j \, \sigma(i) = \sigma(j)} (\tilde{A}_{ij} - \E \tilde{A}_{ij} ) }{
  \sum_{i,j} \mathbf{1}( \sigma(i) = \sigma(j) ) } &> 
  \frac{t}{\sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j)) } \\
\end{align*}

Substituting in the previous bound we had of $t$:

\begin{align*}
 \frac{t}{\sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j)) } &\leq
  2 \frac{  \sqrt{  2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) C_\delta \gamma n \log \frac{k}{\gamma} } + C_\delta \gamma n \log \frac{k}{\gamma} }
        { \sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j) ) }  \\
  &\leq 2 \frac{\sqrt{2 (P_l \vee Q_l) C_\delta \gamma n \log \frac{k}{\gamma}}}
             {\sqrt{ \sum_k \hat{n}_k (\hat{n}_k - 1)}} + 
        2  \frac{C_\delta \gamma n \log \frac{k}{\gamma}}
             {\sum_k \hat{n}_k (\hat{n}_k - 1)} \\
 &\leq  2 \frac{ \sqrt{2 P_l\vee Q_l} \sqrt{ C_\delta \gamma k \log \frac{k}{\gamma}} }
           {n - k} + 
       2 \frac{C_\delta \gamma k \log \frac{1}{\delta}}{n - k} \\
 &\leq 4 \sqrt{ \frac{2 P_l \vee Q_l}{n} } \sqrt{C_\delta k \gamma \log \frac{k}{\gamma}} + 
       4 \frac{C_\delta k \gamma \log \frac{k}{\gamma}}{n} 
\end{align*}
For the second to last inequality, we used the fact that $\sum_k (\hat{n}_k - 1) \frac{\hat{n}_k}{n} = n \sum_k \left( \frac{\hat{n}_k}{n} \right)^2 - 1 \geq \frac{n}{k} - 1 $ because $\frac{\hat{n}_k}{n}$ sums to 1. In the last inequality, we used the assumption that $n-k \geq \frac{n}{2}$. 

To further simplify the expression, we have that $P_l \vee Q_l \geq \frac{c}{n}$ and thus, $\sqrt{ \frac{P_l \vee Q_l}{n} } \geq \frac{c}{n}$. Therefore, we have that, with probability at least $1 - \exp( -C_1 \gamma n \log \frac{k}{\gamma} - (3 + \delta) \log n)$, 
\begin{align}
| \hat{P}_l - \E \hat{P}_l | \leq \sqrt{ \frac{P_l \vee Q_l}{n} } \eta_2 \label{eqn:variance_bound}
\end{align}
where $\eta_2 = 4 \left( \sqrt{2 C_\delta k \gamma \log \frac{k}{\gamma}} 
      + \frac{C_\delta}{c} k \gamma \log \frac{k}{\gamma} \right)$. It is clear that $\eta_2$ can be made arbitrarily small by taking $\gamma k \log k$ to be arbitrarily small. 

 Taking the union bound across all clusterings with error $\gamma$ and across all colors, we have that the probability of (\ref{eqn:variance_bound}) holding simultaneously for all colors $l$ is at least $1 - L n^{-(3+\delta)}$. 

\subsubsection{Combining Bias and Variance}

From the bias and variance analysis, we have that 

\begin{align*}
| \E \hat{P}_l - P_l | &\leq \eta_1 \Delta_l \\
| \hat{P}_l - \E \hat{P}_l | &\leq 
   \sqrt{ \frac{P_l \vee Q_l}{n} } \eta_2
\end{align*}

In Case 1, $\Delta_l \geq C_{thresh} \sqrt{ \frac{P_l \vee Q_l}{n}} $ and thus, 
\[
| \hat{P}_l - P_l| \leq \eta \Delta_l
\]

In Case 2, $\Delta_l \leq C_{thresh} \sqrt{ \frac{P_l \vee Q_l}{n}}$ and thus,
\[
| \hat{P}_l - P_l| \leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}
\]
where $\eta = \eta_1 + \eta_2$.



\subsection{A Lower Bound on the Estimated Probabilities}

The following lower bound will be useful.

\begin{proposition}
\label{prop:prob_estimate_lower_bound}
Let $\sigma = \hat{\sigma}(G)$ be a clustering of the graph. Suppose that $P_l, Q_l \geq \frac{c}{n}$ for all $l$ for some constant $c$ and that $c_1 \leq \frac{P}{Q} \leq c_2$. 

      
Let $\hat{P}_l = \frac{\sum_{u \neq v \,:\, \sigma(u)=\sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {\sum_{u \neq v \,:\, \sigma(u) = \sigma(v)} 1}$ and
    $\hat{Q}_l = \frac{\sum_{u \neq v \,:\, \sigma(u) \neq \sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {\sum_{u \neq v \,:\, \sigma(u) \neq \sigma(v)} 1}$ be the MLE of $P_l$ and $Q_l$ based on $\sigma$. 

Then, we have that, with probability at least $1 - L \exp(- C_{c_1, c_2, c} n)$, for all colors $l$,
\[
\hat{P}_l \vee \hat{Q}_l \geq \frac{ P_l \wedge Q_l}{2}   
\]
\end{proposition}

\begin{proof}

First, we define
\[
\hat{R}_l = \frac{\sum_{u \neq v} \mathbf{1}(A_{uv} = l)}{n(n-1)}
\]
Two points are clear: first, that $\hat{R}_l$ does not depend on $\sigma$ and second, $\hat{R}_l = \lambda_\sigma \hat{P}_l + (1- \lambda_\sigma) \hat{Q}_l$ for $\lambda_\sigma = \frac{\sum_{u \neq v \,:\, \sigma(u) = \sigma(v)}}{n(n-1)}$, thus, under the event that $\hat{R}_l \geq \frac{P_l \wedge Q_l}{2} $, it must be that $\hat{P}_l \vee \hat{Q}_l \geq \frac{P_l \wedge Q_l}{2}$. 

$\hat{R}_l$ is a sum of $n (n-1)$ independent Bernoulli random variables. $\frac{1}{n(n-1)} \sum_{u\neq v} \E \mathbf{1}(A_{uv} = l) = \lambda P_l + (1 - \lambda) Q_l \geq P_l \wedge Q_l$, where $\lambda = \frac{\sum_{u \neq v \,:\, \sigma_0(u) = \sigma_0(v)}}{n(n-1)}$. 

Also, $\frac{1}{n(n-1)} \sum_{u\neq v} \var( \mathbf{1}(A_{uv}= l) ) = \lambda P_l (1-P_l) + (1-\lambda) Q_l (1-Q_l) \leq P_l \vee Q_l$.  

Now we apply Bernstein's inequality.
\begin{align*}
P\left( | \hat{R}_l - \E \hat{R}_l | > \frac{P_l \wedge Q_l}{2} \right) &\leq
  \exp\left( - \frac{ (\frac{P_l \wedge Q_l}{2} )^2 n(n-1)}
                 {P_l \vee Q_l} \right) \\
  &\leq \exp\left( - (c_1 \wedge \frac{1}{c_2}) \frac{c}{4} (n-1) \right) 
\end{align*}

The second inequality follows from the fact that $\frac{P_l \wedge Q_l}{P_l \vee Q_l} \geq (c_1 \wedge \frac{1}{c_2}) $ and that $n(P_l \wedge Q_l) \geq c$. 

Now, we note that
\begin{align*}
P\left( \hat{R}_l \leq \frac{P_l \wedge Q_l}{2} \right) &=
  P\left( \E\hat{R}_l - \hat{R}_l \geq \E\hat{R}_l - \frac{P_l \wedge Q_l}{2} \right) \\
 &\leq P \left( | \E\hat{R}_l - \hat{R}_l | \geq \E\hat{R}_l - \frac{P_l \wedge Q_l}{2} \right) \\
 &\leq P \left( | \E\hat{R}_l - \hat{R}_l | \geq \frac{P_l \wedge Q_l}{2} \right)
\end{align*}
To finish the proof, we need only take an union bound across all colors $L$. 

\end{proof}


\newpage
\section{Controlling probability of misclassification}
\label{sec:misclassify}
Now that we have control over $\hat{P}_l$, we study the effect of plugging in these estimates into the refinement stage.

Let $\sigma_u$ be a clustering for all the nodes except $u$. Suppose that $d_H(\sigma_u, \sigma_0) = \gamma n$. 


We assign $u$ based on the criterion:
\[
\argmax_k \sum_{v\, \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
\]

\begin{proposition}
Suppose that $\sigma_u$ is a clustering of all nodes except for $u$ with error rate $\gamma$, that is, $d_H(\sigma_u, \sigma_0) = \gamma n$. Let $C_{thresh}$ be an absolute constant and suppose that statements of proposition~\ref{prop:estimation_consistency} holds. 

Suppose that $\frac{n I^*}{L} \rightarrow \infty$ and that for all $l$, $P_l \asymp Q_l$.  Suppose also that $P_l, Q_l \leq 1 - \epsilon$ for some fixed arbitrarily small $0 < \epsilon < 1$. 

Then, we have that, with probability at least $1 - \exp \left( - (1 - o(1)) \frac{n}{K} I^* \right)$, 
\[
\sigma_0(u) = \argmax_k \sum_{v\, \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
\]

\end{proposition}




\subsection{Proof}

Throughout the proof, we let $\eta, \eta'$ denote a sequence that converges to 0 and let $C$ denote a constant. Their value could change from line to line. \\

First, define $L_1 = \{ l \,:\, n \frac{\Delta_l^2}{P_l \vee Q_l} \geq C_{thresh} \}$. Then we claim that $C_{\epsilon, c_1, c_2} \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} = I^*( 1 - \eta )$. To see this, observe first that

\begin{align*}
I^* &= -2 \log \sum_l \sqrt{P_l Q_l} \\
  &= C_\epsilon \sum_l (\sqrt{P_l} - \sqrt{Q_l} )^2 \\
  &= C_\epsilon \sum_l \frac{\Delta_l^2}{(\sqrt{P_l} + \sqrt{Q_l})^2} \\
  &= C_{\epsilon, c_1, c_2} \sum_l \frac{\Delta_l^2}{P_l \vee Q_l} 
\end{align*}

Therefore, we have that
\begin{align*}
C_{\epsilon, c_1, c_2} \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} &= I^* - 
 C_{\epsilon, c_1, c_2} \sum_{l \notin L_1} \frac{\Delta_l^2}{P_l \vee Q_l} \\
 &\geq I^* - C_{\epsilon, c_1, c_2} \sum_{l \notin L_1} \frac{C_{thresh}}{n} \\
 &\geq I^* - C_{\epsilon, c_1, c_2} \frac{ L C_{thresh}}{n} \\
 &\geq I^* - \eta I^* 
\end{align*}

The second inequality follows from the definition of $L_1$. The third inequality follows because $\frac{n I^*}{L} \rightarrow \infty$ by assumption. \\


Now we proceed onto the main proof. Suppose without the loss of generality that $\sigma_0(u) = 1$.  We want to then control the probability that for some cluster $k$, 
\begin{align}
\sum_{v\,:\, \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
&\geq 
 \sum_{v\,:\, \sigma_u(v)=1} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
  \quad \trm{(iff)}  \nonumber \\
\sum_{v \,:\, \sigma_u(v) = k} \bar{A}_{uv} - \sum_{v\,:\, \sigma_u(v) = 1} \bar{A}_{uv} 
\label{eqn:bad_event1}
&\geq 0 
\end{align}
where $\bar{A}_{uv} \equiv \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l)$.

Define $m_1 = |\{ v \,:\, \sigma_u(v) = 1 \}|$ and $m_k = | \{ v \,:\, \sigma_u(v) = k \}|$ as the size of clusters $m_1, m_k$ under $\sigma_u$. Define $m_k' = \{ v \,:\, \sigma_u(v) = k,\, \sigma_0(v) = k \}$, $m_1' = \{ v \,:\, \sigma_u(v) = 1 ,\, \sigma_0(v) = 1\}$ as the points correctly clustered by $\sigma_u$. 

With these definitions, the probability of the bad event Equation~\ref{eqn:bad_event1} is upper bounded by the probability of the following:

\begin{align*}
\left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m'_k} \tilde{X}_i \right) - 
\left( \sum_{i=1}^{m_1'} \tilde{X}_i + \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) &\geq 0  \quad \trm{(iff)} \\
\exp( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i - 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) ) &\geq 1 
\end{align*}

 where $\tilde{X}_i = \log \frac{\hat{P}_l}{\hat{Q}_l}$ with probability $P_l$ and $\tilde{Y}_i = \log \frac{\hat{P}_l}{\hat{Q}_l}$ with probability $Q_l$. 



\begin{align*}
& P \left( \exp( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i- 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) ) \geq 1 \right) \\ 
&\leq \E \left( 
\exp( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i - 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) )
 \right) \\ 
&\leq \left( \E \exp( t \tilde{Y}_i ) \right)^{m_k'} 
      \left( \E \exp(t \tilde{X}_i ) \right)^{m_k - m_k'}  
    \left( \E \exp( - t \tilde{X}_i) \right)^{m_1'} 
    \left( \E \exp( -t \tilde{Y}_i ) \right)^{m_1 - m_1'} \\
&\leq \left( \sum_l e^{t \log \frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k'}  
      \left( \sum_l e^{t \log \frac{\hat{P}_l}{\hat{Q}_l}} P_l \right)^{m_k - m_k'} 
      \left( \sum_l e^{- t \log \frac{\hat{P}_l}{\hat{Q_l}} } P_l \right)^{m_1'}
     \left( \sum_l e^{-t \log \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{m_1 - m_1'}
\end{align*}

We will set $t = \frac{1}{2}$, in which case, we have:
\begin{align}
=& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k'}
 \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l \right)^{m_k - m_k'}
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l \right)^{m_1 - m_1'}
       \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1'} \nonumber \\
=&  \left( \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right)^{m_k - m_k'}
 \left( \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right)^{m_1 - m_1'}  
   \label{eqn:excess_error_term} \\
 & \left( \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{m_k} 
    \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1}
   \label{eqn:Ihat_error_term} 
\end{align} 

We will bound term~\ref{eqn:excess_error_term} and \ref{eqn:Ihat_error_term} separately. Loosely speaking, we will show that term~\ref{eqn:excess_error_term} is bounded in magnitude by $\exp( o(I^*) \frac{n}{k} )$ and that term~\ref{eqn:Ihat_error_term} is bounded by $\exp( - \frac{n}{k} (1 + o(1) I^*)$. 


\textbf{Bound for Term~\ref{eqn:excess_error_term}.} 

Now, we can bound term~\ref{eqn:excess_error_term}:
\begin{align*}
\left| 1 -  \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right|
 &= \left| \frac{ \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} (P_l - Q_l) }
     { \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l } \right| \\
&\leq \frac{8}{\sum_l \sqrt{P_l Q_l}} 
     \left| \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} }(P_l - Q_l) \right| \\
&\leq 16 \left|  \sum_{l} \left( \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right) (P_l - Q_l)  \right| \\
&\leq 16 \left| 
     \sum_{l \in L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} - 1 \right)(P_l - Q_l) 
     \right| + \sum_{l \notin L_1} \left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} - 1 \right| \Delta_l \\
&\leq 16 \sum_{l \in L_1} \frac{\Delta^2_l}{Q_l}(1+ \eta') + 
      \sum_{i \notin L_1} 2 C_{thresh} \frac{\Delta_l}{\sqrt{n (P_l \vee Q_l)}} \\
&\leq C_{\epsilon, c_1, c_2} I^* (1 + \eta') + 2 C_{thresh}^2 \frac{L}{n} \\
&\leq C_{\epsilon, c_1, c_2} I^* (1 + \eta')
\end{align*}


Where the second inequality follows from lemma~\ref{lem:sqrt_ratio_times_ql},
second to last inequality follows under the assumption that $\sum_l \sqrt{P_l Q_l} \geq \frac{1}{2}$, and the last inequality follows from Lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1}. 

Identical analysis shows that
\[
\left| 1 - \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right| 
= O(I^*) 
\]

Now, we note that $\exp( | 1 - x | ) \geq |x|$. 
Therefore, term~\ref{eqn:excess_error_term} can be bounded as
\begin{align*}
&  \left( \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right)^{m_k - m_k'}
 \left( \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right)^{m_1 - m_1'}  \\
&\leq \exp( O(I^*) (m_k - m_k' + m_1 - m_1') ) \\
&\leq \exp( O(I^*) \gamma n) \\
&\leq \exp\left( \frac{n}{k} o(I^*) \right) \quad \trm{(since $\gamma k \rightarrow 0$)}
\end{align*}

\textbf{Bound for Term~\ref{eqn:Ihat_error_term}.}


Define 
$\hat{I} = - \log \left( \sum_l \frac{\hat{P}_l}{\hat{Q}_l} Q_l \right) \left( \sum_l \frac{\hat{Q}_l}{\hat{P}_l} P_l \right) $. 
With this definition, 

\begin{align*}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k} 
       \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1} \\
&= \exp( - \hat{I} )^{\frac{m_k + m_1}{2}}  \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} 
\end{align*}

We claim that the following three statements are true. 
\begin{enumerate}
\item[Claim 1] $m_k \geq n_1 - 2 \gamma n$ and likewise for $m_1$.
\item[Claim 2] $\hat{I} - I^* \geq - o(1) I^*$
\item[Claim 3] $\left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} = \exp(\frac{n}{k}  o(I^*)  $
\end{enumerate}

Let us first suppose that these statements are true and see that term~\ref{eqn:Ihat_error_term} can be bounded. 


\begin{align*}
& \exp( - \hat{I} )^{\frac{m_1 + m_k}{2}}  \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}}  \\
&\leq  \exp( - (I^* + (\hat{I} - I^*) )^{\frac{m_1 + m_k}{2}}  
 \left( \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l}
             {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l} \right)^{\frac{m_1 - m_k}{2}}  
  \\
&\leq \exp \left( - (1-o(1)) I^* (n_1 - \gamma n) \right) 
   \left( \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l}
             {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l} \right)^{\frac{m_1 - m_k}{2}}  
   \quad \trm{(by claim 1 and 2)}\\
&\leq \exp \left( - (1-o(1)) \frac{n}{\beta k} I^*  \right) 
   \quad \trm{(by claim 3)}
\end{align*}

The last inequality holds because $\gamma = o\left( \frac{1}{k \log k} \right)$. We will prove each of the three claims in the remainder of the proof.

\textbf{Claim 1:} This is straightforward. $\sigma_u$ has at most $\gamma n$ errors and therefore, $m_1' \geq n_1 - \gamma n$ and $m_1 - m_1' \leq \gamma n$. 

\textbf{Claim 2:} We show that the estimation error of $\hat{P}_l, \hat{Q}_l$ does not make $\hat{I}$ too small.

\begin{align}
\hat{I} - I^* &= - \log \frac{ 
     \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)
     \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)}{ 
          \left( \sum_l \sqrt{P_l Q_l} \right)^2 } \label{eqn:Ihat_Istar}
\end{align}

Let us consider the numerator.
\begin{align*}
& \left( \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)
\left( \sum_l \sqrt{ \frac{\hat{Q}_l}{\hat{P}_l}} P_l \right) \\
&= \left( \sum_l \sqrt{ P_l Q_l} \sqrt{ \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l}} \right) 
     \left( \sum_l \sqrt{P_l Q_l} \sqrt{ \frac{P_l}{\hat{P}_l} \frac{\hat{Q}_l}{ Q_l}} \right) \\
&= \sum_l P_l Q_l + 2\sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} + 
   \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
&= \left( \sum_l \sqrt{P_l Q_l} \right)^2 + \sum_{l < l'} 
                  \sqrt{P_l Q_l P_{l'} Q_{l'}} \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) 
\end{align*}

where we define $T_{l,l'} = \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} 
      \frac{P_{l'}}{\hat{P}_{l'}} \frac{\hat{Q}_{l'}}{Q_{l'}}  $. It will be later shown that $T_{l,l'} \rightarrow 1$ and thus, continuing equation~\ref{eqn:Ihat_Istar},

\begin{align}
\hat{I} - I^* &= - \log \left( 1 + \frac{ \sum_{l<l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right)}
    { \left( \sum_l \sqrt{ P_l Q_l} \right)^2 }  \right) \nonumber \\
     &  \geq  - \log \left( 1 + 4 \sum_{l<l'} \sqrt{P_l Q_l P_{l'} Q_{l'}}  
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right)  \right) 
  \quad \trm{(assuming that $\sum_l \sqrt{P_l Q_l} \geq 1/2$)} \nonumber \\
   & \geq - 4 \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \label{eqn:Ihat_Istar2}
\end{align}

We proceed by first bounding $|T_{l,l'} - 1|$ and then taking the second order approximation of $\left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right)$ around $1$. 

\begin{align*}
|T_{l,l'} - 1| &= \left| \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} 
      \frac{P_{l'}}{\hat{P}_{l'}} \frac{\hat{Q}_{l'}}{Q_{l'}} - 1 \right| \\
 &= \left| \left( 1 - \frac{P_l - \hat{P}_l}{P_l} \right)
    \left( 1 - \frac{\hat{Q}_l - Q_l}{\hat{Q}_l} \right)
   \left( 1- \frac{\hat{P}_{l'} - P_{l'}}{\hat{P}_{l'}}\right)
   \left( 1 -  \frac{Q_{l'}- \hat{Q}_{l'}}{Q_{l'}} \right) -1 \right| \\
&\leq \left( \frac{|P_l - \hat{P}_l|}{P_l} +  \frac{|\hat{Q}_l - Q_l|}{\hat{Q}_l}
           +   \frac{| \hat{P}_{l'} - P_{l'}|}{\hat{P}_{l'}} +
               \frac{| Q_{l'} - \hat{Q}_{l'} | }{Q_{l'}} \right) \\
& \leq 2\left( \frac{|P_l - \hat{P}_l|}{P_l} +  2\frac{|\hat{Q}_l - Q_l|}{Q_l}
           +   \frac{| \hat{P}_{l'} - P_{l'}|}{P_{l'}} +
               \frac{| Q_{l'} - \hat{Q}_{l'} | }{Q_{l'}} \right) 
\end{align*}
where the last inequality follows from lemma~\ref{lem:bound_ratio_P_Pl}.

Since we only work with pairs $(l, l')$ such that $l' > l$ and we can choose whatever ordering we would like. Suppose that the $l$'s are in decreasing order of $\frac{|\hat{P}_l - P_l|}{P_l} + \frac{|\hat{Q}_l - Q_l|}{Q_l}$ and therefore, we have that, for all pairs $l < l'$, 
\[
| T_{l,l'} - 1 | \leq 2 
    \left( \frac{|\hat{P}_l - P_l|}{P_l} + \frac{|\hat{Q}_l - Q_l|}{Q_l} \right)
\]

By proposition~\ref{prop:estimation_consistency}, we have that, for $l \in L_1$, $\frac{|P_l - \hat{P}_l|}{P_l} \leq \eta \frac{\Delta_l}{P_l \vee Q_l}$ and for $l \notin L_1$, $\frac{|P_l - \hat{P}_l|}{P_l} \leq \eta \frac{1}{\sqrt{n (P_l \vee Q_l)}}$ and likewise for the $\frac{|\hat{Q}_l - Q_l|}{Q_l}$ term. We plug these bounds into the previous derivation and get that:
\begin{align*}
|T_{l,l'} - 1|  &\leq \eta  \frac{\Delta_l}{P_l \vee Q_l}  \quad \trm{for $l \in L_1$}\\
|T_{l,l'} - 1 | &\leq \eta \frac{1}{\sqrt{ n (P_l \vee Q_l)}} \quad \trm{for $l \notin L_1$}
\end{align*}
We have used the assumption that $P_l \asymp Q_l$ and folded constants into the sequence $\eta$. 


The Taylor approximation of $\sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2$ around $T_{l,l'}=1$ is:
\begin{align*}
\sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} -2  &\leq 
  \frac{1}{4} (T_{l,l'} - 1)^2 + O (T_{l,l'}-1)^3 
\end{align*}

Continuing on from equation~\ref{eqn:Ihat_Istar2}, we have that
\begin{align*}
\hat{I} - I^* &\geq - 4 \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
&\geq - 4 \sum_{l \in L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) 
     - 4 \sum_{l \notin L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
  &\geq - \sum_{l \in L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
             \eta \left( \frac{\Delta_l}{P_l \vee Q_l}  \right)^2 
        - \sum_{l \notin L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
             \eta \frac{1}{n (P_l \vee Q_l)} \\
 &\geq - \eta \left( \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} \right)
         \left( \sum_{l'}  \sqrt{P_{l'}Q_{l'}} \right) 
       - \eta \left( \sum_{l \notin L_1} \frac{1}{n} \right) 
          \left( \sum_{l'} \sqrt{P_{l'} Q_{l'} } \right) \\
 &\geq - o(I^*)
\end{align*}

The last inequality follows because $\sum_{l'} \sqrt{P_{l'} Q_{l'}} \leq 1$ and because $\sum_{l \notin L_1} \frac{1}{n} \leq \frac{L}{n} = o(I^*)$. This proves claim 2.

\textbf{Claim 3.} 

\begin{align*}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} \\
&= \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} 
   \left( \frac{\sum_l \sqrt{\hat{P}_l \hat{Q}_l}}{\sum_l \sqrt{\hat{P}_l \hat{Q}_l}} \right)^{\frac{m_1 - m_k}{2}} \\
&=  \left( 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} \hat{P_l} } \right)^{\frac{m_1 - m_k}{2}} 
\end{align*}

Assume that $m_k \geq m_1$. The reverse case can be analyzed in the identical manner. Then,
\begin{align*}
&= \left( 1 + 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l)}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( 1+ \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} (\hat{P}_l - P_l)}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l } \right)^{\frac{m_k - m_1}{2}} 
   \\
\end{align*}

By lemma~\ref{lem:sqrt_ratio_times_ql}, the denominators are of constant order. That is, 
$\sum_l \sqrt{ \frac{ \hat{P}_l }{ \hat{Q}_l } \hat{Q}_l } = C$ and 
$\sum_l \sqrt{ \frac{\hat{Q}_l}{P_l} } P_l = C$. 

To bound the numerator term, we apply lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1}. 

\begin{align*}
\left| \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l) \right|  &= 
  \left|  \sum_l \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) 
 \right| \\
& \leq \left| \sum_{l \in L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) \right| +  %next term
  \left| \sum_{l \notin L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) \right| \\
& \leq \sum_{l \in L_1} \eta \frac{\Delta_l^2}{Q_l} + \sum_{l \notin L_1} \eta \frac{1}{n} \\
& \leq \eta I^* + \eta \frac{L}{n}  \\
& \leq \eta I^*  
\end{align*}

The second inequality follows from lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1} and the definition of $L_1$. 

\begin{align*}
& \left( 1 + 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l)}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( 1+ \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} (\hat{P}_l - P_l)}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l } \right)^{\frac{m_k - m_1}{2}} 
\\
&\leq \exp\left( (m_k - m_1) \log(1 + o(I^*) ) \right) \\
&\leq \exp \left( \frac{n}{k} o(I^*) \right) 
\end{align*}

This proves claim 3. 

Multiplying the bounds for term~\ref{eqn:Ihat_error_term} and \ref{eqn:excess_error_term} completes the proof. 



\subsection{Lemmas}

Here we collect various lemmas used in the proof.

We often use the bound that $\frac{1}{2} P \leq \hat{P}_l \leq 2 P_l$. The following lemma justifies this.

\begin{lemma}
\label{lem:bound_ratio_P_Pl}
Let $\eta$ be a sequence that tends to zero. Let $l$ be any color and suppose that $P_l \asymp Q_l$ and that $P_l, Q_l \geq \frac{c}{n}$ for some absolute constant $c$. Suppose either that $|\hat{P}_l - P_l| = \eta \Delta_l$ or that $| \hat{P}_l - P_l | = \eta \sqrt{ \frac{P_l \vee Q_l}{n} }$. 

Then, for all small enough $\eta$, we have that
\[
\frac{1}{2} P \leq \hat{P}_l \leq 2 P_l
\]

\end{lemma}

\begin{proof}
Under the first condition, one need only observe that there exists an absolute constant $C$ for which $\Delta_l \leq C P_l$. Under the second condition, we use the assumption that $P_l \geq \frac{C}{n}$ for some absolute constant $C$. 

\end{proof}

The following lemma bounds the quantity $\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} - 1$ for either the case that $\Delta_l \geq C_{thresh} \sqrt{ \frac{P_l \vee Q_l}{n} }$ or else. 

\begin{lemma}
\label{lem:sqrt_ratio_pl_ql_minus_1}
Let $\eta$ be a sequence that tends to 0. Fix a color $l$ and assume that $P_l \asymp Q_l$ and that $P_l, Q_l \geq \frac{c}{n}$ for some absolute constant $c$. 
\begin{enumerate}
\item 
Suppose $\hat{P}_l$ and $\hat{Q}_l$ satisfies $n \frac{\Delta_l^2}{P_l \vee Q_l} \geq C_{thresh}$ and therefore
\begin{align*}
| \hat{P}_l - P_l | &= \eta \Delta_l  \\
| \hat{Q}_l - Q_l | &= \eta \Delta_l  
\end{align*}

Then, we have that 
\[
\sqrt{ \frac{\hat{P}_l }{\hat{Q}_l} } - 1 \left\{ \begin{array}{cc} 
                    \leq \frac{P_l - Q_l}{Q_l} (1 + \eta') & (\trm{if }  P_l \geq Q_l) \\
                    \geq \frac{P_l - Q_l}{Q_l} (1 + \eta') & (\trm{if } P_l < Q_l) 
           \end{array} \right.
\]
where $\eta' \rightarrow 0$ and does not depend on the color $l$. 

\item
Suppose $\hat{P}_l$ and $\hat{Q}_l$ satisfies $n \frac{\Delta_l^2}{P_l \vee Q_l} \leq C_{thresh}$ and therefore
\begin{align*}
| \hat{P}_l - P_l | &= \eta \sqrt{ \frac{P_l \vee Q_l}{n} } \\
| \hat{Q}_l - Q_l | &= \eta \sqrt{ \frac{P_l \vee Q_l}{n} }
\end{align*}
Then, we have that
\[
\left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right| \leq
 2 C_{thresh} \frac{1}{\sqrt{n  (P_l \vee Q_l) } }
\]

\end{enumerate}
\end{lemma}


\begin{proof}


First, note that 
\begin{align*}
 \frac{P_l}{Q_l} - 1 &= \frac{P_l - Q_l}{Q_l}  
\end{align*}

We will show that $\frac{\hat{P}}{\hat{Q}}$ behaves similarly. 

As a preliminary step, we note that
\[
\frac{\hat{Q}_l - Q_l}{Q_l} = \eta \frac{\Delta_l}{Q_l} = \eta' \rightarrow 0
\]
where we used the assumption that $P_l \asymp Q_l$.

\begin{align*}
\frac{\hat{P}_l}{\hat{Q}_l} - 1 &= 
     \frac{ \hat{P}_l - P_l + P_l }{ \hat{Q}_l - Q_l + Q_l} -1  \\
  &=  \frac{  \frac{\hat{P}_l - P_l}{Q_l} + \frac{P_l}{Q_l}}
       { \frac{\hat{Q}_l - Q_l}{Q_l} + 1} - 1 \\
 &= \left( \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} \right)
    \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta')  \right) -1  \\
 &= \frac{P_l-Q_l}{Q_l} + \eta' \frac{\Delta_l}{Q_l} 
\end{align*}



Therefore, 
\begin{align*}
\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} - 1 &= 
   \sqrt{ 1 + \frac{P_l - Q_l}{Q_l} + \eta' \frac{\Delta_l}{Q_l}}  - 1\\
  &= \sqrt{ 1 + \frac{P_l - Q_l}{Q_l} } 
    \left( \sqrt{ 1 + \eta' \frac{\Delta_l}{P_l}} \right) - 1 \\
  &= \sqrt{ 1 + \frac{P_l - Q_l}{Q_l} } (1 + \eta' \frac{\Delta_l}{P_l} ) - 1  \\
  &= \sqrt{ 1 + \frac{P_l - Q_l}{Q_l}} - 1 + \sqrt{ \frac{P_l}{Q_l}} \eta' \frac{\Delta_l}{P_l} \\ 
  & \left\{ \begin{array}{cc}
      \leq \frac{P_l - Q_l}{Q_l} (1+ \eta') & \trm{(if $P_l \geq Q_l$)} \\
      \geq \frac{P_l - Q_l}{Q_l} (1+ \eta') & \trm{(if $P_l < Q_l$)} 
     \end{array} \right.
\\
\end{align*}

The second and the third equality holds for all small enough $\eta'$. The last inequality follows because $P_l \asymp Q_l$. \\


Symmetry yields that 

\begin{align*}
\sqrt{ \frac{\hat{Q}_l}{\hat{P}_l} } - 1  &
   \left\{ \begin{array}{cc}
      \leq \frac{Q_l - P_l}{P_l} (1 + \eta') & \trm{(if $Q_l \geq P_l$)} \\
      \geq \frac{Q_l - P_l}{P_l} (1 + \eta') & \trm{(if $Q_l < P_l$)} 
     \end{array} \right. \\
\end{align*}

This proves the first case. The proof of the second case is almost identical. 

First, under the assumption that $P_l, Q_l \geq \frac{c}{n}$, it is clear that $\frac{\hat{Q}_l - Q_l}{Q_l} = \eta \sqrt{ \frac{1}{ n Q_l} } = \eta' \rightarrow 0$. 

Therefore, it still follows that
\begin{align*}
\frac{\hat{P}_l}{\hat{Q}_l} - 1 &= 
     \left( \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} \right)
     \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} ( 1 + \eta') \right) - 1 \\
 &= \frac{P_l - Q_l}{Q_l} + \eta' \sqrt{ \frac{1}{ n (P_l \vee Q_l)} } 
\end{align*}

and it is clear that $\eta'$ must satisfy the condition that $\frac{P_l - Q_l}{Q_l} + \eta'\sqrt{ \frac{1}{n (P_l \vee Q_l)} } + 1 > 0$. 



\begin{align*}
\left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right| &= 
 \left|  \sqrt{ 1 + \frac{P_l - Q_l}{Q_l} + \eta' \frac{1}{\sqrt{n (P_l \vee Q_l)} }}
   -1  \right| \\
  &\leq \left| \frac{P_l - Q_l}{Q_l} + \eta' \frac{1}{\sqrt{ n (P_l \vee Q_l)}}        \right| \\
 &\leq 2 C_{thresh} \frac{1}{\sqrt{n (P_l \vee Q_l)} } 
\end{align*}

The first inequality follows because $ \sqrt{1 + x} - 1 \leq x$ for $x \geq 0$ and $\sqrt{ 1 + x} - 1 \geq x$ for $-1 < x < 0$. 
The second inequality follows because $\left| \frac{P_l - Q_l}{Q_l} \right| \leq C_{thresh} \frac{1}{\sqrt{n (P_l \vee Q_l)}} $. 

\end{proof}


The following lemma bounds $\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } Q_l$. 
\begin{lemma}
\label{lem:sqrt_ratio_times_ql}

Let $\eta$ be a sequence that tends to 0. Fix a color $l$ and suppose $P_l \asymp Q_l$. Suppose also that
\[
\frac{|\hat{Q}_l - Q_l|}{Q_l} = \eta \quad 
\frac{|\hat{P}_l - P_l|}{P_l} = \eta
\]

Then, we have that for all small enough $\eta$, 
\[
\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } Q_l \geq \frac{1}{2} \sqrt{\hat{P}_l \hat{Q}_l} \geq
  \frac{1}{8} \sqrt{P_l Q_l}
\]
\end{lemma}

First, we note that the condition that $\frac{|\hat{Q}_l - Q_l|}{Q_l} \rightarrow 0$ is satisfied for $l$ such that $\Delta_l \geq C_{thresh}\sqrt{\frac{P_l \vee Q_l}{n}}$ as well as for $l$ such that $\Delta_l \leq C_{thresh} \sqrt{\frac{P_l \vee Q_l}{n}}$ if we also add the condition that $P_l, Q_l = \frac{C}{n}$ for some constant $C$. This is because, in the latter case, it is known that $\frac{|\hat{P}_l - P_l| }{P_l} \leq \eta \sqrt{\frac{1}{n (P_l \vee Q_l)}}$. 

\begin{proof}


\begin{align*}
& \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \\
&= \sqrt{ \hat{P}_l \hat{Q}_l} \frac{Q_l}{\hat{Q}_l} \\
&= \sqrt{\hat{P}_l \hat{Q}_l} \frac{1}{ \frac{Q_l - \hat{Q}_l}{Q_l} + 1 } \\
&= \sqrt{\hat{P}_l \hat{Q}_l} 
  \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta') \right)  \\
&= \sqrt{\hat{P}_l \hat{Q}_l} (1 - \eta)
\end{align*}

where in the second to last equality, we used the fact that $\frac{\hat{Q}_l - Q_l}{Q_l} = \eta' \rightarrow 0$. 

Clearly then, for small enough $\eta$, we continue the bound as 
\begin{align*}
&\geq \frac{1}{2} \sqrt{ \hat{P}_l \hat{Q}_l} 
\end{align*}

Also, for small enough $\eta$, we have that $\hat{P}_l \geq \frac{1}{2} P_l$ and $\hat{Q}_l \geq \frac{1}{2} Q_l$ and thus, we have the final bound
\[
\frac{1}{8} \sqrt{ P_l Q_l}
\]
as desired. 

\end{proof}





\newpage
\section{Choosing the initial clustering}
\label{sec:initial_clustering}

We will assume for now that the number of clusters $k$ is fixed. Recall that the stage 1 of our algorithm is as follows. Let $\tau$ be an input parameter. We define $I_l = \frac{\Delta_l^2 }{P_l \vee Q_l}$. 

\begin{enumerate}
\item For each color $l$:
  \begin{enumerate}
   \item Perform spectral clustering on $\tilde{A}_{ij} \equiv \mathbf{1}(A_{ij} = l)$ to get $\sigma^l$. 
   \item Estimate $\hat{P}_l, \hat{Q}_l$ via counts from $\sigma^l$. 
   \item Estimate $\sqrt{I_l}$ via 
  $\sqrt{ \hat{I}_l } \equiv \frac{|\hat{P}_l - \hat{Q}_l|}{\sqrt{ \hat{P}_l \vee \hat{Q}_l}}$. 
   \end{enumerate}
\item Output $l^*$ for which $\sqrt{\hat{I}}$ is maximized
\end{enumerate}

Assuming that $ n I_{tot} \rightarrow \infty$, we want to say that the $l^*$ we output satisfies $n I_{l^*} \rightarrow \infty $. It must be noted $l^*$ depends on $n$. We will omit the dependence in our notation. 

This claim follows from proposition~\ref{prop:initial_guarantee} below. 

\begin{proposition}
\label{prop:initial_guarantee}
Let $k$ be fixed. Suppose that $c_1 \leq \frac{P_l}{Q_l} \leq c_2$ for all colors $l$. Fix a constant $\delta > 0$. Let $\sigma^l$ be a spectral clustering of the graph based on $\tilde{A}_{ij} = \mathbf{1}(A_{ij} = l)$ and let $\hat{P}_l, \hat{Q}_l$ be estimates of $P_l, Q_l$ constructed from $\sigma^l$.


Suppose $n$ is larger enough such that $n \geq c_3 k^3$ for some absolute constant $c_3$ and that $n \geq 2 \beta k$. 

Then, there is a positive constant $C_{test}$ such that, with probability at least $1 - Ln^{-3 + \delta}$,

\begin{enumerate} 
\item for all colors $l$ satisfying $\Delta_l \geq C_{test} \sqrt{ \frac{P_l \vee Q_l}{n}}, $  we have that
\begin{align}
\frac{1}{\sqrt{5}} \frac{ | P_l - Q_l |}{\sqrt{P_l \vee Q_l}}  \leq \frac{ | \hat{P}_l - \hat{Q}_l| }{\sqrt{ \hat{P}_l \vee \hat{Q}_l }} \leq  
\frac{3}{\sqrt{3}} \frac{ | P_l - Q_l | }{\sqrt{ P_l \vee Q_l}} 
\end{align}

\item for all colors satisfying $\Delta_l < C_{test} \sqrt{ \frac{P_l \vee Q_l}{n} }$, we have that

\begin{align}
\frac{ | \hat{P}_l - \hat{Q}_l|}{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} \leq C_{test} C_{c_1, c_2, \beta, k, \delta, c} \sqrt{ \frac{1}{n} } 
\label{eqn:bad_l_initialization}
\end{align}

where $C_{c_1, c_2, \beta, k, \delta, c}$ is a constant. 

\end{enumerate}

\end{proposition}

The specific values $\frac{1}{\sqrt{5}}$ and $\frac{3}{\sqrt{3}}$ are not important. $C_{test}$ can be tweaked to allow for other values. 

\begin{proof} 
% Theorem 10.1 requires that n >= 2 beta k
% and that (k^3 p)/(p-q)^2 n^2 \leq c

Consider a fixed color $l$ and let $\sigma^l$ be a spectral clustering based on color $l$. Then, if $\Delta_l \geq \sqrt{ \frac{P_l \vee Q_l}{n} }$, we have that $\frac{k^3}{I_l n^2} \leq c_3$ by our condition that $n \geq c_3 k^3$. Thus, we can apply theorem~\ref{thm:spectral_rate} and conclude that $\gamma = l(\sigma^l, \sigma_0) \leq C_{c_1,c_2} \frac{\beta^2 k^2 (P_l \vee Q_l)}{\Delta_l^2 n}$, with probability at least $1 - n^{-4}$, for some constant $C_{c_1, c_2}$. 

From the proof of estimation error of $\hat{P}_l$ (\ref{prop:estimation_consistency}), we have that,
\begin{align*}
|\hat{P}_l - P_l| \leq \Delta_l \eta_1 + \sqrt{\frac{P_l \vee Q_l}{n} }\eta_2
\end{align*}
for $\eta_1 = 4 k \gamma$ and 
$\eta_2 = 4 \left( \sqrt{ C_\delta k \gamma \log \frac{k}{\gamma}} + \frac{C_\delta}{c} k \gamma \log \frac{k}{\gamma} \right)$. 
with probability at least $1 - n^{-(3+\delta)}$  and likewise for $\hat{Q}_l - Q_l$. 

We want to set $C_{test} \geq 1$ such that $|\hat{P}_l - P_l | \leq \frac{1}{4} \Delta_l$. It is straight forward to verify that this is possible by setting $\eta_1 = \eta_2 = 1/8$, which is possible by setting $\gamma$ to be small enough. The chosen $C_{test}$ depends on $\beta, k, c_1, c_2, \delta, c$. 

Having set $C_{test}$ such that $| \hat{P}_l - P_l | \leq \frac{1}{4} \Delta_l$. We now proceed to bound $\frac{| \hat{P}_l - \hat{Q}_l| }{\sqrt{\hat{P}_l \vee \hat{Q}_l}}$. 

First we bound the numerator.
\begin{align*}
|\hat{P}_l - \hat{Q}_l| &= | \hat{P}_l - P_l + P_l - Q_l + Q_l - \hat{Q}_l| \\
    &\leq \Delta_l + | \hat{P}_l - P_l| + |\hat{Q}_l - Q_l| \\
    &\leq \Delta_l + \frac{1}{2} \Delta_l = \frac{3}{2} \Delta_l \\
|\hat{P}_l - \hat{Q}_l| &\geq \Delta_l - |\hat{P}_l - P_l| - |\hat{Q}_l - Q_l| \\
    &\geq \frac{1}{2} \Delta_l
\end{align*}

Next we bound the denominator. We will suppose without the loss of generality that $P_l \geq Q_l$. 
\begin{align*}
| \hat{P}_l - P_l | &\leq \frac{1}{4} \Delta_l  \quad (\Rightarrow) \\
|\hat{P}_l - P_l| &\leq \frac{1}{4} P_l \quad (\Rightarrow) \\
\hat{P}_l - P_l &\geq - \frac{1}{4} P_l \quad (\Rightarrow) \\
\hat{P}_l &\geq \frac{3}{4} P_l 
\end{align*}

Thus implying that $\sqrt{ \hat{P}_l \vee \hat{Q}_l } \geq \sqrt{ \frac{3}{4} P_l}$. 
\begin{align*}
| \hat{P}_l - P_l | &\leq \frac{1}{4} \Delta_l \quad (\Rightarrow) \\
| \hat{P}_l - P_l| &\leq \frac{1}{4} P_l \quad (\Rightarrow) \\
\hat{P}_l - P_l &\leq \frac{1}{4} P_l \quad (\Rightarrow) \\
\hat{P}_l &\leq \frac{5}{4} P_l 
\end{align*}

The same reasoning can be used to show that $\hat{Q}_l \leq \frac{5}{4} P_l$. Thus, $\sqrt{ \hat{P}_l \vee \hat{Q}_l } \leq \sqrt{ \frac{5}{4} P_l }$. Putting these bounds together, we have
\begin{align*}
\frac{ | \hat{P}_l - \hat{Q}_l |}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} &\geq 
          \frac{1}{\sqrt{5}} \frac{| P_l - Q_l | }{ \sqrt{ P_l \vee Q_l} }
\end{align*}

\begin{align*}
\frac{ | \hat{P}_l - \hat{Q}_l |}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} &\leq 
          \frac{3}{\sqrt{3}} \frac{| P_l - Q_l |}{\sqrt{P_l \vee Q_l}} 
\end{align*}

This proves the first claim. To prove the second claim, we again consider the equation
\begin{align*}
|\hat{P}_l - P_l| \leq \Delta_l \eta_1 + \sqrt{\frac{P_l \vee Q_l}{n} }\eta_2
\end{align*}
for $\eta_1 = 4 k \gamma$ and 
$\eta_2 = 4 \left( \sqrt{ C_\delta k \gamma \log \frac{k}{\gamma}} + \frac{C_\delta}{c} k \gamma \log \frac{k}{\gamma} \right)$. 

Without the lower bound on $\Delta_l$, we know only that $\gamma \leq 1$ and thus, conclude that
\[
| \hat{P}_l - P_l | \leq C_{test} C_{\beta, k, \delta} \sqrt{ \frac{P_l \vee Q_l}{n} } 
\]
and likewise with $|\hat{Q}_l - Q_l|$, both uniformly for all $l$, with probability $1 - L n^{-(3+\delta)}$. 

By proposition~\ref{prop:prob_estimate_lower_bound}, we have also that, with probability at least $1 - L exp(- C_{c_1, c_2, c} n)$, that
\[
\hat{P}_l \vee \hat{Q}_l \geq \frac{P_l \wedge Q_l}{2}
\]
Putting these together, we have that
\begin{align*}
|\hat{P}_l - \hat{Q}_l| &= | \hat{P}_l - P_l + P_l - Q_l + Q_l - \hat{Q}_l| \\
    &\leq \Delta_l + | \hat{P}_l - P_l| + |\hat{Q}_l - Q_l| \\
    &\leq \Delta_l + C_{test} C_{\beta, k, \delta} \sqrt{ \frac{P_l \vee Q_l}{n} } \\
    &\leq 2 C_{test} C_{\beta, k, \delta} \sqrt{ \frac{P_l \vee Q_l}{n} }
\end{align*}
and that
\begin{align*}
\sqrt{\hat{P}_l \vee \hat{Q}_l} &\geq \sqrt{\frac{P_l \wedge Q_l}{2}} \\
                 &\geq \sqrt{\frac{1}{2} (c_1 \wedge \frac{1}{c_2} ) (P_l \vee Q_l)} 
\end{align*} 
and thus,
\begin{align*}
\frac{| \hat{P}_l - \hat{Q}_l| }{\sqrt{ \hat{P}_l \vee \hat{Q}_l} } \leq 2 C_{test} C_{c_1, c_2, \beta, k, \delta} \sqrt{ \frac{1}{n} }
\end{align*}

\end{proof}

The following theorem formalizes the intuition that if $\frac{n I}{L} \rightarrow \infty$, then, $ n I_{l^*} \rightarrow \infty$ as well where $l^*$ is the color chosen by initialization. 

\begin{theorem}
Let $M_0$ be a constant (whose value is given in the proof). Let $M > M_0$ be arbitrarily fixed. Suppose $n_0$ is such that all $n > n_0$, $\frac{n I}{L} \geq M^2$.

Then, for all $n > n_1$ for some constant $n_1$ dependent on $n_0, c_3, k, beta$, with probability at least $1 - 2L n^{-(3+\delta)}$, we have that $n I_{l^*} \geq \frac{M^2}{C_{c_1}} \frac{15}{9}$ for some constant $C_{c_1}$. 
\end{theorem}

\begin{proof}

Let $C_{test}$ be the constant in proposition~\ref{prop:initial_guarantee}. Let $C_{c_1, c_2, c, \beta, k, \delta} \geq 1$ be the constant in equation~\ref{eqn:bad_l_initialization}.

Let $C_{c_1}$ be a constant such that $I \leq C_{c_1} \sum_{l=1}^L \frac{\Delta_l^2}{P_l \vee Q_l}$; such a constant exists by lemma~\ref{lem:simplify_renyi2}. 

We set $M_0 > C_{test} C_{c_1, c_2, c, \beta, k, \delta} \sqrt{C_{c_1}} \sqrt{5}$. \\

Suppose that $M > M_0$ and that $\frac{n I}{L} \geq M$. By lemma~\ref{lem:simplify_renyi2}, we have that
\begin{align*}
\frac{1}{L} n C_{c_1} \sum_{l=1}^L \frac{\Delta_l^2}{P_l \vee Q_l} \geq M^2
\end{align*}

Therefore, there must exist some color $l$ such that $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq \frac{M^2}{C_{c_1}}$. By our choice of $M_0$, we have that $\frac{M_0}{\sqrt{C_{c_1}}} \geq C_{test}$ and thus, $\frac{M}{\sqrt{C_{c_1}}} \geq C_{test}$. Thus, so long as $n > n_0$, there also exists some color $l$ such that $\Delta_l \geq C_{test} \sqrt{\frac{P_l \vee Q_l}{n}}$. \\


Let $n_1 = min(n_0, c_3 k^3, 2 \beta k)$ so that proposition~\ref{prop:initial_guarantee} applies for $n > n_1$. We now only consider $n$ large enough so that $n > n_1$.
Suppose that the probability event of proposition~\ref{prop:initial_guarantee} holds, which happens with probability at least $1 - L n^{-(3+\delta)}$. 

\textbf{Step 1.} We claim that $l^*$ satisfies $\Delta_{l^*} \geq C_{test} \sqrt{ \frac{P_l \vee Q_l}{n}} $. Let $l$ be a color such that $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq \frac{M^2}{C_{c_1}}$ and suppose $l^*$ does not satisfy $\Delta_{l^*} \geq C_{test} \sqrt{ \frac{P_l \vee Q_l}{n} }$. 

Then, we have that, by proposition~\ref{prop:initial_guarantee}, 
\begin{align*}
\frac{| \hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} 
  \leq \frac{| \hat{P}_{l^*} - \hat{Q}_{l^*} | }{\sqrt{ \hat{P}_{l^*} \vee \hat{Q}_{l^*}}} 
         \leq C_{test} C_{c_1, c_2, c, \beta, k, \delta} \sqrt{ \frac{1}{n}} 
\end{align*}

But, because $l$ satisfies $\Delta_l \geq C_{test} \sqrt{ \frac{P_l \vee Q_l}{n}}$, we also have
\begin{align*}
\frac{| \hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} 
  \geq \frac{1}{\sqrt{5}} \frac{ | P_l - Q_l|}{\sqrt{P_l \vee Q_l}} 
   \geq \frac{1}{\sqrt{5}} \frac{M}{\sqrt{C_{c_1}}} \sqrt{ \frac{1}{n}}
\end{align*}
This is a contradiction by our definition of $M_0$. 

\textbf{Step 2:}

Again, let $l$ be a color such that $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq \frac{M^2}{C_{c_1}}$.

\begin{align*}
\frac{ |P_{l^*} - Q_{l^*}|}{\sqrt{ P_{l^*} \vee Q_{l^*}}} &\geq 
\frac{\sqrt{3}}{3} \frac{|\hat{P}_{l^*} - \hat{Q}_{l^*} | }{\sqrt{ \hat{P}_{l^*} \vee \hat{Q}_{l^*} }} 
  \\
& \geq
\frac{\sqrt{3}}{3} \frac{|\hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}}  \\
 &\geq \frac{|P_l - Q_l|}{\sqrt{P_l \vee Q_l}} \frac{\sqrt{15}}{3}  \\
 &\geq \frac{M}{\sqrt{C_{c_1}}} \frac{\sqrt{15}}{3} 
\end{align*}
where the first inequality follows from the fact that $l^*$ satisfies $\Delta_{l^*} \geq C_{test} \sqrt{ \frac{P_l \vee Q_l}{n}}$ and proposition~\ref{prop:initial_guarantee}. The third inequality again follows from proposition~\ref{prop:initial_guarantee}.


\end{proof}

\section{Continuous Distributions}

In this section, we suppose that the weights of within-cluster edges are drawn from a density $p$ and that of between-cluster edges are drawn from a density $q$.
\begin{align*}
A_{ij} &\sim p  \quad \trm{if $\sigma_0(i) = \sigma_0(j)$} \\
A_{ij} &\sim q  \quad \trm{if $\sigma_0(i) = \sigma_0(j)$}
\end{align*}

In this case, the continuous Renyi divergence is 
\[
I = -2 \log \int \sqrt{p(x)q(x)} dx 
\]

Under the oracle setting, the rate of recovery is similar to that of proposition~\ref{prop:weak_recovery_oracle}. 
\begin{proposition} 
\label{prop:weak_recovery_oracle_continuous}
(Oracle Setting Upper Bound for Continuous Distributions)
Assume $\frac{n I}{K \log K} \rightarrow \infty$. The maximum likelihood estimator $\hat{\sigma}$ in the oracle setting achieves:
\[
\sup_{\Theta(n, K, \beta, P, Q)} \E r(\hat{\sigma}, \sigma) \leq \left\{ 
    \begin{array}{cc} 
   \exp \left( - (1 + o(1)) \frac{nI}{2} \right ), \, & K=2, \\
   \exp \left( - (1 + o(1)) \frac{nI}{\beta K} \right ), \,& K\geq 3
  \end{array} \right. 
\]   
\end{proposition}
The proof is identical to that of proposition~\ref{prop:weak_recovery_oracle}, replacing sums with integrals where needed. 


\subsection{Rate Optimal Recovery}

Let us go to the general setting. Our goal is to show that under certain conditions, rate-optimal recovery is possible for continuous distributions.

\subsubsection{Assumptions}
\label{sec:continuous_assumptions}

\begin{enumerate}
\item[A1] $p(x), q(x)$ are supported on $[0,1]$, and $C \geq p(x) \geq c > 0$. 
\item[A2] Let $\gamma(x) = \frac{p(x) - q(x)}{\| p - q \|_2}$. Suppose that $|\gamma(x)| \leq M$ for some constant $M$. 
\item[A3] $|p'(x)|, |q'(x)|, |\gamma'(x)| \leq M'$ for some constant $M'$. 
\end{enumerate}


Let us see a specific example in which these generalizations are relevant. 

\begin{example}

Suppose that $f(x)$ is a density supported on $[0, 1-\alpha]$ and $g(x)$ is a density, bounded away from 0, and supported on $[0,1]$. We define $p(x), q(x)$ as mixtures.

\begin{align*}
p(x) &= \lambda f(x) + (1-\lambda) g(x)\\
q(x) &= \lambda f(x - \alpha) + (1-\lambda) g(x)
\end{align*}

where the mixing weight $1 > \lambda > 0$ is a constant. Since $g(x)$ is bounded away from 0, it is clear that assumption A1 is satisfied. 

We suppose that $f$ is twice-differentiable, $g$ is differentiable, and that $|f'(x)|, |g'(x)|, |f''(x)| \leq M$ for some constant $M > 0$. We suppose also that $\int f'(x)^2 dx > 0$.

\begin{align*}
p(x) - q(x) &= \lambda (f(x) - f(x-\alpha)) 
\end{align*}

Also, we have, by Taylor theorem, that
\begin{align*}
f(x - \alpha) &= f(x) + f'(x) \alpha + \frac{f''(x_1)}{2} \alpha^2 \quad \trm{for some $x_1$ in between $x, x-\alpha$} \\
\frac{f(x) - f(x-\alpha)}{\alpha} &= f'(x) + \frac{1}{2} f''(x_1) \alpha
\end{align*}

Therefore,
\begin{align*}
\int (p(x) - q(x))^2 dx &= \lambda^2 \alpha^2 \int 
   \left( \frac{f(x) - f(x - \alpha)}{\alpha} \right)^2 dx \\
  &= \lambda^2 \alpha^2 \int (f'(x) + \frac{1}{2} f''(x_1) \alpha)^2 dx \\
 &= \lambda^2 \alpha^2 \left( \int f'(x)^2 dx  + \int f'(x) f''(x_1) \alpha + f''(x_1)^2 \alpha^2 dx \right) 
\end{align*}

Since $|f'(x)|, |f''(x)| \leq M$, we conclude that, for some constant $C$, we have that
\[
\int (p(x) - q(x))^2 dx = C \alpha^2 + O(\alpha^3) 
\]

For all small enough $\alpha$, it is thus clear that
\[
\gamma(x) = \frac{p(x) - q(x)}{\| p - q \|_2} = C \cdot \frac{p(x) - q(x)}{\alpha} + O(\sqrt{\alpha})
\]
for some constant $C$ independent of $x, \alpha$. To bound $|\gamma(x)|$ and $|\gamma'(x)|$, it is thus sufficient to analyze $\tilde{\gamma}(x) = \frac{p(x) - q(x)}{\alpha}$. 

\begin{align*}
\left| \frac{ p(x) - q(x)}{\alpha} \right| &= \lambda \left| \frac{f(x) - f(x-\alpha)}{\alpha} \right| \\
 &= \left| f'(x) + \frac{1}{2} f''(x_1) \alpha \right| \\
 &\leq 2M  \quad \trm{for small enough $\alpha$}
\end{align*}

Thus, assumption A2 has been verified. 

\begin{align*}
\frac{ \tilde{\gamma}(x) - \tilde{\gamma}(x - c) }{c} &= 
 \frac{\lambda}{c} \left(   \frac{f(x) - f(x-\alpha)}{\alpha} - \frac{f(x-c) - f(x-c - \alpha)}{\alpha}  \right) \\
  &= \frac{\lambda}{\alpha} \left( \frac{f(x) - f(x-c)}{c} - 
                  \frac{ f(x-\alpha) - f(x - \alpha - c)}{c} \right) \\
 &\trm{ take the limit $c \rightarrow 0$} \\
\tilde{\gamma}'(x) &= \frac{\lambda}{\alpha} (f'(x) - f'(x-\alpha) ) 
\end{align*}

Thus, it is clear that $ | \tilde{\gamma}'(x) | \leq \lambda M$ and we have verified assumption A3. 

\end{example}

\subsubsection{Continuous and Discretized Renyi Divergence}

First, we show that, under the assumptions we have listed, the continuous Renyi divergence scales as $\alpha^2$. 

\begin{proposition}
\label{prop:continuous_renyi_order}
Let $I = -2 \log \int \sqrt{p(x)q(x)} dx$ be the continuous Renyi divergence. Define $\delta(x) = q(x) - p(x)$ and let $\alpha$ be a real number such that 
\[
C' \int q(x) \left( \frac{\delta{x}}{q(x)} \right)^2 dx \geq \alpha^2 \geq c' \int q(x) \left( \frac{\delta(x)}{q(x)} \right)^2 dx
\]
 for some constants $C', c'$. Define $\gamma(x) = \frac{\delta(x)}{\alpha}$. 

Suppose $\left| \frac{\delta(x)}{q(x)} \right| \leq \frac{1}{2}$, that 
$ \int q(x) \left| \frac{\gamma(x)}{q(x)} \right|^3 dx \leq M$.

We have that, with $d = \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx$,
\[
I = d \alpha^2 ( 1 + \eta )
\]
where, for any $\alpha < \frac{1}{CM}$, $|\eta| \leq M \alpha$. 

\end{proposition}

\begin{proof}
First, let us note that
\[
\frac{1}{c'} \geq \int q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx \geq \frac{1}{C'}
\]

Now, denote $H = \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx$ as the continuous Hellinger distance. Denote $\delta(x) = q(x) - p(x)$ and note that $\alpha(x) = \gamma(x) \alpha$. 

\begin{align*}
  &\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \\
=& \int ( \sqrt{q(x)} - \sqrt{q(x) - \delta(x)} )^2 dx \\
=& \int q(x) \left( 1 - \sqrt{ 1 - \frac{\delta(x)}{q(x)}} \right)^2 dx 
\end{align*}

Since $\left| \frac{\delta(x)}{q(x)} \right| \leq \frac{1}{2}$, we can take Taylor series expansion of $f(z) = \sqrt{1 - z}$ around 0 and plug in $\frac{\delta(x)}{q(x)}$. \\


Therefore, there exists some function $\xi(x)$ satisfying $|\xi(x)| \leq \left( \frac{\delta(x)}{q(x)} \right)^2$ such that

\begin{align*}
=& \int q(x) \left( 1 - (1 - \frac{1}{2} \frac{\delta(x)}{q(x)} + \xi(x) ) \right)^2 dx \\
=& \int q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} + \xi(x) \right)^2 dx\\
\end{align*}

Define $\xi_2(x) = \xi(x) \frac{\delta(x)}{q(x)}$. Since $| \frac{\delta(x)}{q(x)} | \leq 1/2$, it cannot be that $q(x) = 0$ and $\delta(x) \neq 0$, and $\xi_2(x)$ is thus well-defined. Also note that 

\[
|\xi_2(x)| \leq |\xi(x)| \left| \frac{q(x)}{\delta(x)} \right| \leq
   \left| \frac{\delta(x)}{q(x)} \right|
\]

\begin{align*}
=& \int q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} (1 + \xi_2(x)) \right)^2 dx\\
=& \int q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} \right)^2 (1 + 2 \xi_2(x) + \xi_2(x)^2 dx\\
=& \alpha^2 (1 + \eta) \int q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} \right)^2 dx
\end{align*}


where 

\[
\eta = \frac{ \int q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} \right)^2 ( 2 \xi_2(x) + \xi_2(x)^2 )  dx }
             { \int q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} \right)^2 dx} 
  = \frac{ \int q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 ( 2 \xi_2(x) + \xi_2(x)^2 )  dx }
             { \int q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx} 
\]

We want to show that $|\eta| \rightarrow 0$ as $\alpha \rightarrow 0$. By our definition of $\alpha$, $\int q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx \geq \frac{1}{C'}$, and the denominator is at most $\frac{1}{C'}$. 

\begin{align*}
|\eta| &= C' \left| \int q(x) \left(\frac{\gamma(x)}{q(x)} \right)^2 ( 2 \xi_2(x) + \xi_2(x)^2 )  dx \right| \\
    &\leq C' \int q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 2 (| \xi_2(x) | + \xi_2(x)^2) dx \\
    &\leq C' \int q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 
           2\left| \frac{\delta(x)}{q(x)} \right| +
         C' \int q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 
           2\left( \frac{\delta(x)}{q(x)} \right)^2  \\
    &\leq C' \alpha \int q(x) \left| \frac{\gamma(x)}{q(x)} \right|^3 dx + 
          C' \alpha \int q(x) \left| \frac{\gamma(x)}{q(x)} \right|^3 dx
\end{align*}
   
The second inequality follows from our bound $|\xi_2(x)| \leq \left| \frac{\delta(x)}{q(x)} \right| \leq \alpha \left| \frac{\gamma(x)}{q(x)} \right|$. The last inequality follows from the assumption that $\left| \frac{\gamma(x)}{q(x)} \right| \leq \frac{1}{2}$. 

Under the assumption that $\int q(x) \left| \frac{\gamma(x)}{q(x)} \right|^3 dx \leq M < \infty$,  we have that $| \eta | \leq M C' \alpha$. 

We have just shown then that $H = \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = \alpha^2 (1+\eta) \int q(x) \left( \frac{1}{2} \frac{\gamma(x)}{q(x)} \right)^2 dx$. 


Now, it remains to bound $I$ in terms of $H$. 
\begin{align*}
I =& -2 \log \int \sqrt{p(x)q(x)} dx \\
 =& -2 \log \left( 1 - \frac{1}{2} \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right) \\
 =& -2 \log (1 - \frac{1}{2} H) 
\end{align*}

If $\alpha < \frac{c'}{C' M}$, then we have that $H \leq 2 \alpha^2 \leq 1$.

\[
(1 + \eta') H \geq I \geq H
\]

where $|\eta'| \leq \frac{1}{2} H  \leq \alpha^2 $. 

This completes the proof with $d = \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx$. 

\end{proof}


Next, we define the \textbf{discretized Renyi divergence}. Let the interval $[0,1]$ be divided into $L$ equally spaced sub-intervals. Let $l$ index each of these sub-intervals and let $B = 1/L$ be the length of each sub-interval.

Let $P_l = \int_{\trm{Bin}_l} p(x) dx$ and $Q_l = \int_{\trm{Bin}_l} q(x)dx$. We define the \emph{discretized Renyi divergence} as $\tilde{I} = -2 \log \sum_{l=1}^L \sqrt{P_l Q_l} $ and $\tilde{H} = \sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2$. 


\begin{proposition}
\label{prop:discrete_renyi_order}
Let $L \geq 1$ be arbitrary and let $\tilde{I}_L = -2 \log \sum_{l=1}^L \sqrt{P_l Q_l}$ be the discretized Renyi divergence. Suppose $\alpha^2 \geq c' \int q(x) \left( \frac{\delta(x)}{q(x)} \right)^2 dx$ for some constant $C$. \\

Suppose that $\left| \frac{\delta(x)}{q(x)} \right| \leq 1/2$, and that $\int q(x) \left| \frac{\gamma(x)}{q(x)} \right|^3 dx \leq M$. 

Let $d_L = \sum_l P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 $

Then, we have that,
\[
 \tilde{I}_L = d_L \alpha^2 ( 1 + \eta_L )
\]
where, for any $\alpha < \frac{c}{2M}$, $|\eta_L| \leq \frac{8M}{c} \alpha$.

\end{proposition}

\begin{proof}

Again, we first analyze the discretized Hellinger distance. The discretized Renyi divergence can be bounded in terms of the discretized Hellinger distance in the same fashion as the continuous case. 

\begin{align*}
\tilde{H} &= \sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2 \\
   &= \sum_{l=1}^L Q_l \left( 1 - \sqrt{P_l}{Q_l} \right)^2 \\
 &= \sum_{l=1}^L Q_l \left( 1 - \sqrt{1 - \frac{Q_l - P_l}{Q_l}} \right)^2 
\end{align*}

Define $\delta_l = Q_l - P_l$ and $\gamma_l = \frac{\delta_l}{\alpha}$. We note that
\begin{align}
\left| \frac{\delta_l}{Q_l} \right| \leq 
   \int \left|\frac{\delta(x)}{q(x)} \right| \frac{q(x)}{Q_l} dx \leq \frac{1}{2}
 \label{eqn:delta_l_q_l_bound}
\end{align}

Therefore, we can take the Taylor approximation
\begin{align*}
\tilde{H} &= \sum_{l=1}^L Q_l \left( 1 - \sqrt{ 1 - \frac{\delta_l}{Q_l}} \right)^2
\\
&= \sum_{l=1}^L Q_l \left( \frac{1}{2} \frac{\delta_l}{Q_l} + \eta_l \right)^2 
\end{align*}
where $|\eta_l| \leq \left( \frac{\delta_l}{Q_l} \right)^2$. Define $\eta'_l = \eta_l \frac{Q_l}{\delta_l}$. Then, 

\begin{align*}
&= \sum_{l=1}^L Q_l \left( \frac{1}{2} \frac{\delta_l}{Q_l} \right)^2 (1 + \eta'_l)^2 \\
&= \left( \sum_{l=1}^L Q_l \left( \frac{1}{2} \frac{\delta_l}{Q_l} \right)^2 \right) ( 1 + \eta_L) 
\end{align*}

where 
\[
\eta_L = \frac{ 
  \sum_{l=1}^L Q_l \left( \frac{1}{2} \frac{\delta_l}{Q_l} \right)^2 (2 \eta'_l + \eta^{\prime 2}_l)}
{\sum_{l=1}^L Q_l \left( \frac{1}{2} \frac{\delta_l}{Q_l} \right)^2} 
 = \frac{ 
  \sum_{l=1}^L Q_l \left( \frac{\gamma_l}{Q_l} \right)^2 (2 \eta'_l + \eta^{\prime 2}_l)}
{\sum_{l=1}^L Q_l \left( \frac{\gamma_l}{Q_l} \right)^2} 
\]

The denominator $\sum_{l=1}^L Q_l \left( \frac{\gamma_l}{Q_l} \right)^2$, is, by proposition~\ref{prop:convergence_discrete_continuous_renyi}, at least 
$ \frac{1}{2} \int q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2$ for all large enough $L$. That quantity is in turn at least $c'$. 

To bound the numerator, we note that for a single bin $l$, we have
\[
\int_{Bin_l} \frac{q(x)}{Q_l} \left| \frac{\gamma(x)}{q(x)} \right|^3 dx \geq
 \left| \int_{Bin_l} \frac{q(x)}{Q_l} \frac{\gamma(x)}{q(x)} dx \right|^3 =
 \left| \frac{\gamma_l}{Q_l} \right|^3
\]
where the first inequality follows from Jensen's inequality. Therefore, we have that
\begin{align*}
\sum_{l=1}^L Q_l \left( \frac{\gamma_l}{Q_l} \right)^2 2 \eta'_l &\leq
   2 \alpha \sum_{l=1}^L Q_l \left| \frac{\gamma_l}{Q_l} \right|^3 \\
 &\leq 2 \alpha \int q(x) \left| \frac{\gamma(x)}{q(x)} \right|^3 dx \\
 &\leq 2 \alpha M
\end{align*}
and that
\begin{align*}
\sum_{l=1}^L Q_l \left( \frac{\gamma_l}{Q_l} \right)^2 \eta^{\prime 2}_l &\leq
 \frac{\alpha}{2}  \sum_{l=1}^L Q_l \left| \frac{\gamma_l}{Q_l} \right|^3  \\
  &\leq \alpha M
\end{align*}

where the first inequality follows because $|\eta^{\prime 2}_l| \leq \left( \frac{\delta_l}{Q_l} \right)^2 \leq \alpha \left| \frac{\gamma_l}{Q_l} \right| \frac{1}{2}$ by equation~\ref{eqn:delta_l_q_l_bound}. The second inequality follows from the above derivations.

Thus, we conclude that, for all large enough $L$, $|\eta_L| \leq \alpha \frac{3M}{c'}$. 

The bound to go from $H$ to $I$ proceeds exactly as before.
\end{proof}



Next, we will show that $\lim_{L \rightarrow \infty} \sup_n \left| \frac{d_L}{d} - 1 \right| = 0$. 

\begin{proposition}
\label{prop:convergence_discrete_continuous_renyi}

Let $d = \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx$ and $d_L = \sum_l P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2$. 

Suppose assumptions A1, A3 hold. Then, we have that
\[
\lim_{L \rightarrow \infty} \sup_n \left| \frac{d_L}{d} - 1 \right| = 0
\]

\end{proposition}


Note that $B \rightarrow 0$ is equivalent to $L \rightarrow \infty$ since $B = 1/L$. 

\begin{proof}

Let $\trm{Bin}_l = [a_l, b_l]$. 
\begin{align*}
P_l &= \int_{\trm{Bin}_l} p(x) dx \\
  &= \int_{a_l}^{b_l} p(x) dx \\
 &= \int_{a_l}^{b_l} p(a_l) + p'(c_x) (x - a_l) dx \quad \trm{for some $c_x \in [a_l, b_l]$}\\
 &= B p(a_l) + B^2 \xi_l \quad \trm{where $|\xi_l| \leq M'/2$}
\end{align*}

Likewise, we have that $\gamma_l = B \gamma(a_l) + B^2 \xi'_l$ for some $\xi'_l$ such that $|\xi'_l | \leq M'/2$. 

\begin{align*}
d_L &= \sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 \\
  &= \sum_{l=1}^L B \left(
     (p(a_l) + B \xi_l) \left( \frac{1}{2} \frac{\gamma(a_l) + B \xi'_l}{p(a_l) + B \xi_l} \right)^2 \right) 
\end{align*}

We will simplify this expression step by step, using the fact that $|\xi_l|, |\xi'_l| \leq M'/2$ and that $|\gamma(a_l)| \leq M$ and $|p(a_l)| \geq c$. 

\begin{align*}
d_L = \sum_{l=1}^L B
     p(a_l)  \left( \frac{1}{2} \frac{\gamma(a_l) + B \xi'_l}{p(a_l) + B \xi_l} \right)^2 + \sum_{l=1}^L B^2 \xi_l  \left( \frac{1}{2} \frac{\gamma(a_l) + B \xi'_l}{p(a_l) + B \xi_l} \right)^2 
\end{align*}

The second term is of magnitude at most $B M' \frac{1}{4} \left( \frac{M + B (M'/2)}{c - B (M'/2)} \right)^2$. This is clearly $c_{M, M', c} B$ for some constant $c_{M, M', c}$ independent of $n$. 

We now turn to the first term.
\begin{align*}
& \sum_{l=1}^L B
   p(a_l)  \left( \frac{1}{2} \frac{\gamma(a_l) + B \xi'_l}{p(a_l) + B \xi_l} \right)^2 \\
&= \sum_{l=1}^L B p(a_l) \left( \frac{1}{4} \frac{\gamma(a_l) + B \xi_l'}{p(a_l)} \right)^2
   \left( \frac{1}{ 1 + B \frac{\xi_l}{p(a_l)} } \right)^2 \\
&\leq \sum_{l=1}^L B p(a_l) \left( \frac{1}{4} 
    \frac{ \gamma(a_l)^2 + B \xi'_l \gamma(a_l) + (B \xi'_l)^2}{p(a_l)^2} \right) ( 1 + \xi''_l) 
\end{align*} 

where $\xi''_l$ satisfies $| \xi''_l | \leq \frac{3}{8} B M'/c$ for all $B \leq c / M'$. From here, it is straightforward to verify that the first term is 
\[
\sum_{l=1}^L Bp(a_l) \left( \frac{1}{2} \frac{\gamma(a_l)}{p(a_l)} \right)^2 + \trm{remainder}
\]
where the remainder term is bounded by magnitude by $c_{M, M', c}B$ for some constant $c_{M, M', c}$ independent of $n$. 

Define $d_R = \sum_{l=1}^L B p(a_l) \left( \frac{1}{2} \frac{\gamma(a_l)}{p(a_l)} \right)^2 $ where $R$ stands for Riemann sum. What we have just shown is that

\[
| d_R - d_L | \leq c_{M, M', c} B 
\]

Now, we look at $| d_R - d |$:
\begin{align*}
|d_R - d| &= \frac{1}{4} \left| \int \frac{\gamma(x)^2}{p(x)} dx - \sum_l B \frac{\gamma(a_l)^2}{p(a_l)} \right| \\
 &\leq \sup_x \left| \frac{ d \frac{\gamma(x)^2}{p(x)} }{dx} \right| B^2 \\
 &\leq \sup_x \left| \frac{ 2 \gamma'(x) \gamma(x) p(x) - p'(x) \gamma(x)^2 }{p(x)^2} \right| B^2 \\
 &\leq \frac{2 M^2 M' }{c^2} B^2 
\end{align*}

Thus, we have that
\begin{align*}
\left|\frac{d_L}{d} - 1 \right| &= \frac{| d_L - d |}{d} \\
  &\leq \frac{1}{d} \left( |d_L - d_R| + |d_R - d| \right) \\
  &\leq \frac{1}{d} c_{M, M', c} B 
\end{align*}

Since $\int \gamma(x)^2 dx = 1$, we have that $d = \frac{1}{4} \int \frac{\gamma(x)^2}{p(x)} dx \geq \frac{1}{4 C} \int \gamma(x)^2 dx = \frac{1}{4C}$. Thus

\[
\lim_{L \rightarrow \infty} \sup_n  \left| \frac{d_L}{d} - 1 \right| = 0
\]
 
\end{proof}


These propositions together imply the following:
\begin{shaded}
\begin{theorem}
\label{thm:relative_convergence_discrete_continuous_renyi}
Suppose assumptions A1-A3 are satisfied. Let $\tilde{I}_L$ be the discretized Renyi divergence discretized at level $L$.

Let $n \rightarrow \infty$, then, we have that, for \textbf{any sequences} $L_n \rightarrow \infty, \alpha_n \rightarrow 0$,

\[
\lim_{n \rightarrow \infty} \left| \frac{\tilde{I}_L}{I} - 1 \right| \rightarrow 0
\]

\end{theorem}
\end{shaded}

\begin{proof}

By proposition~\ref{prop:continuous_renyi_order} and proposition~\ref{prop:discrete_renyi_order}, we have that, for all $\alpha < \frac{c}{4M}$,

\begin{align*}
| \tilde{I}_L - I | 
 & \leq \left| d_L \alpha^2 ( 1 + \eta_L) - d \alpha^2 ( 1 + \eta) \right| \\
  &\leq d\alpha^2 \left| \frac{d_L}{d} (1 + \eta_L) - (1+\eta) \right|  \\
 &\leq I \left| \frac{d_L}{d} \frac{(1 + \eta_L)}{(1+\eta)} - 1 \right| \\
& \Rightarrow \\
\left| \frac{\tilde{I}_L}{I} - 1 \right| &\leq 
\left | \frac{d_L}{d} \frac{(1+\eta_L)}{1 + \eta} - 1 \right|
\end{align*}

where $|\eta|, |\eta_L| \leq \frac{12M}{c} \alpha$ for all $L$. Thus, it is clear that 
\[
\lim_{\alpha_n \rightarrow 0} \sup_L \left| \frac{1+\eta_L}{1+\eta} - 1 \right| = 0
\]

Furthermore, it has been shown ... that
\[
\lim_{L_n \rightarrow \infty} \sup_\alpha \left| \frac{d_L}{d} - 1 \right| = 0
\]

Let $\epsilon > 0$ be arbitrarily fixed. 
Choose $\alpha$ such that $\sup_L \left| \frac{1+\eta_L}{1+\eta} - 1 \right| \leq \epsilon/3$ and choose $L$ such that $\sup_\alpha \left| \frac{d_L}{d} -1 \right| \leq \epsilon/3$. 

Choose $n_0$ such that $\alpha_n < \alpha$ and $L_n < L$ for all $n > n_0$. 

Then, we have that, for all $n > n_0$:
\begin{align*}
& \left| \frac{d_L}{d} \frac{1+\eta_L}{1+\eta} - 1 \right | \\
&= \left| \frac{d_L}{d} \frac{1 + \eta_L}{1 + \eta} - \frac{d_L}{d} + \frac{d_L}{d} - 1 \right| \\
&= \left| \frac{d_L}{d} \left( \frac{1+\eta_L}{1+\eta} - 1 \right) + \left( \frac{d_L}{d} - 1 \right) \right| \\
&= \left| \left( \frac{1+\eta_L}{1 + \eta} - 1 \right) + \left( \frac{d_L}{d} -1 \right) \left( \frac{1+\eta_L}{1+\eta} - 1 \right) + \left( \frac{d_L}{d} - 1 \right) \right| \\
&\leq  \left| \frac{1+\eta_L}{1 + \eta} - 1 \right| + \left| \left( \frac{d_L}{d} -1 \right) \left( \frac{1+\eta_L}{1+\eta} - 1 \right) \right| + \left| \frac{d_L}{d} - 1  \right| \\
&\leq \epsilon
\end{align*}

The claim thus follows. 

\end{proof}

\subsubsection{Recovery Procedure}

The recovery procedure is straightforward. We bin the interval $[0,1]$ into $L$ uniformly spaced bins. We define $\tilde{A}_{ij} = l$ if $A_{ij} \in bin_l$. From theorem~\ref{prop:rate_optimal}, we know that from $\{\tilde{A}_{ij}\}$, we can achieving $l(\hat{\sigma}, \sigma_0) \leq \exp\left( - (1 - o(1)) \frac{n \tilde{I}_L}{\beta K } \right)$. 

From theorem~\ref{thm:relative_convergence_discrete_continuous_renyi}, we have also that $\tilde{I}_L \geq (1 - o(1)) I$. These facts give us the following theorem:

\begin{theorem}
\label{thm:continuous_optimal_recovery}
Suppose assumptions A1-A3 are satisfied, suppose $k$ is fixed and $n I \rightarrow \infty$. Then, for all sequence $L \rightarrow \infty$ such that $\frac{n I}{L} \rightarrow \infty$, discretization and level $L$ achieves
\[
\liminf_{n \rightarrow \infty} \sup_{\sigma_0, p, q} 
   P \left( l(\sigma_0, \hat{\sigma}) \geq 
              \exp \left( - (1-o(1)) \frac{n I}{K \beta} \right) \right)
   = 0
\]
\end{theorem}

\begin{proof}

If $\frac{nI}{L} \rightarrow \infty$, then $\frac{n \tilde{I}_L}{L} = \frac{n I}{L}(1 - o(1)) \rightarrow \infty$ as well. Thus, theorem~\ref{prop:rate_optimal} applies and we have that
\[
\liminf_{n \rightarrow \infty} \sup_{\sigma_0, p, q} 
   P \left( l(\sigma_0, \hat{\sigma}) \geq 
              \exp \left( - (1-o(1)) \frac{n \tilde{I}_L}{K \beta} \right) \right)
   = 0
\]

Applying $\tilde{I}_L = I(1-o(1))$ again finishes the argument.

\end{proof}


\subsection{An Useful Extension}

Let $R \subset [0,1]$ be open. $R$ is perhaps varying with $n$. \\

Let $\{h_n(x)\}$ be a family of functions $h_n : R \rightarrow \R$. Define $M_n(\kappa) = \mu\left( \{ x \in R \,:\, |h_n(x)| \geq \kappa\} \right)$ and $M(\kappa) = \sup_n M_n(\kappa)$. Let $t > 0$, we say that the family $\{h_n\}$ is $t$-\textbf{tight} if (a) $\{ x \in R \,:\, |h_n(x)| \geq \kappa\}$ is, for any $n$ and $\kappa$, a union of at most $K$ intervals and (b) $\lim_{\kappa \rightarrow \infty} \kappa^t M(\kappa) < \infty$. Clearly, if $\sup_n |h_n(x)|$ is bounded, then $h_n$ is $t$-tight for any $t$. 

Assumptions:
\begin{enumerate}
\item[A1] Suppose that $C \geq p(x), q(x) $.
\item[A2] Define $\alpha^2 = \int_R \frac{(p(x) - q(x))^2}{q(x)} dx$ and $\gamma(x) = \frac{p(x) - q(x)}{\alpha}$. Suppose $\left| \frac{\gamma(x)}{q(x)} \right| \leq M$ on $R$ for some constant $M$.
\item[A3] Suppose that $\frac{p'(x)}{\alpha}, \frac{q'(x)}{\alpha}, \frac{p'(x)}{q(x)}, \frac{q'(x)}{q(x)}$ are $t$-tight. 
\end{enumerate}

We will assume that $\alpha < 1$ so that $p'(x), q'(x)$ constitute a $t$-tight family as well. Note that A3 implies that $\gamma'(x)$ and $\frac{\gamma'(x)}{q(x)}$ are $t$-tight.

Now we will extend proposition~\ref{prop:continuous_renyi_order}, \ref{prop:discrete_renyi_order}, \ref{prop:convergence_discrete_continuous_renyi}, and theorem~\ref{thm:relative_convergence_discrete_continuous_renyi} under the above assumptions.

Although $p(x), q(x)$ are no longer densities when restricted to $R$, this is not a problem.We can address this issue by, instead of working with the Renyi divergence, working with Hellinger $\int_R (\sqrt{p(x)}-\sqrt{q(x)})^2 dx$ instead. 

Propositions~\ref{prop:continuous_renyi_order} and \ref{prop:discrete_renyi_order} follow immediately if we consider Hellinger distance instead of Renyi divergence. 

\subsubsection{Handling Proposition~\ref{prop:convergence_discrete_continuous_renyi}}

Suppose we have $L$ bins with bin width $B$. 

We say that a bin $l$ is good if 
$$
\sup_{x \in Bin_l} |q'(x)|, \sup_{x \in Bin_l} |\gamma'(x)|, 
\sup_{x \in Bin_l} \left| \frac{q'(x)}{q(x)} \right|, 
\sup_{x \in Bin_l} \left| \frac{\gamma'(x)}{q(x)} \right| \leq L^{\frac{1}{t+1}}
$$; 
the exponent is chosen to balance two error terms that follow later. We will now argue, by tightness, that the proportional of bad bins goes to 0 as $L \rightarrow \infty$.\\

Since $\left \{x \,:\, |q'(x)|, |\gamma'(x)|, |\frac{q'(x)}{q(x)}|, |\frac{\gamma'(x)}{q(x)}| \geq L^{\frac{1}{t+1}} \right \}$ is a union of at most $K$ intervals, we have that

\begin{align*}
B \# \left \{ l \,:\, |p'(x)|, |\gamma'(x)|, |\frac{q'(x)}{q(x)}|, 
                |\frac{\gamma'(x)}{q(x)}|   
           \geq L^{\frac{1}{t+1}} \right \} &\leq 
   \mu \left( \left\{x \,:\, |p'(x)|, |\gamma'(x)|, |\frac{q'(x)}{q(x)}|, 
                |\frac{\gamma'(x)}{q(x)}|  
         \geq L^{\frac{1}{t+1}} \right\} \right) + 2KB \\
  &\leq M( L^{\frac{1}{t+1}} ) + 2KB \\
  &\leq C_M L^{ - \frac{t}{t+1}} + 2 K B \\
  & \leq C_{M, K} L^{ - \frac{t}{t+1}}
\end{align*}

For each bin $l$, define $x_l = \sup_{x \in Bin_l} | q(x) |$ (attainable since $q$ is continuous).

Now, for a good bin, we have that 

\begin{align*}
Q_l &= \int_{\trm{Bin}_l} q(x) dx \\
  &= \int_{a_l}^{b_l} q(x) dx \\
 &= \int_{a_l}^{b_l} q(x_l) + q'(c_x) (x - x_l) dx \quad \trm{for some $c_x \in [a_l, b_l]$}\\
 &= B q(x_l) + \int_{a_l}^{b_l} q'(c_x)(x-x_l) dx  \\
 &= B q(x_l) + B^2 \xi_l \\
\end{align*}

where we define $\xi_l = \frac{1}{B^2} \int_{a_l}^{b_l} q'(c_x) (x-x_l) dx$. There are two things to note about $\xi_l$: first, 
$$B |\xi_l| \leq \frac{1}{B} \int_{a_l}^{b_l} L^{\frac{1}{t+1}} (x-x_l) dx \leq C' L^{- \frac{t}{t+1}}$$.
Second, 
$$B \frac{\xi_l}{q(x_l)} = \frac{1}{B} \int_{a_l}^{b_l} \frac{q'(c_x)}{q(x_l)} (x - x_l)dx 
                \leq \frac{1}{B} \int_{a_l}^{b_l} L^{\frac{1}{t+1}} (x - x_l) dx \leq 
 C' L^{- \frac{t}{t+1}} $$ 


For a bad bin, we have that
\begin{align*}
Q_l = \int_{Bin_l} q(x) dx \leq C B
\end{align*}
We perform like analysis on $\gamma$:
\begin{align*}
\gamma_L &= \int_{Bin_l} \gamma(x) dx \\
     &=  \int_{a_l}^{b_l} \gamma(x_l) + \gamma'(c_x) (x - x_l) dx  \\
     &= B \gamma(x_l) + B^2 \xi'_l
\end{align*}  

where $\xi'_l = \frac{1}{B^2} \int_{a_l}^{b_l} \gamma'(c_x)(x - x_l) dx$. It is easy to verify that $B |\xi'_l| \leq C' L^{ - \frac{t}{t+1}}$ and that $ B \frac{\xi'_l}{q(x_l)} \leq C' L^{-\frac{t}{t+1}}$. 


We also need to bound $\frac{\gamma_l}{Q_l}$ for the upcoming derivations. 
\begin{align*}
\frac{1}{Q_l} \int_{Bin_l} \gamma(x) dx &= \frac{1}{Q_l} \int_{Bin_l} \frac{\gamma(x)}{q(x)} q(x) dx \\
  &\leq \frac{1}{Q_l} \int_{Bin_l} M q(x) dx \\
  &\leq M
\end{align*}

Now, we have

\begin{align*}
d_L &= \sum_{l=1}^L Q_l \left( \frac{1}{2} \frac{\gamma_l}{Q_l} \right)^2 \\
  &= \sum_{l \, good}  B \left(
     (q(x_l) + B \xi_l) \left( \frac{1}{2} \frac{\gamma(x_l) + B \xi'_l}{q(x_l) + B \xi_l} \right)^2 \right) + \sum_{l \, bad} Q_l \left( \frac{1}{2} \frac{\gamma_l}{Q_l} \right)^2 \\
   &=   \sum_{l \, good}  B \left(
     (q(x_l) + B \xi_l) \left( \frac{1}{2} \frac{\gamma(x_l) + B \xi'_l}{q(x_l) + B \xi_l} \right)^2 \right) + C_{M,K} L^{-\frac{t}{t+1}}  \\
\end{align*}
We first focus our attention on the term $\left( \frac{\gamma(x_l) + B \xi'_l}{q(x_l) + B\xi_l} \right)^2$. We would like to show that this term is $\left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 + O(L^{-\frac{t}{t+1}})$. 

\begin{align*}
\left( \frac{\gamma(x_l) + B \xi'_l}{q(x_l) + B\xi_l} \right)^2 &= 
    \left( \frac{\gamma(x_l)}{q(x_l)} + B \frac{\xi'_l}{q(x_l)}  \right)^2 \left(\frac{1}{1 + B \frac{\xi_l}{q(x_l)}} \right)^2 \\
  &=  \left( \frac{\gamma(x_l)}{q(x_l)} + B \frac{\xi'_l}{q(x_l)}  \right)^2 \left(1 - \frac{1}{2} B \frac{\xi_l}{q(x_l)} \right)^2 \\
  &= \left( \frac{\gamma(x_l)}{q(x_l)} + B \frac{\xi'_l}{q(x_l)}  \right)^2 \left(1 - \frac{1}{2} B \frac{\xi_l}{q(x_l)} \right) 
\end{align*}




\begin{align*}
  d_L &= \sum_{l \, good} B q(x_l) \left( \frac{1}{2} \frac{\gamma(x_l)}{q(x_l)} \right)^2 + C' L^{-\frac{t}{t+1}}  + C_{M,K} L^{-\frac{t}{t+1}} \\
  &= \sum_{l=1}^L B q(x_l) \left( \frac{1}{2} \frac{\gamma(x_l)}{q(x_l)} \right)^2  C' L^{-\frac{t}{t+1}}  + 2 C_{M,K} L^{-\frac{t}{t+1}} \\
\end{align*}

Thus, $|d_L - d_R| = o(1)$. 

In like fashion, we can bound $| d_R - d|$. 



\newpage
\section{Unbounded Continuous Distributions}



Let $P, Q$ be unbounded continuous distributions. One strategy to handle unbounded distributions is to apply a monotone transform $\Phi: \R \rightarrow [0,1]$ to transform the random variables into the bounded distribution setting. Unfortunately, the transformed densities often do not satisfy the assumptions we applied in the previous section. We must relax some of those assumptions.

First, a lemma:
\begin{lemma}
Let $p, q : \mathcal{A} \rightarrow \R$ be two continuous densities and let $\Phi : \mathcal{A} \rightarrow \mathcal{B}$ be a strictly monotone function. Let $X, Y$ be two random variables drawn from $p,q$ respectively and let $\Phi(X), \Phi(Y)$ have densities $\tilde{p}, \tilde{q}$ respectively. Then we have that
\[
-2 \log \int \sqrt{p(x)q(x)} dx = -2 \log \int \sqrt{ \tilde{p}(z) \tilde{q}(z)} dz
\]
\end{lemma}

\begin{proof}
Because $\Phi$ is strictly monotone, it is almost everywhere differentiable. Thus, the density $\tilde{p}(z) = p(\Phi^{-1}(z)) \frac{1}{\Phi'(\Phi^{-1}(z))}$ and likewise for $\tilde{q}$. Therefore,

\begin{align*}
 -2 \log \int \sqrt{ \tilde{p}(z) \tilde{q}(z)} dz &=
    -2 \log \int \sqrt{ p(\Phi^{-1}(z)) q(\Phi^{-1}(z)) } \frac{1}{\Phi'(\Phi^{-1}(z))} dz \\
 &= -2 \log \int \sqrt{ p(x) q(x)} dx
\end{align*}

where the second equality follows with the change of variables $x = \Phi^{-1}(z)$ and 
$dx = \frac{dz}{\Phi'(\Phi^{-1}(z))}$. 
\end{proof}


\subsection{Transformation}

Let $\tilde{P}, \tilde{p}$ be the original CDF and density, likewise for $\tilde{Q}$ and $\tilde{q}$. 

Suppose we use transform $\Phi : \R \rightarrow [0,1]$. Let $\phi$ be the density of $\Phi$. 

Let $\tilde{X}$ be drawn from $\tilde{P}$, then the CDF of $\Phi(X)$ is 
\begin{align*}
P(x) &= \tilde{P}( \Phi(\tilde{X}) \leq x) \\
   &= \tilde{P}( \tilde{X} \leq \Phi^{-1}(x)) \\
   &= \tilde{P}( \Phi^{-1}(x))
\end{align*}

And the PDF of $\Phi(X)$ is 
\[
\frac{ d \tilde{P}( \Phi^{-1}(x))}{dx}  = \frac{\tilde{p}(\Phi^{-1}(x))}{\Phi'(\Phi^{-1}(x))} 
= \frac{\tilde{p}(\Phi^{-1}(x))}{\phi(\Phi^{-1}(x))}
\]

\subsubsection{Gaussian Example}

Suppose that $\tilde{X} \sim N(\theta, 1)$ and $\tilde{Y} \sim N(-\theta, 1)$ and the transform $\Phi$ is the CDF of a standard normal.

Then, $\Phi(\tilde{X})$ has the density: 
\begin{align*}
p(x) &= \frac{\exp\left( - \frac{1}{2} (\Phi^{-1}(x) - \theta)^2 \right)}{\exp\left( - \frac{1}{2} \Phi^{-1}(x)^2 \right)} \\
   &= \exp\left( \Phi^{-1}(x) \theta - \frac{1}{2} \theta^2 \right)
\end{align*}
and $\Phi(\tilde{Y})$ has the density:
\[
q(x) = \exp \left( -\Phi^{-1}(x) \theta - \frac{1}{2} \theta^2 \right)
\]

\subsubsection{Location Family}

Suppose that $\tilde{X} \sim \exp( f(x - \theta))$ and $\tilde{Y} \sim \exp(f(x))$ where $f$ is differentiable. 

Let $\Phi$ be the distribution function of some distribution with differentiable density $\phi$.

We will use the CDF $\Phi$ to perform the transformation $\Phi(\tilde{X})$ and $\Phi(\tilde{Y})$. 

Then, we have that
\begin{align*}
p(x) &=  \frac{\exp\left( f(\Phi^{-1}(x) - \theta) \right)}{\phi(\Phi^{-1}(x))} \\
q(x) &=  \frac{\exp\left( f(\Phi^{-1}(x)) \right)}{\phi(\Phi^{-1}(x))} \\
\frac{p(x)}{q(x)} &=\exp\left( f(\Phi^{-1}(x) - \theta) - f(\Phi^{-1}(x)) \right) 
\end{align*}


We will work out the derivative as well:

\begin{align*}
p'(x) &= \frac{1}{\phi(\Phi^{-1}(x))^2} \left[ f'(\Phi^{-1}(x) - \theta) \exp f(\Phi^{-1}(x)-\theta) -
                       \frac{\phi'(\Phi^{-1}(x))}{\phi(\Phi^{-1}(x))} \exp f(\Phi^{-1}(x) - \theta) \right] \\
q'(x) &= \frac{1}{\phi(\Phi^{-1}(x))^2} \left[ f'(\Phi^{-1}(x)) \exp f(\Phi^{-1}(x)) -
                       \frac{\phi'(\Phi^{-1}(x))}{\phi(\Phi^{-1}(x))} \exp f(\Phi^{-1}(x)) \right] \\
\end{align*}

These derivatives are complicated, but we can simplify the difference $p'(x) - q'(x)$ into something that is easier to digest. Recall that $\frac{p(x)}{q(x)}$ is the ratio $\frac{\exp f(\Phi^{-1}(x) - \theta)}{\exp f(\Phi^{-1}(x))}$. Define the remainder term $R(\Phi^{-1}(x), \theta) = f'(\Phi^{-1}(x) - \theta) - f'(\Phi^{-1}(x))$. 

\begin{align*}
p'(x) - q'(x) = & \frac{1}{\phi(\Phi^{-1}(x))^2} \left[ 
              f'(\Phi^{-1}(x) - \theta) \exp f(\Phi^{-1}(x) - \theta) - f'(\Phi^{-1}(x)) \exp f(\Phi^{-1}(x)) \right] \\
      &  - \frac{1}{\phi(\Phi^{-1}(x))^2} \frac{\phi'(\Phi^{-1}(x))}{\phi(\Phi^{-1}(x))} \left[ \exp f(\Phi^{-1}(x) - \theta) - \exp f(\Phi^{-1}(x)) \right] \\
    = & \frac{\exp f(\Phi^{-1}(x)) }{\phi(\Phi^{-1}(x))^2} \left[ 
             \frac{p(x)}{q(x)} R(\Phi^{-1}(x), \theta) + 
             \left(\frac{p(x)}{q(x)} - 1 \right) f'(\Phi^{-1}(x))  \right] \\
   & - \frac{\exp f(\Phi^{-1}(x)) }{\phi(\Phi^{-1}(x))^2} \frac{\phi'(\Phi^{-1}(x))}{\phi(\Phi^{-1}(x))}  \left[ \frac{p(x)}{q(x)} - 1  \right] \\
 = & \frac{q(x) }{\phi(\Phi^{-1}(x))} \left[ 
             \frac{p(x)}{q(x)} R(\Phi^{-1}(x), \theta) + 
             \left(\frac{p(x)}{q(x)} - 1 \right) f'(\Phi^{-1}(x))  \right] \\
   & - \frac{ q(x) }{\phi(\Phi^{-1}(x))} \frac{\phi'(\Phi^{-1}(x))}{\phi(\Phi^{-1}(x))}  \left[ \frac{p(x)}{q(x)} - 1  \right] 
\end{align*}



\subsection{Good Set and Bad Set}

Define the sets $R \subset [0,1]$ as below. Define $\alpha^2 = \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx$. 

$$
R = \left\{ x \in [0,1] \,:\, \frac{1}{\alpha} \left| \frac{p(x)}{q(x)} - 1 \right| 
  \leq M \right \}
$$

 $R$ is the good set and $R^c$ is the bad set. 


We make two further assumptions:

\begin{enumerate}
\item[A4] $R^c$ is a union of at most $K$ intervals and $\mu(R^c) = o(I)$ where $\mu$ is the Lebesgue measure.
\item[A5] $\int_{R^c} \frac{p(x)}{q(x)} dx = o(I)$
\end{enumerate}

These are in addition to the previous assumptions.

\begin{enumerate}
\item[A1] Suppose that $p(x), q(x) \leq C$ on the whole $[0,1]$.
\item[A2] Define $\alpha_R^2 = \int_R \frac{(p(x) - q(x))^2}{q(x)}dx$ and 
          $\gamma(x) = \frac{p(x)-q(x)}{\alpha_R}$. Suppose that $| \frac{\gamma(x)}{q(x)} | \leq M$ for all $x \in R$. 
\item[A3] Suppose that $\frac{p'(x)}{\alpha}, \frac{q'(x)}{\alpha}, \frac{p'(x)}{q(x)}, \frac{q'(x)}{q(x)}$ are $t$-tight over $R$. 
\end{enumerate}

\begin{proposition}
A1, A4, and A5 together implies the following:
\begin{align*}
\int_{R^c} (\sqrt{p(x)} - \sqrt{q(x)})^2 dx &= o\left( \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right)   \\
\int_{R^c} \frac{(p(x) - q(x))^2}{p(x)} dx &= o\left( \int \frac{(p(x) - q(x))^2}{p(x)} dx \right) 
\end{align*}
\end{proposition}


\begin{proof}

\begin{align*}
\int_{R^c} (\sqrt{p(x)} - \sqrt{q(x)})^2 dx &\leq \int_{R^c} p(x) + q(x) - 2\sqrt{p(x)q(x)} dx \\
  &\leq \mu(R^c) C \\
  &\leq o(I)
\end{align*}
The chi-squared distance bound follows in the same way.

\end{proof}


Now we have to take care of how $R^c$ interacts with the binning. What is tricky about this task is that if $R^c$ is smaller than the width of a single bin, we cannot simply throw away the entire bin containing $R^c$. 

We divide our analysis into two cases, $ IL \rightarrow c < \infty$ and $IL \rightarrow \infty$. In the second case, $\frac{1}{L} = o(I)$, which means each bin has small width and we can throw away the bins that intersect $R^c$ -- we would throw away at most a small number of bins. 

In the first case, for each bin $l$, we define $Bin'_l = Bin_l \cap R$ and $Bin''_l = Bin_l \cap R^c$. Since $B = \frac{1}{L} = \Omega(I)$ and $\mu(R^c) = o(I)$, it must be that $\mu(Bin'_l ) = B( 1 - \mu(R^c) L) = B(1 - o(1))$. 

Then, we have that 

$$
(\sqrt{P_l} - \sqrt{Q_l})^2 \leq  
$$

The argument proceeds roughly as follows:
\begin{align*}
& \left| \sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2 - \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| \\
&= \left| \sum_{l \in R_1} ( \sqrt{P_l} - \sqrt{Q_l} )^2 + \sum_{l \in R_2} (\sqrt{P_l} - \sqrt{Q_l})^2 - 
    \int_{R_1} (\sqrt{p(x)} - \sqrt{q(x)})^2 dx - \int_{R_2} (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| \\
&\leq \left| \sum_{l \in R_1} (\sqrt{P_l} - \sqrt{Q_l})^2 - \int_{R_1} (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| + 
    \left| \sum_{l \in R_2} (\sqrt{P_l} - \sqrt{Q_l})^2 \right| + \left| \int_{R_2} (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right|
\end{align*}

Each of the term above then is $o \left( \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right)$. 

\subsection{Verifying Assumptions under the Gaussian Setting}

Let $p(x)$ and $q(x)$ be the transformed Gaussian densities and let us see what the sets $R_1, R_2$ looks like. 

\subsubsection{Properties of $\Phi, \Phi^{-1}$}

\begin{lemma}
For $\tilde{x} \geq 0$, we have that $ \Phi(\tilde{x}) = 1 - \Phi(-\tilde{x})$.\\

For $x \geq 1/2$, we have that $\Phi^{-1}(x) = - \Phi^{-1}(1- x)$. 
\end{lemma}

\begin{proof}
Let $\tilde{x} \geq 0$, then 
\begin{align*}
\Phi(\tilde{x}) = P(\tilde{X} \leq \tilde{x}) = 1 - P(\tilde{X} \geq \tilde{x}) = 1 - P(\tilde{X} \leq - \tilde{x}) = 1 - \Phi(-\tilde{x}) 
\end{align*}

Now, suppose $x \geq 1/2$. Then $\Phi^{-1}(x) \geq 0$.
\begin{align*}
x &= \Phi( \Phi^{-1}(x) ) \\
x &= 1 - \Phi( - \Phi^{-1}(x)) \\
1 - x &= \Phi( - \Phi^{-1}(x)) \\
- \Phi^{-1}(1-x) &= \Phi^{-1}(x)
\end{align*}

\end{proof}

\begin{lemma}

For all $\tilde{x} \in [-\infty, -2]$ and $x \in [0, 0.018]$, we have that
\begin{align*}
\exp(-\frac{\tilde{x}^2}{2} ) \geq & \Phi(\tilde{x}) \geq \exp( - \tilde{x}^2) \\
-\sqrt{2 \log \frac{1}{x} } \leq & \Phi^{-1}(x) \leq -\sqrt{ \log \frac{1}{x} } 
\end{align*}
For all $\tilde{x} \in [2, \infty]$ and for all $x \in [0.982, 1]$, we have that
\begin{align*}
1 - \exp(-\tilde{x}^2/2) \leq & \Phi(\tilde{x}) \leq 1 - \exp(-\tilde{x}^2) \\
\sqrt{ 2 \log \frac{1}{1-x}} \geq & \Phi^{-1}(x) \geq \sqrt{ \log \frac{1}{1-x} } 
\end{align*}

\end{lemma}

\begin{proof}

First we handle the lower bound.

Define $\zeta(\tilde{x}) = \exp(- \tilde{x}^2)$, it is clear that $\zeta(\tilde{x})$ is monotonically increasing for all $\tilde{x} \in [-\infty, 0]$ and $\zeta \,:\, [-\infty, 0] \rightarrow [0, 1]$. Thus, for $x \in [0, 1]$, it is well-defined to state that $\zeta^{-1}(x) = -\sqrt{ \log \frac{1}{x} }$. 

Now, suppose $\tilde{x} \leq 0$, then we have that
\[
\Phi(\tilde{x}) \geq \frac{1}{\sqrt{2\pi}} \frac{|\tilde{x}|}{|\tilde{x}|^2 + 1} \exp( - \tilde{x}^2 / 2) 
\] 

To see that $\Phi(\tilde{x}) \geq \zeta(\tilde{x})$, observe the following chain of equivalent statements:
\begin{align*}
\frac{1}{\sqrt{2\pi}} \frac{|\tilde{x}|}{|\tilde{x}|^2 + 1} \exp( - \tilde{x}^2 / 2) &\geq \exp( - \tilde{x}^2) \\
\log \frac{|\tilde{x}|}{|\tilde{x}|^2 + 1} + \log \frac{1}{\sqrt{2 \pi}} - \frac{\tilde{x}^2}{2} &\geq - \tilde{x}^2 \\
\frac{\tilde{x}^2}{2} &\geq - \log \frac{|\tilde{x}|}{|\tilde{x}|^2 + 1} - \log \frac{1}{\sqrt{2\pi}} \\
\frac{\tilde{x}^2}{2} &\geq \log \left( |\tilde{x}| + \frac{1}{|\tilde{x}|} \right) - \frac{1}{2} \log 2 \pi
\end{align*}

It is easy to verify numerically that the last statement holds for all $\tilde{x} \leq -2$. 

Now, suppose $x \leq 0.018 < \zeta(-2)$. It is clear then that $\zeta^{-1}(x) \leq -2$ and therefore $\Phi(\zeta^{-1}(x)) \geq \zeta(\zeta^{-1}(x))$ by what we just proved. We then have that
\[
\Phi(\Phi^{-1}(x)) = \zeta(\zeta^{-1}(x)) \leq \Phi( \zeta^{-1}(x)) 
\]

Since $\Phi$ is monotonically increasing, we conclude that $\Phi^{-1}(x) \leq \zeta^{-1}(x)$. 

Now, for the upper bound. We know that, for all $\tilde{x} \leq 0$, that
\[
\frac{1}{\sqrt{2\pi}} \frac{1}{|\tilde{x}|} \exp\left( - \frac{\tilde{x}^2}{2} \right) \geq \Phi(\tilde{x})
\]

For $\tilde{x} \leq -1$, it is clear that $\zeta_2(\tilde{x}) = \exp\left( - \frac{\tilde{x}^2}{2} \right) \geq \Phi(\tilde{x})$. $\zeta_2(\tilde{x})$ is monotonically increasing in $[-\infty, 0]$ and its inverse exists: $\zeta_2 : [0, 1/2] \rightarrow [-\infty, 0]$, $\zeta_2^{-1}(x) = -\sqrt{2 \log \frac{1}{x}}$. 

Suppose $x \leq 0.1 \leq \Phi(-1) \leq \zeta_2(-1)$. Then, $\Phi^{-1}(x) \leq -1$ and so $\Phi( \Phi^{-1}(x)) \leq \zeta_2 (\Phi^{-1}(x))$. 
\[
x = \zeta_2(\zeta_2^{-1}(x)) \leq  \zeta_2(\Phi^{-1}(x)) 
\]
Thus, $\zeta_2^{-1}(x) \leq \Phi^{-1}(x)$. 

The second statements hold by the previous lemma. 

\end{proof}


\subsubsection{Left Side}

Let us first consider the set $A = \{ x \,:\, p(x) \leq \frac{1}{2}\}$. 

\begin{align*}
\exp\left( \Phi^{-1}(x) \theta - \frac{1}{2} \theta^2 \right) &\leq \frac{1}{2}  \\
\Phi^{-1}(x) \theta - \frac{1}{2} \theta^2 &\leq \log \frac{1}{2} \\
\Phi^{-1}(x) &\leq \frac{1}{\theta} \left(\log \frac{1}{2} + \frac{1}{2} \theta^2\right) 
\end{align*}

$\Phi^{-1}(x) \rightarrow -\infty$ as $x \rightarrow 0$. Suppose we can upper bound $\Phi^{-1}(x)$ on the segment $(-\infty, 0]$ with some other function $\zeta(x)$. 
Then, the set $\{ x \,:\, \zeta(x) \leq \frac{1}{\theta} \left( \log \frac{1}{2} + \frac{1}{2} \theta^2 \right) \}$ is a superset of $A$. 

 Let us assume that $\theta$ is small enough such that $\log \frac{1}{2} + \frac{1}{2} \theta^2 < 0$. 
A superset of $A = \{ x \,:\, p(x) \leq \frac{1}{2} \}$ is the set of $x$ such that

\begin{align*}
-\sqrt{\log \frac{1}{x}} &\leq \frac{1}{\theta} \left( \log \frac{1}{2} + \frac{1}{2}\theta^2 \right) \\
\log \frac{1}{x} &\geq \frac{1}{\theta^2} \left( \log \frac{1}{2} + \frac{1}{2} \theta^2 \right)^2 \\
x & \leq \exp \left( - \frac{1}{\theta^2} \left( \log \frac{1}{2} + \frac{1}{2} \theta^2 \right)^2 \right)
\end{align*}

It is clear that as $\theta \rightarrow 0$, $x$ goes to 0 rapidly. 

\subsubsection{Right Side}

Now let us suppose that $A = \{ x \,:\, p(x) \geq 1.5\}$. We can characterize $A$ as before:
\begin{align*}
\exp( \Phi^{-1}(x) \theta - \frac{1}{2} \theta^2 ) &\geq 1.5 \\
\Phi^{-1}(x) \theta - \frac{1}{2} \theta^2 &\geq \log 1.5 \\
\Phi^{-1}(x) &\geq \frac{ \log 1.5 + \frac{1}{2}\theta^2}{\theta} 
\end{align*}

Since $\Phi^{-1}(x) \rightarrow \infty$ as $x \rightarrow 1$, we are looking at the right end of the $[0,1]$ interval. We need a lower bound on $\Phi^{-1}(x)$, which translates to an upper bound on $\Phi(\tilde{x})$.

Suppose $\tilde{x} \geq 2$, then:
\[
\Phi(\tilde{x}) = 1 - \Phi(- \tilde{x}) \leq 1 - \exp( - \tilde{x}^2) = \zeta(\tilde{x})
\]

Setting $x = \zeta(\tilde{x})$, we get that $\tilde{x} = \sqrt{ \log \frac{1}{1-x}} = \zeta^{-1}(x)$. And so, for $x \in [0.982, 1]$, we have that $\Phi^{-1}(x) \geq \zeta^{-1}(x)$. Therefore, a sufficient condition for $x \in A$ is that $\zeta^{-1}(x) \geq \frac{ \log 1.5 + \frac{1}{2}\theta^2}{\theta}$. 

\begin{align*}
\sqrt{\log \frac{1}{1-x}} &\geq \frac{\log 1.5 + \frac{1}{2} \theta^2}{\theta} \\
 \log \frac{1}{1-x} &\geq \left(  \frac{\log 1.5 + \frac{1}{2} \theta^2}{\theta} \right)^2 \\
 1-x &\leq \exp\left( - \left(  \frac{\log 1.5 + \frac{1}{2} \theta^2}{\theta} \right)^2 \right) \\
 x &\geq 1 -  \exp\left( - \left(  \frac{\log 1.5 + \frac{1}{2} \theta^2}{\theta} \right)^2 \right)
\end{align*}

\subsubsection{Derivatives}



\subsubsection{Divergence Verification}

If $\tilde{p}$, $\tilde{q}$ are the densities for, respectively, $N(\theta, 1), N(-\theta, 1)$, we have that
\[
\int ( \sqrt{p(x)} - \sqrt{q(x)} )^2 dx = 2 \theta^2
\]

Our aim then is to show that 
\[
\int_{R_2}  ( \sqrt{p(x)} - \sqrt{q(x)} )^2 dx = o \theta^2 
\]

\begin{align*}
\int_{R_2}  ( \sqrt{p(x)} - \sqrt{q(x)} )^2 dx \leq \int_{R_2} p(x) dx + \int_{R_2} q(x) dx
\end{align*}

Thus, we focus our attention on $\int_{R_2} p(x) dx$ and $\int_{R_2} q(x) dx$. Again, 

Define $A = \{ x \,:\, p(x) \geq 1.5\}$ and we will bound $\int_A p(x) dx$. 

We have already characterized $A$ before. Suppose $\theta$ is small enough such that $A \subset [0.98, 1]$. 

\begin{align*}
\int_A p(x) dx &= \int_A \exp( \Phi^{-1}(x) \theta - \frac{1}{2}\theta^2 ) dx \\ 
   &\leq \int_A \exp \left( \sqrt{2 \log \frac{1}{1-x}} \theta - \frac{1}{2} \theta^2 \right) dx \\
   &= \int_{1-A} \exp\left( \theta \sqrt{2 \log \frac{1}{x} } - \frac{1}{2} \theta^2 \right) dx \\
   &= \int_{1-A} \left(\frac{1}{x}\right)^{\frac{2\theta}{\sqrt{2 \log \frac{1}{x}}}} \exp(-\frac{1}{2} \theta^2) dx
\end{align*}

Now suppose $\theta$ is large enough such that $\sqrt{2 \log \frac{1}{x}} \geq 2$ for all $x \in A$. 

Then, we have that
\begin{align*}
 & \int_{1-A} \left(\frac{1}{x}\right)^{\frac{2\theta}{\sqrt{2 \log \frac{1}{x}}}} \exp(-\frac{1}{2} \theta^2) dx \\
&\leq \int_{1-A} \left( \frac{1}{x} \right)^{\theta} \exp(-\frac{1}{2}\theta^2) dx \\
&\leq \frac{1}{1 - \theta} x^{1-\theta} \Big|^{c}_0 \\
&\leq \frac{1}{1-\theta} c^{1-\theta} 
\end{align*}
where $c$ is the length of the interval $A$. 

\newpage
\section{Technical Lemmas}

The following two lemmas give an alternative form to the Renyi-divergence. 

\begin{lemma}
\label{lem:simplify_renyi2}
Let $P = \{ P_l \}_{l = 0,..., \infty}$ and $Q = \{ Q_l \}_{l=0,...,\infty}$ be two discrete distributions and suppose that $c_1 \leq \frac{P_l}{Q_l} \leq c_2$ for all $l$. 

Let $I = - 2 \log \sum_l \sqrt{ P_l Q_l}$ and let $H = \frac{1}{2} \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2$. 

Then, we have that $H \leq 1 - c_1$ and that
\[
I = (1 + C_H) \sum_{l = 0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 
\]
where $| C_H | \leq \frac{H}{2 c_1} \leq \frac{1-c_1}{2c_1}$. 

Furthermore, we have that
\begin{align*}
I = C_I \sum_{l=0}^\infty \frac{\Delta_l^2}{P_l \vee Q_l} 
\end{align*}

where $2(1 + C_H)  \geq C_I \geq  1 + C_H$.
\end{lemma}

As a consequence, if $H \rightarrow 0$, then we have that $I = \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2$. 

\begin{proof}

\begin{align*}
H &= \frac{1}{2} \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 \\
 &= 1 - \sum_{l=0}^\infty \sqrt{P_l Q_l} \\
 &\leq 1 - \sum_{l=0}^\infty Q_l c_1 \\
 &\leq 1 - c_1
\end{align*}
The first inequality follows from $P_l \geq c_1 Q_l$. 

Then, we have that
\begin{align*}
I &= - 2 \log (1 - H) \\
  &= 2 (1 + C_H) H \\
  &= (1 + C_H) \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 
\end{align*}

where $|C_H| \leq \frac{H}{2c_1}$. 

To derive the second claim, one calculates
\begin{align*}
I &= (1 + C_H) \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 \frac{(\sqrt{P_l} + \sqrt{Q_l})^2}{(\sqrt{P_l} + \sqrt{Q_l})^2} \\
  &= (1 + C_H) \sum_{l=0}^\infty \frac{(P_l - Q_l)^2}{(\sqrt{P_l} + \sqrt{Q_l})^2}
\end{align*}

The claim follows from the fact that $(P_l \vee Q_l) \leq (\sqrt{P_l} + \sqrt{Q_l})^2 \leq 2 (P_l \vee Q_l)$

\end{proof}


\begin{lemma}
\label{lem:simplify_renyi}
Let $P = \{ P_l \}_{l = 0,..., \infty}$ and $Q = \{ Q_l \}_{l=0,...,\infty}$ be two discrete distributions and suppose $P_0, Q_0 \rightarrow 1$. 

Let $I = - 2 \log \sum_l \sqrt{ P_l Q_l}$.

Then, we have that $I \rightarrow 0$ and 
\[
I = (1 + o(1)) \sum_{l = 1}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 
\]

\end{lemma}

\begin{proof}
First, it is clear that if $P_0, Q_0 \rightarrow 1$, then 
\begin{align*}
\sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 &= 
   (\sqrt{P_0} - \sqrt{Q_0})^2 + \sum_{l=1}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 \\
 &= (\sqrt{P_0} - \sqrt{Q_0})^2 + \sum_{l=1}^\infty P_l + \sum_{l=1}^\infty Q_l - 
  2 \sum_{l=1}^\infty \sqrt{P_l Q_l} \\
 &\leq  (\sqrt{P_0} - \sqrt{Q_0})^2 + \sum_{l=1}^\infty P_l + \sum_{l=1}^\infty Q_l \\
\end{align*}

Therefore, $\lim_{n\rightarrow \infty} \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 = 0$. 

\begin{align*}
I &= -2 \log \sum_{l=0}^\infty \sqrt{P_l Q_l} \\
  &= -2 \log \left( 1 - \frac{1}{2} \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 \right) \\ 
  &= (1 + o(1)) \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 \quad \trm{(since the sum tends to 0)}
\end{align*}

We will show that $(\sqrt{P_0} - \sqrt{Q_0})^2 = o \left( \sum_{l=1}^\infty (\sqrt{P_l} - \sqrt{Q_l} )^2 \right)$ and the result follows immediately.

Let $P' = 1 - P_0$ and $Q' = 1 - Q_0$. 
\begin{align*}
(\sqrt{P_0} - \sqrt{Q_0})^2 &= (\sqrt{1-P'} + \sqrt{1-Q'})^2 \\
  &= (1-P') \left( 1 - \sqrt{ \frac{1-Q'}{1-P'}} \right)^2 \\
 &= (1-P') \left( 1 - \sqrt{ 1 - \frac{Q' - P'}{1 - P'} } \right)^2 \\
 &\leq (1-P') \left( 1 - (1 - \frac{1}{2} \left( \frac{Q' - P'}{1 - P'} \right) (1+o(1)) ) \right)^2 \\
 &\leq (1 - P') \left( \frac{1}{2} \left( \frac{Q'-P'}{1-P'} \right) (1+o(1)) \right)^2 \\
 &\leq \frac{1}{4} \left( \frac{Q' - P'}{1 - P'} \right)^2 (1 + o(1)) \leq \frac{1}{4} ( Q' - P')^2 (1 + o(1)) 
\end{align*}

\begin{align*}
\sum_{l=1}^\infty (\sqrt{P_l}  - \sqrt{Q_l})^2 &= \sum_{l=1}^\infty P_l + Q_l - 2 \sqrt{P_lQ_l} \\
 &\geq P' + Q' - 2 \sqrt{ \left( \sum_{l=1}^\infty P_l \right) \left( \sum_{l=1}^\infty Q_l \right) } \\
 &= P' + Q' - 2\sqrt{P' Q'} \\
 &= (\sqrt{P'} - \sqrt{Q'})^2 \\
 &= P' \left( 1 - \sqrt{ \frac{Q'}{P'} } \right)^2 \\ 
 &= P' \left( 1 - \sqrt{ 1 - \frac{P' - Q'}{P'} } \right)^2 \\
 &\geq P' \left( 1 - ( 1 - \frac{1}{2} \frac{P' - Q'}{P'} (1 + o(1)) ) \right)^2 \\
 &\geq P' \left( \frac{1}{2} \frac{P' - Q'}{P'} (1 + o(1)) \right)^2 \\
 &\geq \frac{1}{4} \left( \frac{(P' - Q')^2}{P'} \right) (1 + o(1)) 
\end{align*}

Thus, we have shown that 
\begin{align*}
(\sqrt{P_0} - \sqrt{Q_0})^2& \leq \frac{1}{4} (Q' - P')^2 (1 + o(1)) \\
\sum_{l=1}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 & \geq \frac{1}{4} \frac{(P' - Q')^2}{P'} (1 + o(1))
\end{align*}

Since $P' \rightarrow 0$, the proof is complete.

\end{proof}

\section{Reference Results}

\subsection{Existing Results from Literature}

Let $\Theta_0(n, k, p, q, \beta)$ be the parameter space of homogeneous stochastic block model with $p$ as the within-cluster probability and $q$ as the between-cluster probability. The following theorem follows from Theorem 3 and Proposition 1 of \cite{gao2015achieving}. 

\begin{theorem}
\label{thm:spectral_rate}
Assume $p \leq C_1 q$ and that $p, q = \Omega( \frac{1}{n} )$ and suppose that $n \geq 2 \beta k$. Suppose there is some $c \in (0,1)$ such that 

$$ \frac{k^3 p}{(p-q)^2 n^2} \leq c$$. 

Suppose we apply Unnormalized-Spectral-Clustering with trim constant $\tau = C_2 \bar{d}$ and a sufficiently small post-processing constant $\mu > 0$. Then, for any constant $C'$, there exists some $C > 0$ dependent only on $C', C_1, C_2, \mu$ such that

\[
l(\hat{\sigma}, \sigma_0) \leq C \frac{\beta^2 k^2 p}{(p-q)^2 n }
\]

with probability at least $1 - n^{-C'}$. 

\end{theorem}

We note that $\frac{(p-q)^2}{p} = I$. Restated, this theorem says that if $p \asymp q$ and if $ \frac{k}{nI} \rightarrow 0$, then, the error rate $\gamma$ of spectral clustering goes to zero with probability $1 - n^{-C'}$ for any constant $C' > 0$. 




\bibliographystyle{plain}
\bibliography{note}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
