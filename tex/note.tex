\documentclass{article}
\usepackage{minx_math}
\usepackage[top=3.5cm, bottom=3.5cm, left=3.5cm, right=3.5cm]{geometry}
\begin{document}

\title{Detecting Communities on Colourful and Weighted Graphs}
\maketitle

\begin{abstract}
\centering
\noindent 
A graph is known, edges and nodes, \\
communities hide within its folds. \\
Stochastic blocks the model is, \\
weighted edges a novel goal.
\end{abstract}

\tableofcontents


\newpage
\section{Preliminary}

Suppose we have a graph of $n$ nodes and each edge may take on any of $L$ colors. The graph is generated by a stochastic block model with $K$ clusters. Each within-cluster edge takes on color $l \in \{1,...,L\}$ with probability $P_l$ and each between-cluster edge with probability $Q_l$. We suppose that $P_l, Q_l \rightarrow 0$ so that the graph is sparse. Let $n_k$ be the size of cluster $k$, we will suppose that $\frac{n}{\beta K} \leq n_k \leq \frac{\beta n}{K}$ for some $\beta \geq 1$. \\

The goal is to recover the clustering. We want to estimate $\hat{\sigma} : \{1,...,n\} \rightarrow \{1,...,K\}$ such that $\min_{\tau \in S_K} d_H( \hat{\sigma}, \tau \circ \sigma)$ is small where $S_K$ is the permutation group over $K$ elements.\\

We suppose that the number of colors $L$ is finite and that the probabilities $P_l, Q_l$ are unknown to us. We refer to the setting where $P_l, Q_l$ are known as the \emph{oracle setting}. 

We define the Renyi divergence between $P,Q$ as
\[
I_{tot} = -2 \log \sum_l \sqrt{P_l Q_l}
\]

\textbf{Our goal is to show that all results (i.e., weak consistency rates, strong consistency thresholds) that hold under the oracle setting also hold when the distribution $\{P_l, Q_l\}$ is unknown. }

\section{Weak Recovery Under the Oracle Setting}

\cite{zhangminimax} characterizes the minimax rate of weak recovery for Bernoulli $P,Q$. Their results and proofs can be extended in a straightforward manner to general discrete $P,Q$ under the oracle setting. 

\begin{proposition} 
\label{prop:weak_recovery_oracle}
(Oracle Setting Upper bound)
Assume $\frac{n I_{tot}}{K \log K} \rightarrow \infty$. The maximum likelihood estimator $\hat{\sigma}$ in the oracle setting achieves:
\[
\sup_{\Theta(n, K, \beta, P, Q)} \E r(\hat{\sigma}, \sigma) \leq \left\{ 
    \begin{array}{cc} 
   \exp \left( - (1 + o(1)) \frac{nI_{tot}}{2} \right ), \, & K=2, \\
   \exp \left( - (1 + o(1)) \frac{nI_{tot}}{\beta K} \right ), \,& K\geq 3
  \end{array} \right. 
\]

\end{proposition}

\begin{proof}
Proof of this proposition follows that of Theorem 3.2 in~\cite{zhangminimax}. We describe only the parts that need to be modified. 

Because we have a different likelihood function, our $T(\sigma)$ takes on a different form from that of~\cite{zhangminimax} at the bottom of page 8:
\[
T(\sigma) = \sum_{i<j} \mathbf{1}_{\sigma(i) = \sigma(j)} \sum_{l=0}^L \log \frac{P_l}{Q_l} \mathbf{1}_{A_{ij} = l} 
\]

Let $\sigma_0$ denote the true community assignment. We make a mistake if for some other community assignment $\sigma$, we get $T(\sigma) > T(\sigma_0)$. The key part of the proof is to bound

$$
P_m \equiv P\left(\exists \sigma \,:\,  T(\sigma) > T(\sigma_0),\, d_H(\sigma, \sigma_0) = m \right)
$$

To that end, we bound the probability of error of a fixed $\sigma$ $m$-distant from $\sigma_0$ in Hamming distance. We will prove an analogue of Proposition 5.1 in~\cite{zhangminimax}:

Let $\sigma$ be an arbitrary assignment satisfying $d(\sigma, \sigma_0) = m$. Let $X_i, Y_i$ be random variables such that
\[
X_i = \log \frac{P_l}{Q_l} \trm{ w.p. $P_l$} \qquad 
Y_i = \log \frac{P_l}{Q_l} \trm{ w.p. $Q_l$} 
\]
and $\alpha, \gamma$ be integers where
\[
\alpha = | \{ (i,j) \,:\, \sigma_0(i) = \sigma_0(j) \wedge \sigma(i) \neq \sigma(j) \} | 
\quad
\gamma = | \{ (i,j) \,:\, \sigma_0(i) \neq \sigma_0(j) \wedge \sigma(i) = \sigma(j) \} | 
\]

Then
\begin{align}
P( T(\sigma) \geq T(\sigma_0) ) \leq 
  P\left( \sum_{i=1}^\alpha X_i - \sum_{i=1}^\gamma Y_i < 0 \right) \leq 
  \exp( - \frac{\gamma + \alpha}{2} I) \label{eqn:new_prop51}
\end{align}

Lemma 5.3 from~\cite{zhangminimax} bounds $\alpha, \gamma$ and Proposition 5.2 bounds the number of $\sigma$'s (up to equivalent classes) such that $d_H(\sigma, \sigma_0) = m$. These pieces together bounds $P_m$. The rest of the proof follows~\cite{zhangminimax} exactly starting from Page 16. \\

We devote the rest of the proof toward proving equation~\ref{eqn:new_prop51}.

\begin{align*}
T(\sigma_0) - T(\sigma') = &
   \sum_{i<j} \mathbf{1}_{\sigma_0(i) = \sigma_0(j) \wedge \sigma'(i) \neq \sigma'(j)} \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log \frac{P_l}{Q_l} \\
    &- \sum_{i<j} \mathbf{1}_{\sigma_0(i) \neq \sigma_0(j) \wedge \sigma'(i) = \sigma'(j)} \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log \frac{P_l}{Q_l} \\
 &=  \sum_{i=1}^\alpha X_i - \sum_{i=1}^\gamma Y_i 
\end{align*}


\begin{align*}
P( \sum_{i=1}^\gamma Y_i - \sum_{i=1}^\alpha X_i > 0 ) &\leq 
  \E \left( e^{- t \sum_{i=1}^\alpha X_i} e^{t \sum_{i=1}^\gamma Y_i} \right) \\
 & \leq  \E \left( e^{ - t X_1 \alpha} e^{ t Y_1 \gamma} \right ) \\
 &\leq \left( \E e^{ - t X_i} \E e^{t Y_1} \right)^{(1-w)\alpha + w \gamma} 
       \frac{ \left( \E e^{t Y_1} \right)^{(1-w)(\gamma - \alpha)} }
            { \left( \E e^{-t X_1} \right)^{w (\gamma - \alpha)} }
\end{align*}

We will show that when $t=1/2$ and $w=1/2$, the fraction term equals $1$ and the first term equals $\exp\left( - (1/2 \alpha + 1/2 \gamma) I \right)$. 

Note that 
\begin{align*}
\E e^{ -t X_1} &= \sum_l P_l e^{ - t \log \frac{P_l}{Q_l}} \\
   &= \sum_l P_l \left( \frac{Q_l}{P_l} \right)^t \\ 
   &= \sum_l \sqrt{P_l Q_l} \quad \trm{(if $t=1/2$)}
\end{align*}

\begin{align*}
\E e^{ t Y_1} &= \sum_l Q_l e^{ t \log \frac{P_l}{Q_l} } \\
   &= \sum_l Q_l \left( \frac{P_l}{Q_l} \right)^t \\
   &= \sum_l \sqrt{P_l Q_l} \quad \trm{if $t=1/2$}
\end{align*}

\begin{align*}
P( \sum_{i=1}^\gamma Y_i - \sum_{i=1}^\alpha X_i > 0) &\leq 
    \left( \sum_l \sqrt{P_lQ_l} \right)^{\alpha + \gamma} \\
   &\leq  \exp( - \frac{1}{2} I)^{\alpha + \gamma} \\
   &\leq \exp\left( - \frac{(\alpha+\gamma)}{2}  I \right)
\end{align*}
   

\end{proof}

\section{Weak Recovery in the General Setting}

\begin{proposition}
\label{prop:consistency_possible}
 If weak recovery (consistency) is possible under the oracle setting, then it is possible when $P,Q$ are unknown.
\end{proposition}

\begin{proof}
From proposition~\ref{prop:weak_recovery_oracle}, we know that weak recovery is possible iff $\frac{n I_{tot}}{K \log K} \rightarrow \infty$. 

From Lemma~\ref{lem:simplify_renyi}, we have
\begin{align*}
I_{tot} &= (1+o(1))\sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2 
\end{align*}


Since $I_{tot} = \omega( \frac{K \log K}{n} )$ by hypothesis, it must be that, for some $l$, $(\sqrt{P_l} - \sqrt{Q_l})^2 = \omega( \frac{K \log K}{n} )$. 

We choose such an $l$ and consider an estimator $\hat{\sigma}_l$ that uses only the information $\mathbf{1}_{A_{ij} = l}$. 

Since the Renyi-divergence $I_l$ of $Ber(P_l)$ and $Ber(Q_l)$ is
\[
I_l = (1+o(1)) \left( (\sqrt{P_l} - \sqrt{Q_l})^2 + (\sqrt{1-P_l} - \sqrt{1-Q_l})^2 \right)
\]
We have that $\frac{ n I_l}{K \log K} \rightarrow \infty$ and weak consistency is thus achievable with $\hat{\sigma}_l$. 

\end{proof}

Although weak consistency is achievable with the estimator $\hat{\sigma}_l$ that considers only $\mathbf{1}_{A_{ij} = l}$, the estimator converges at $\exp( - \frac{n I_l}{\beta K})$ and therefore does not converge at the same rate. It is easy to see that $I_l \leq I_{tot} \leq L I_l (1 - o(1))$ where the second inequality holds as equality under some cases. 

\section{Rate Optimal Recovery}

We propose the following algorithm to recover the clusters in the general setting. We proceed in two stages: first, we identify a color $l$ that can provide consistent recovery by itself (such $l$ must exist by proposition~\ref{prop:consistency_possible}), and second, we use $\sigma^l$ -- a clustering based on $l$ -- to estimate $\{ \hat{P}_l, \hat{Q}_l \}$ and then use the estimates to refine the clustering. The second stage closely follows the algorithm from \cite{gao2015achieving}. \\

\textbf{Stage 1. Identify a consistent color}
\begin{enumerate}
\item For each color $l$:
  \begin{enumerate}
   \item Perform spectral clustering on $\tilde{A}_{ij} \equiv \mathbf{1}(A_{ij} = l)$ to get $\sigma^l$.
   \item Estimate $\hat{P}_l, \hat{Q}_l$ via counts from $\sigma^l$. 
   \item Estimate $\sqrt{I_l}$ via 
  $\sqrt{ \hat{I}_l } \equiv \frac{| \hat{P}_l - \hat{Q}_l |}{\sqrt{ \hat{P}_l \vee \hat{Q}_l}}$. 
   \end{enumerate}
\item Discard from $L$ all colors $l$ such that $\sqrt{\hat{I}_l} \leq \sqrt{ \frac{1}{n}}$.
\item Output $l^*$ for which $\sqrt{\hat{I}}$ is maximized
\end{enumerate}

 \textbf{Stage 2. Refine clusters}
\begin{enumerate}
\item For each node $u$:
  \begin{enumerate}
  \item Use previously computed $l^*$ to perform spectral clustering on $\mathcal{G}_{-u}$, get $\sigma_u$.
   \item Use $\sigma_u$ to estimate $\hat{P}_l, \hat{Q}_l$.
   \item Assign $\hat{\sigma}(u) = \arg\max_k \sum_{v \,:\, \sigma_u(v) = k} \sum_l 
                  \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) $. 
   \end{enumerate}

\item Run consensus. 

\item Output $\hat{\sigma}$. 

\end{enumerate}

\begin{proposition}
\label{prop:rate_optimal}
Suppose $K$ is fixed and that $n \frac{I_{tot}}{L K} \rightarrow \infty$ where $L$ is the number of colors. Suppose that $P_l \asymp Q_l$ for all $l$. Then the above procedure has error rate satisfying
\[
\lim_{n \rightarrow \infty}  \sup_{\sigma_0, \{P_l, Q_l\}} P \left( l(\hat{\sigma}, \sigma_0) \geq \exp\left( - (1 - \eta) \frac{ n I_{tot}}{\beta K} \right) \right) = 0
\]
\end{proposition}


In particular, this shows that the threshold behavior (for the symmetric $K=2$ case and finite colors $L$) that \cite{jog2015information} demonstrates hold even if the $P_l, Q_l$'s are not known. 

The rest of the write-up constitute the proof of proposition~\ref{prop:rate_optimal}. The proof proceeds in three steps. In the first step (section~\ref{sec:estimation}, we prove a general result that controls the estimation quality $| \hat{P}_l - P_l |$ and $|\hat{Q}_l - Q_l|$ where $\hat{P}_l, \hat{Q}_l$ are constructed from a consistent clustering algorithm. In the second step (section~\ref{sec:initial_clustering}), we provide guarantee for the first stage of the our proposed algorithm. In the third step, we analyze the second stage of the proposed algorithm.

\end{enumerate}

\section{Estimation}
\label{sec:estimation}

Let $\sigma_0$ be the true clustering and $\hat{\sigma}$ be a clustering algorithm.


\begin{proposition}
\label{prop:estimation_consistency}
Let $\sigma = \hat{\sigma}(G)$ be a clustering of the graph with error rate $\gamma$. That is, $d_H(\sigma, \sigma_0) = \gamma n$. Let $\Delta_l = | P_l - Q_l |$. Suppose $\gamma K \log K \rightarrow 0$ and that $P_l \vee Q_l \geq \frac{c}{n}$ for all $l$ for some constant $c$. 


Let $\hat{P}_l = \frac{\sum_{u,v \,:\, \sigma(u)=\sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {\sum_{u,v \,:\, \sigma(u) = \sigma(v)} 1}$ and
    $\hat{Q}_l = \frac{\sum_{u,v \,:\, \sigma(u) \neq \sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {\sum_{u,v \,:\, \sigma(u) \neq \sigma(v)} 1}$ be the MLE of $P_l$ and $Q_l$ based on $\sigma$. 

Let $C_{thresh}$ be an absolute constant and let $\delta$ be a positive, fixed, and arbitrarily small real number. Then there exists a sequence $\eta \rightarrow 0$ such that, for all $l \in L$:

\begin{itemize}
\item[Case 1] If $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq C_{thresh}$, then
\begin{align*}
 | \hat{P}_l - P_l | &\leq \eta \Delta_l \\
 | \hat{Q}_l - Q_l | &\leq \eta \Delta_l 
\end{align*}
\item[Case 2] If $\frac{n \Delta_l^2}{P_l \vee Q_l} \leq C_{thresh}$, then
\begin{align*}
 | \hat{P}_l - P_l | &\leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}} \\
 | \hat{Q}_l - Q_l | &\leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}
\end{align*}
\end{itemize}

with probability at least $1 - L n^{-(3 + \delta)}$. 
\end{proposition}

We note that the sequence $\eta$ is a function of $\gamma K \log K$ and that it does not depend on the color $l$. In other words, $| \hat{P}_l - P_l |$ and $|\hat{Q}_l - Q_l|$ converge uniformly for all colors; this is important when $L$ is allowed to increase with $n$. 


\subsection{Proof} 

We use a rough clustering $\sigma$ to estimate the parameters. $\sigma$ itself is a random variable dependent on the edge variables but we will analyze a fixed $\sigma$ and then take the union bound.

Let $\sigma_0$ be the true clustering and $\sigma$ be a rough one. 
Suppose that $d_H(\sigma, \sigma_0) = \gamma n$.

There are at most $\binom{n}{\gamma n}$ possible assignment $\sigma$'s that satisfy the distance constraint. 

\[
\log \binom{n}{\gamma n} \leq 
  \log \left( \frac{ n(n-1) ...(n-\gamma n+1) }{(\gamma n)!} \right) \leq
  \log \left( \frac{ n^{\gamma n} e^{\gamma n} }
     { (\gamma n)^{\gamma n} } \frac{1}{\sqrt{2\pi \gamma n}} \right)
 \leq C \gamma n \log \frac{1}{\gamma} 
\]



\subsubsection{Bias of $\hat{P}_l$}


Our estimator of $P_l$ is 
\[
\hat{P_l} = \frac{ \sum_{i,j \,:\, \sigma(i) = \sigma(j)} \mathbf{1}(A_{ij} = l) }{
                   \sum_{i,j \,:\, \sigma(i) = \sigma(j)} }
\]

Because $\sigma$ is an imperfect clustering, $\hat{P}_l$ will be biased. In fact, $\E \hat{P}_l$ is a convex combination of $P_l, Q_l$. 
\begin{align}
\E \hat{P_l} &= 
   \frac{ \sum_{i,j \,:\, \sigma(i) = \sigma(j)} \mathbf{1}(\sigma_0(i) = \sigma_0(j) ) P_l + 
               \mathbf{1}(\sigma_0(i) \neq \sigma_0(j)) Q_l }{
                   \sum_{i,j \,:\, \sigma(i) = \sigma(j)} 1 } \\
  &= (1 - \lambda) P_l + \lambda Q_l  \label{eqn:bias_simple_bound}
\end{align}
for $\lambda = \frac{\sum_{i,j \,:\, \sigma(i) = \sigma(j)} 
     \mathbf{1}(\sigma_0(i) \neq \sigma_0(j)) }{\sum_{i,j \,:\, \sigma(i) = \sigma(j)} 1}$.

We will assume that $P_l \geq Q_l$ first. Then, $\E \hat{P}_l \leq P_l$. 

To get a lower bound of the bias, observe that
\begin{align*}
\frac{\sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j)) \mathbf{1}(\sigma_0(i) \neq \sigma_0(j)) }{\sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j)) } &= 
   \frac{\sum_k \sum_{i,j \,:\, \sigma(i)=\sigma(j)=k} \mathbf{1}(\sigma_0(i) \neq \sigma_0(j))}{\sum_k \hat{n}_k (\hat{n}_k-1)} \\
  &\leq \frac{ \sum_k \gamma_k n \hat{n}_k }{\sum_k \hat{n}_k(\hat{n}_k - 1)} \quad
  \left(\trm{for $\sum_k \gamma_k = \gamma$}  \right) \\
  &\leq \frac{\max_k \hat{n}_k \sum_k \gamma_k n }{\min_k \hat{n}_k \sum_k (\hat{n}_k - 1)} \\
 &\leq \frac{\max_k \hat{n}_k }{\min_k \hat{n}_k} \gamma
\end{align*}

We define $\hat{n}_k = \sum_i \mathbf{1}(\sigma(i) = k)$. We can bound the ratio term as follows:
\begin{align*}
\frac{\max_k \hat{n}_k}{\min_k \hat{n}_k} &\leq
   \frac{ \frac{\beta n}{k} + \gamma n}{\frac{n}{\beta k} - \gamma n} \\
  &\leq \frac{\beta^2 + k \gamma \beta}{1 - k \gamma \beta} \\
 &\leq (\beta^2 + k\gamma \beta) (1 + \eta') 
\end{align*}

where $|\eta'| < 2 | k \gamma \beta|$ and thus, $\eta' \rightarrow 0$. It is also clear that $\eta'$ does not depend on $l$. 
We then have that
\[
P_l - \beta^2 \gamma (P_l - Q_l) (1 + \eta')  \leq \E \hat{P}_l \leq P_l
\]

In the event that $Q_l \geq P_l$, it is straightforward to check that
\[
P_l \leq \E \hat{P}_l \leq P_l + \beta^2 \gamma (Q_l - P_l)(1+ \eta')
\]


In both cases, we have the following bound:
\begin{align*}
| \E \hat{P}_l - P_l | &\leq \beta^2 \gamma \Delta_l (1 + \eta')  \\
   &\leq \eta \Delta_l \\
\end{align*}
where $\eta \rightarrow 0$ since $\gamma \rightarrow 0$. 

where $\Delta_l  = |Q_l - P_l|$. 

\subsubsection{Variance of $\hat{P}_l$}

Having handled the bias, we now bound the deviation.

Let $ \tilde{A}_{ij} = \mathbf{1}(A_{ij} = l)$. Then, by Bernstein's inequality,
\[
P\left( \left| \sum_{i,j\,:\, \sigma(i) = \sigma(j)} (\tilde{A}_{ij} - \E \tilde{A}_{ij} ) \right|  > t 
 \right) \leq 2 \exp\left( 
    - \frac{t^2}{ 2 \sum_{i,j \, \sigma(i) = \sigma(j)} \E \tilde{A}_{ij}  + \frac{2}{3}t } 
\right)
\]

We first bound $\sum_{i,j \, \sigma(i) = \sigma(j)} \E \tilde{A}_{ij}$:
\begin{align*}
\sum_{i,j \, \sigma(i) = \sigma(j)} \E \tilde{A}_{ij} &=
  \sum_k \hat{n}_k (\hat{n}_k - 1) \E \hat{P}_l \\
 &\leq (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) \quad 
  \trm{(by Equation~\ref{eqn:bias_simple_bound})}
\end{align*}

Therefore,
\[
P\left( \left| \sum_{i,j\,:\, \sigma(i) = \sigma(j)} (\tilde{A}_{ij} - \E \tilde{A}_{ij} ) \right|  > t 
 \right) \leq 2 \exp\left( 
    - \frac{t^2}{ 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1)  + \frac{2}{3}t } 
\right)
\]

We want to bound the probability by $\exp( - C_1 \gamma n \log \frac{1}{\gamma} - (3+\delta) \log n )$. Assuming that $\gamma \geq \frac{1}{n}$, we have that $\gamma \log \gamma^{-1} \geq \frac{1}{n} \log n$ and thus $C_1 \gamma n \log \frac{1}{\gamma} + (3 + \delta) \log n \leq C_\delta \gamma n \log \frac{1}{\gamma}$. 

We choose $t$ such that
\begin{align*}
t^2 &= 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) \left( 
    C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta) \log n \right) \vee 
   \left( C_1 \gamma n \log \frac{1}{\gamma} + (3 + \gamma) \log n \right)^2  \\
 &\leq 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) C_{\delta} \gamma n \log \frac{1}{\gamma} \vee \left( C_\delta \gamma n \log \frac{1}{\gamma} \right)^2  \\
 &\leq \left( \sqrt{  2 (P_l \vee Q_l)  \sum_k \hat{n}_k (\hat{n}_k - 1) C_\delta \gamma n \log \frac{1}{\gamma} } + C_\delta \gamma n \log \frac{1}{\gamma} \right)^2
\end{align*}

It easy to check that, regardless of which among $\{ 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) ,\, C_\delta \gamma n \log \frac{1}{\gamma} \}$ is larger, the probability term is at most $\exp( - C_1 \gamma n \log \frac{1}{\gamma} - (3+\delta) \log n )$. Thus, with at most that probability:

\begin{align*}
| \hat{P}_l - \E \hat{P}_l | =
\frac{\sum_{i,j \, \sigma(i) = \sigma(j)} (\tilde{A}_{ij} - \E \tilde{A}_{ij} ) }{
  \sum_{i,j} \mathbf{1}( \sigma(i) = \sigma(j) ) } &> 
  \frac{t}{\sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j)) } \\
\end{align*}

Substituting in the previous bound we had of $t$:

\begin{align*}
 \frac{t}{\sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j)) } &\leq
   \frac{  \sqrt{  2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) C_\delta \gamma n \log \frac{1}{\gamma} } + C_\delta \gamma n \log \frac{1}{\gamma} }
        { \sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j) ) }  \\
  &\leq \frac{\sqrt{2 (P_l \vee Q_l) C_\delta \gamma n \log \frac{1}{\gamma}}}
             {\sqrt{ \sum_k \hat{n}_k (\hat{n}_k - 1)}} + 
          \frac{C_\delta \gamma n \log \frac{1}{\gamma}}
             {\sum_k \hat{n}_k (\hat{n}_k - 1)} \\
 &\leq 2 \frac{ \sqrt{P_l\vee Q_l} \sqrt{ C_\delta \gamma \log \frac{1}{\gamma}} }
           {\sqrt{ \min_k (\hat{n}_k - 1)}} + 
       \frac{C_\delta \gamma \log \frac{1}{\delta}}{\min_k (\hat{n}_k - 1)} \\
 &\leq 2 \sqrt{ \frac{P_l \vee Q_l}{n} } \sqrt{C_\delta \beta k \gamma \log \frac{1}{\gamma}} + 
       \frac{C_\delta \beta k \gamma \log \frac{1}{\gamma}}{n} 
\end{align*}
For the second to last inequality, we used the fact that $\sum_k (\hat{n}_k - 1) \frac{\hat{n}_k}{n} \geq \min_k (\hat{n}_k - 1)$ because $\frac{\hat{n}_k}{n}$ sums to 1. 

To further simplify the expression, we note that $C_\delta \beta k \gamma \log \frac{1}{\gamma} \rightarrow 0$ and does not depend on the color $l$. Furthermore, we have that $P_l \vee Q_l \geq \frac{c}{n}$ and thus, $\sqrt{ \frac{P_l \vee Q_l}{n} } \geq \frac{c}{n}$. Using these observations, we conclude that
\begin{align}
| \hat{P}_l - \E \hat{P}_l | \leq \sqrt{ \frac{P_l \vee Q_l}{n} } \eta \label{eqn:variance_bound}
\end{align}

with probability at least $\exp( -C_1 \gamma n \log \frac{1}{\gamma} - (3 + \delta) \log n)$. $\eta \rightarrow 0$ and does not depend on the color $l$ (though it does depend on $\delta$). Taking the union bound across all clusterings with error $\gamma$ and across all colors, we have that the probability of (\ref{eqn:variance_bound}) holding simultaneously for all colors $l$ is at least $L n^{-(3+\delta)}$. 

\subsubsection{Combining Bias and Variance}

From the bias and variance analysis, we have that 

\begin{align*}
| \E \hat{P}_l - P_l | &\leq \eta \Delta_l \\
| \hat{P}_l - \E \hat{P}_l | &\leq 
   \sqrt{ \frac{P_l \vee Q_l}{n} } \eta
\end{align*}

In Case 1, $\Delta_l \geq C_{thresh} \sqrt{ \frac{P_l \vee Q_l}{n}} $ and thus, 
\[
| \hat{P}_l - P_l| \leq \eta \Delta_l
\]

In Case 2, $\Delta_l \leq C_{thresh} \sqrt{ \frac{P_l \vee Q_l}{n}}$ and thus,
\[
| \hat{P}_l - P_l| \leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}
\]





\newpage
\section{Controlling probability of misclassification}
\label{sec:misclassify}
Now that we have control over $\hat{P}_l$, we study the effect of plugging in these estimates into the refinement stage.

Let $\sigma_u$ be a clustering for all the nodes except $u$. Suppose that $d_H(\sigma_u, \sigma_0) = \gamma n$. 


We assign $u$ based on the criterion:
\[
\argmax_k \sum_{v\, \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
\]

\begin{proposition}
Suppose that $\sigma_u$ is a clustering of all nodes except for $u$ with error rate $\gamma$, that is, $d_H(\sigma_u, \sigma_0) = \gamma n$. Let $C_{thresh}$ be an absolute constant and suppose that statements of proposition~\ref{prop:estimation_consistency} holds. 

Suppose that $\frac{n I^*}{L} \rightarrow \infty$ and that for all $l$, $P_l \asymp Q_l$.  Suppose also that $P_l, Q_l \leq 1 - \epsilon$ for some fixed arbitrarily small $0 < \epsilon < 1$. 

Then, we have that, with probability at least $1 - \exp \left( - (1 - o(1)) \frac{n}{K} I^* \right)$, 
\[
\sigma_0(u) = \argmax_k \sum_{v\, \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
\]

\end{proposition}




\subsection{Proof}

Throughout the proof, we let $\eta, \eta'$ denote a sequence that converges to 0 and let $C$ denote a constant. Their value could change from line to line. \\

First, define $L_1 = \{ l \,:\, n \frac{\Delta_l^2}{P_l \vee Q_l} \geq C_{thresh} \}$. Then we claim that $C_{\epsilon, c_1, c_2} \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} = I^*( 1 - \eta )$. To see this, observe first that

\begin{align*}
I^* &= -2 \log \sum_l \sqrt{P_l Q_l} \\
  &= C_\epsilon \sum_l (\sqrt{P_l} - \sqrt{Q_l} )^2 \\
  &= C_\epsilon \sum_l \frac{\Delta_l^2}{(\sqrt{P_l} + \sqrt{Q_l})^2} \\
  &= C_{\epsilon, c_1, c_2} \sum_l \frac{\Delta_l^2}{P_l \vee Q_l} 
\end{align*}

Therefore, we have that
\begin{align*}
C_{\epsilon, c_1, c_2} \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} &= I^* - 
 C_{\epsilon, c_1, c_2} \sum_{l \notin L_1} \frac{\Delta_l^2}{P_l \vee Q_l} \\
 &\geq I^* - C_{\epsilon, c_1, c_2} \sum_{l \notin L_1} \frac{C_{thresh}}{n} \\
 &\geq I^* - C_{\epsilon, c_1, c_2} \frac{ L C_{thresh}}{n} \\
 &\geq I^* - \eta I^* 
\end{align*}

The second inequality follows from the definition of $L_1$. The third inequality follows because $\frac{n I^*}{L} \rightarrow \infty$ by assumption. \\


Now we proceed onto the main proof. Suppose without the loss of generality that $\sigma_0(u) = 1$.  We want to then control the probability that for some cluster $k$, 
\begin{align}
\sum_{v\,:\, \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
&\geq 
 \sum_{v\,:\, \sigma_u(v)=1} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
  \quad \trm{(iff)}  \nonumber \\
\sum_{v \,:\, \sigma_u(v) = k} \bar{A}_{uv} - \sum_{v\,:\, \sigma_u(v) = 1} \bar{A}_{uv} 
\label{eqn:bad_event1}
&\geq 0 
\end{align}
where $\bar{A}_{uv} \equiv \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l)$.

Define $m_1 = |\{ v \,:\, \sigma_u(v) = 1 \}|$ and $m_k = | \{ v \,:\, \sigma_u(v) = k \}|$ as the size of clusters $m_1, m_k$ under $\sigma_u$. Define $m_k' = \{ v \,:\, \sigma_u(v) = k,\, \sigma_0(v) = k \}$, $m_1' = \{ v \,:\, \sigma_u(v) = 1 ,\, \sigma_0(v) = 1\}$ as the points correctly clustered by $\sigma_u$. 

With these definitions, the probability of the bad event Equation~\ref{eqn:bad_event1} is upper bounded by the probability of the following:

\begin{align*}
\left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m'_k} \tilde{X}_i \right) - 
\left( \sum_{i=1}^{m_1'} \tilde{X}_i + \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) &\geq 0  \quad \trm{(iff)} \\
\exp( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i - 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) ) &\geq 1 
\end{align*}

 where $\tilde{X}_i = \log \frac{\hat{P}_l}{\hat{Q}_l}$ with probability $P_l$ and $\tilde{Y}_i = \log \frac{\hat{P}_l}{\hat{Q}_l}$ with probability $Q_l$. 



\begin{align*}
& P \left( \exp( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i- 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) ) \geq 1 \right) \\ 
&\leq \E \left( 
\exp( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i - 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) )
 \right) \\ 
&\leq \left( \E \exp( t \tilde{Y}_i ) \right)^{m_k'} 
      \left( \E \exp(t \tilde{X}_i ) \right)^{m_k - m_k'}  
    \left( \E \exp( - t \tilde{X}_i) \right)^{m_1'} 
    \left( \E \exp( -t \tilde{Y}_i ) \right)^{m_1 - m_1'} \\
&\leq \left( \sum_l e^{t \log \frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k'}  
      \left( \sum_l e^{t \log \frac{\hat{P}_l}{\hat{Q}_l}} P_l \right)^{m_k - m_k'} 
      \left( \sum_l e^{- t \log \frac{\hat{P}_l}{\hat{Q_l}} } P_l \right)^{m_1'}
     \left( \sum_l e^{-t \log \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{m_1 - m_1'}
\end{align*}

We will set $t = \frac{1}{2}$, in which case, we have:
\begin{align}
=& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k'}
 \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l \right)^{m_k - m_k'}
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l \right)^{m_1 - m_1'}
       \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1'} \nonumber \\
=&  \left( \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right)^{m_k - m_k'}
 \left( \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right)^{m_1 - m_1'}  
   \label{eqn:excess_error_term} \\
 & \left( \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{m_k} 
    \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1}
   \label{eqn:Ihat_error_term} 
\end{align} 

We will bound term~\ref{eqn:excess_error_term} and \ref{eqn:Ihat_error_term} separately. Loosely speaking, we will show that term~\ref{eqn:excess_error_term} is bounded in magnitude by $\exp( o(I^*) \frac{n}{k} )$ and that term~\ref{eqn:Ihat_error_term} is bounded by $\exp( - \frac{n}{k} (1 + o(1) I^*)$. 


\textbf{Bound for Term~\ref{eqn:excess_error_term}.} 

Now, we can bound term~\ref{eqn:excess_error_term}:
\begin{align*}
\left| 1 -  \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right|
 &= \left| \frac{ \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} (P_l - Q_l) }
     { \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l } \right| \\
&\leq \frac{8}{\sum_l \sqrt{P_l Q_l}} 
     \left| \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} }(P_l - Q_l) \right| \\
&\leq 16 \left|  \sum_{l} \left( \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right) (P_l - Q_l)  \right| \\
&\leq 16 \left| 
     \sum_{l \in L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} - 1 \right)(P_l - Q_l) 
     \right| + \sum_{l \notin L_1} \left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} - 1 \right| \Delta_l \\
&\leq 16 \sum_{l \in L_1} \frac{\Delta^2_l}{Q_l}(1+ \eta') + 
      \sum_{i \notin L_1} 2 C_{thresh} \frac{\Delta_l}{\sqrt{n (P_l \vee Q_l)}} \\
&\leq C_{\epsilon, c_1, c_2} I^* (1 + \eta') + 2 C_{thresh}^2 \frac{L}{n} \\
&\leq C_{\epsilon, c_1, c_2} I^* (1 + \eta')
\end{align*}


Where the second inequality follows from lemma~\ref{lem:sqrt_ratio_times_ql},
second to last inequality follows under the assumption that $\sum_l \sqrt{P_l Q_l} \geq \frac{1}{2}$, and the last inequality follows from Lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1}. 

Identical analysis shows that
\[
\left| 1 - \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right| 
= O(I^*) 
\]

Now, we note that $\exp( | 1 - x | ) \geq |x|$. 
Therefore, term~\ref{eqn:excess_error_term} can be bounded as
\begin{align*}
&  \left( \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right)^{m_k - m_k'}
 \left( \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right)^{m_1 - m_1'}  \\
&\leq \exp( O(I^*) (m_k - m_k' + m_1 - m_1') ) \\
&\leq \exp( O(I^*) \gamma n) \\
&\leq \exp\left( \frac{n}{k} o(I^*) \right) \quad \trm{(since $\gamma k \rightarrow 0$)}
\end{align*}

\textbf{Bound for Term~\ref{eqn:Ihat_error_term}.}


Define 
$\hat{I} = - \log \left( \sum_l \frac{\hat{P}_l}{\hat{Q}_l} Q_l \right) \left( \sum_l \frac{\hat{Q}_l}{\hat{P}_l} P_l \right) $. 
With this definition, 

\begin{align*}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k} 
       \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1} \\
&= \exp( - \hat{I} )^{\frac{m_k + m_1}{2}}  \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} 
\end{align*}

We claim that the following three statements are true. 
\begin{enumerate}
\item[Claim 1] $m_k \geq n_1 - 2 \gamma n$ and likewise for $m_1$.
\item[Claim 2] $\hat{I} - I^* \geq - o(1) I^*$
\item[Claim 3] $\left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} = \exp(\frac{n}{k}  o(I^*)  $
\end{enumerate}

Let us first suppose that these statements are true and see that term~\ref{eqn:Ihat_error_term} can be bounded. 


\begin{align*}
& \exp( - \hat{I} )^{\frac{m_1 + m_k}{2}}  \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}}  \\
&\leq  \exp( - (I^* + (\hat{I} - I^*) )^{\frac{m_1 + m_k}{2}}  
 \left( \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l}
             {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l} \right)^{\frac{m_1 - m_k}{2}}  
  \\
&\leq \exp \left( - (1-o(1)) I^* (n_1 - \gamma n) \right) 
   \left( \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l}
             {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l} \right)^{\frac{m_1 - m_k}{2}}  
   \quad \trm{(by claim 1 and 2)}\\
&\leq \exp \left( - (1-o(1)) \frac{n}{\beta k} I^*  \right) 
   \quad \trm{(by claim 3)}
\end{align*}

The last inequality holds because $\gamma = o\left( \frac{1}{k \log k} \right)$. We will prove each of the three claims in the remainder of the proof.

\textbf{Claim 1:} This is straightforward. $\sigma_u$ has at most $\gamma n$ errors and therefore, $m_1' \geq n_1 - \gamma n$ and $m_1 - m_1' \leq \gamma n$. 

\textbf{Claim 2:} We show that the estimation error of $\hat{P}_l, \hat{Q}_l$ does not make $\hat{I}$ too small.

\begin{align}
\hat{I} - I^* &= - \log \frac{ 
     \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)
     \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)}{ 
          \left( \sum_l \sqrt{P_l Q_l} \right)^2 } \label{eqn:Ihat_Istar}
\end{align}

Let us consider the numerator.
\begin{align*}
& \left( \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)
\left( \sum_l \sqrt{ \frac{\hat{Q}_l}{\hat{P}_l}} P_l \right) \\
&= \left( \sum_l \sqrt{ P_l Q_l} \sqrt{ \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l}} \right) 
     \left( \sum_l \sqrt{P_l Q_l} \sqrt{ \frac{P_l}{\hat{P}_l} \frac{\hat{Q}_l}{ Q_l}} \right) \\
&= \sum_l P_l Q_l + 2\sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} + 
   \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
&= \left( \sum_l \sqrt{P_l Q_l} \right)^2 + \sum_{l < l'} 
                  \sqrt{P_l Q_l P_{l'} Q_{l'}} \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) 
\end{align*}

where we define $T_{l,l'} = \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} 
      \frac{P_{l'}}{\hat{P}_{l'}} \frac{\hat{Q}_{l'}}{Q_{l'}}  $. It will be later shown that $T_{l,l'} \rightarrow 1$ and thus, continuing equation~\ref{eqn:Ihat_Istar},

\begin{align}
\hat{I} - I^* &= - \log \left( 1 + \frac{ \sum_{l<l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right)}
    { \left( \sum_l \sqrt{ P_l Q_l} \right)^2 }  \right) \nonumber \\
     &  \geq  - \log \left( 1 + 4 \sum_{l<l'} \sqrt{P_l Q_l P_{l'} Q_{l'}}  
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right)  \right) 
  \quad \trm{(assuming that $\sum_l \sqrt{P_l Q_l} \geq 1/2$)} \nonumber \\
   & \geq - 4 \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \label{eqn:Ihat_Istar2}
\end{align}

We proceed by first bounding $|T_{l,l'} - 1|$ and then taking the second order approximation of $\left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right)$ around $1$. 

\begin{align*}
|T_{l,l'} - 1| &= \left| \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} 
      \frac{P_{l'}}{\hat{P}_{l'}} \frac{\hat{Q}_{l'}}{Q_{l'}} - 1 \right| \\
 &= \left| \left( 1 - \frac{P_l - \hat{P}_l}{P_l} \right)
    \left( 1 - \frac{\hat{Q}_l - Q_l}{\hat{Q}_l} \right)
   \left( 1- \frac{\hat{P}_{l'} - P_{l'}}{\hat{P}_{l'}}\right)
   \left( 1 -  \frac{Q_{l'}- \hat{Q}_{l'}}{Q_{l'}} \right) -1 \right| \\
&\leq \left( \frac{|P_l - \hat{P}_l|}{P_l} +  \frac{|\hat{Q}_l - Q_l|}{\hat{Q}_l}
           +   \frac{| \hat{P}_{l'} - P_{l'}|}{\hat{P}_{l'}} +
               \frac{| Q_{l'} - \hat{Q}_{l'} | }{Q_{l'}} \right) \\
& \leq 2\left( \frac{|P_l - \hat{P}_l|}{P_l} +  2\frac{|\hat{Q}_l - Q_l|}{Q_l}
           +   \frac{| \hat{P}_{l'} - P_{l'}|}{P_{l'}} +
               \frac{| Q_{l'} - \hat{Q}_{l'} | }{Q_{l'}} \right) 
\end{align*}
where the last inequality follows from lemma~\ref{lem:bound_ratio_P_Pl}.

Since we only work with pairs $(l, l')$ such that $l' > l$ and we can choose whatever ordering we would like. Suppose that the $l$'s are in decreasing order of $\frac{|\hat{P}_l - P_l|}{P_l} + \frac{|\hat{Q}_l - Q_l|}{Q_l}$ and therefore, we have that, for all pairs $l < l'$, 
\[
| T_{l,l'} - 1 | \leq 2 
    \left( \frac{|\hat{P}_l - P_l|}{P_l} + \frac{|\hat{Q}_l - Q_l|}{Q_l} \right)
\]

By proposition~\ref{prop:estimation_consistency}, we have that, for $l \in L_1$, $\frac{|P_l - \hat{P}_l|}{P_l} \leq \eta \frac{\Delta_l}{P_l \vee Q_l}$ and for $l \notin L_1$, $\frac{|P_l - \hat{P}_l|}{P_l} \leq \eta \frac{1}{\sqrt{n (P_l \vee Q_l)}}$ and likewise for the $\frac{|\hat{Q}_l - Q_l|}{Q_l}$ term. We plug these bounds into the previous derivation and get that:
\begin{align*}
|T_{l,l'} - 1|  &\leq \eta  \frac{\Delta_l}{P_l \vee Q_l}  \quad \trm{for $l \in L_1$}\\
|T_{l,l'} - 1 | &\leq \eta \frac{1}{\sqrt{ n (P_l \vee Q_l)}} \quad \trm{for $l \notin L_1$}
\end{align*}
We have used the assumption that $P_l \asymp Q_l$ and folded constants into the sequence $\eta$. 


The Taylor approximation of $\sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2$ around $T_{l,l'}=1$ is:
\begin{align*}
\sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} -2  &\leq 
  \frac{1}{4} (T_{l,l'} - 1)^2 + O (T_{l,l'}-1)^3 
\end{align*}

Continuing on from equation~\ref{eqn:Ihat_Istar2}, we have that
\begin{align*}
\hat{I} - I^* &\geq - 4 \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
&\geq - 4 \sum_{l \in L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) 
     - 4 \sum_{l \notin L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
  &\geq - \sum_{l \in L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
             \eta \left( \frac{\Delta_l}{P_l \vee Q_l}  \right)^2 
        - \sum_{l \notin L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
             \eta \frac{1}{n (P_l \vee Q_l)} \\
 &\geq - \eta \left( \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} \right)
         \left( \sum_{l'}  \sqrt{P_{l'}Q_{l'}} \right) 
       - \eta \left( \sum_{l \notin L_1} \frac{1}{n} \right) 
          \left( \sum_{l'} \sqrt{P_{l'} Q_{l'} } \right) \\
 &\geq - o(I^*)
\end{align*}

The last inequality follows because $\sum_{l'} \sqrt{P_{l'} Q_{l'}} \leq 1$ and because $\sum_{l \notin L_1} \frac{1}{n} \leq \frac{L}{n} = o(I^*)$. This proves claim 2.

\textbf{Claim 3.} 

\begin{align*}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} \\
&= \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} 
   \left( \frac{\sum_l \sqrt{\hat{P}_l \hat{Q}_l}}{\sum_l \sqrt{\hat{P}_l \hat{Q}_l}} \right)^{\frac{m_1 - m_k}{2}} \\
&=  \left( 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} \hat{P_l} } \right)^{\frac{m_1 - m_k}{2}} 
\end{align*}

Assume that $m_k \geq m_1$. The reverse case can be analyzed in the identical manner. Then,
\begin{align*}
&= \left( 1 + 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l)}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( 1+ \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} (\hat{P}_l - P_l)}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l } \right)^{\frac{m_k - m_1}{2}} 
   \\
\end{align*}

By lemma~\ref{lem:sqrt_ratio_times_ql}, the denominators are of constant order. That is, 
$\sum_l \sqrt{ \frac{ \hat{P}_l }{ \hat{Q}_l } \hat{Q}_l } = C$ and 
$\sum_l \sqrt{ \frac{\hat{Q}_l}{P_l} } P_l = C$. 

To bound the numerator term, we apply lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1}. 

\begin{align*}
\left| \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l) \right|  &= 
  \left|  \sum_l \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) 
 \right| \\
& \leq \left| \sum_{l \in L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) \right| +  %next term
  \left| \sum_{l \notin L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) \right| \\
& \leq \sum_{l \in L_1} \eta \frac{\Delta_l^2}{Q_l} + \sum_{l \notin L_1} \eta \frac{1}{n} \\
& \leq \eta I^* + \eta \frac{L}{n}  \\
& \leq \eta I^*  
\end{align*}

The second inequality follows from lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1} and the definition of $L_1$. 

\begin{align*}
& \left( 1 + 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l)}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( 1+ \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} (\hat{P}_l - P_l)}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l } \right)^{\frac{m_k - m_1}{2}} 
\\
&\leq \exp\left( (m_k - m_1) \log(1 + o(I^*) ) \right) \\
&\leq \exp \left( \frac{n}{k} o(I^*) \right) 
\end{align*}

This proves claim 3. 

Multiplying the bounds for term~\ref{eqn:Ihat_error_term} and \ref{eqn:excess_error_term} completes the proof. 



\subsection{Lemmas}

Here we collect various lemmas used in the proof.

We often use the bound that $\frac{1}{2} P \leq \hat{P}_l \leq 2 P_l$. The following lemma justifies this.

\begin{lemma}
\label{lem:bound_ratio_P_Pl}
Let $\eta$ be a sequence that tends to zero. Let $l$ be any color and suppose that $P_l \asymp Q_l$ and that $P_l, Q_l \geq \frac{c}{n}$ for some absolute constant $c$. Suppose either that $|\hat{P}_l - P_l| = \eta \Delta_l$ or that $| \hat{P}_l - P_l | = \eta \sqrt{ \frac{P_l \vee Q_l}{n} }$. 

Then, for all small enough $\eta$, we have that
\[
\frac{1}{2} P \leq \hat{P}_l \leq 2 P_l
\]

\end{lemma}

\begin{proof}
Under the first condition, one need only observe that there exists an absolute constant $C$ for which $\Delta_l \leq C P_l$. Under the second condition, we use the assumption that $P_l \geq \frac{C}{n}$ for some absolute constant $C$. 

\end{proof}

The following lemma bounds the quantity $\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} - 1$ for either the case that $\Delta_l \geq C_{thresh} \sqrt{ \frac{P_l \vee Q_l}{n} }$ or else. 

\begin{lemma}
\label{lem:sqrt_ratio_pl_ql_minus_1}
Let $\eta$ be a sequence that tends to 0. Fix a color $l$ and assume that $P_l \asymp Q_l$ and that $P_l, Q_l \geq \frac{c}{n}$ for some absolute constant $c$. 
\begin{enumerate}
\item 
Suppose $\hat{P}_l$ and $\hat{Q}_l$ satisfies $n \frac{\Delta_l^2}{P_l \vee Q_l} \geq C_{thresh}$ and therefore
\begin{align*}
| \hat{P}_l - P_l | &= \eta \Delta_l  \\
| \hat{Q}_l - Q_l | &= \eta \Delta_l  
\end{align*}

Then, we have that 
\[
\sqrt{ \frac{\hat{P}_l }{\hat{Q}_l} } - 1 \left\{ \begin{array}{cc} 
                    \leq \frac{P_l - Q_l}{Q_l} (1 + \eta') & (\trm{if }  P_l \geq Q_l) \\
                    \geq \frac{P_l - Q_l}{Q_l} (1 + \eta') & (\trm{if } P_l < Q_l) 
           \end{array} \right.
\]
where $\eta' \rightarrow 0$ and does not depend on the color $l$. 

\item
Suppose $\hat{P}_l$ and $\hat{Q}_l$ satisfies $n \frac{\Delta_l^2}{P_l \vee Q_l} \leq C_{thresh}$ and therefore
\begin{align*}
| \hat{P}_l - P_l | &= \eta \sqrt{ \frac{P_l \vee Q_l}{n} } \\
| \hat{Q}_l - Q_l | &= \eta \sqrt{ \frac{P_l \vee Q_l}{n} }
\end{align*}
Then, we have that
\[
\left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right| \leq
 2 C_{thresh} \frac{1}{\sqrt{n  (P_l \vee Q_l) } }
\]

\end{enumerate}
\end{lemma}


\begin{proof}


First, note that 
\begin{align*}
 \frac{P_l}{Q_l} - 1 &= \frac{P_l - Q_l}{Q_l}  
\end{align*}

We will show that $\frac{\hat{P}}{\hat{Q}}$ behaves similarly. 

As a preliminary step, we note that
\[
\frac{\hat{Q}_l - Q_l}{Q_l} = \eta \frac{\Delta_l}{Q_l} = \eta' \rightarrow 0
\]
where we used the assumption that $P_l \asymp Q_l$.

\begin{align*}
\frac{\hat{P}_l}{\hat{Q}_l} - 1 &= 
     \frac{ \hat{P}_l - P_l + P_l }{ \hat{Q}_l - Q_l + Q_l} -1  \\
  &=  \frac{  \frac{\hat{P}_l - P_l}{Q_l} + \frac{P_l}{Q_l}}
       { \frac{\hat{Q}_l - Q_l}{Q_l} + 1} - 1 \\
 &= \left( \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} \right)
    \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta')  \right) -1  \\
 &= \frac{P_l-Q_l}{Q_l} + \eta' \frac{\Delta_l}{Q_l} 
\end{align*}



Therefore, 
\begin{align*}
\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} - 1 &= 
   \sqrt{ 1 + \frac{P_l - Q_l}{Q_l} + \eta' \frac{\Delta_l}{Q_l}}  - 1\\
  &= \sqrt{ 1 + \frac{P_l - Q_l}{Q_l} } 
    \left( \sqrt{ 1 + \eta' \frac{\Delta_l}{P_l}} \right) - 1 \\
  &= \sqrt{ 1 + \frac{P_l - Q_l}{Q_l} } (1 + \eta' \frac{\Delta_l}{P_l} ) - 1  \\
  &= \sqrt{ 1 + \frac{P_l - Q_l}{Q_l}} - 1 + \sqrt{ \frac{P_l}{Q_l}} \eta' \frac{\Delta_l}{P_l} \\ 
  & \left\{ \begin{array}{cc}
      \leq \frac{P_l - Q_l}{Q_l} (1+ \eta') & \trm{(if $P_l \geq Q_l$)} \\
      \geq \frac{P_l - Q_l}{Q_l} (1+ \eta') & \trm{(if $P_l < Q_l$)} 
     \end{array} \right.
\\
\end{align*}

The second and the third equality holds for all small enough $\eta'$. The last inequality follows because $P_l \asymp Q_l$. \\


Symmetry yields that 

\begin{align*}
\sqrt{ \frac{\hat{Q}_l}{\hat{P}_l} } - 1  &
   \left\{ \begin{array}{cc}
      \leq \frac{Q_l - P_l}{P_l} (1 + \eta') & \trm{(if $Q_l \geq P_l$)} \\
      \geq \frac{Q_l - P_l}{P_l} (1 + \eta') & \trm{(if $Q_l < P_l$)} 
     \end{array} \right. \\
\end{align*}

This proves the first case. The proof of the second case is almost identical. 

First, under the assumption that $P_l, Q_l \geq \frac{c}{n}$, it is clear that $\frac{\hat{Q}_l - Q_l}{Q_l} = \eta \sqrt{ \frac{1}{ n Q_l} } = \eta' \rightarrow 0$. 

Therefore, it still follows that
\begin{align*}
\frac{\hat{P}_l}{\hat{Q}_l} - 1 &= 
     \left( \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} \right)
     \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} ( 1 + \eta') \right) - 1 \\
 &= \frac{P_l - Q_l}{Q_l} + \eta' \sqrt{ \frac{1}{ n (P_l \vee Q_l)} } 
\end{align*}

and it is clear that $\eta'$ must satisfy the condition that $\frac{P_l - Q_l}{Q_l} + \eta'\sqrt{ \frac{1}{n (P_l \vee Q_l)} } + 1 > 0$. 



\begin{align*}
\left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right| &= 
 \left|  \sqrt{ 1 + \frac{P_l - Q_l}{Q_l} + \eta' \frac{1}{\sqrt{n (P_l \vee Q_l)} }}
   -1  \right| \\
  &\leq \left| \frac{P_l - Q_l}{Q_l} + \eta' \frac{1}{\sqrt{ n (P_l \vee Q_l)}}        \right| \\
 &\leq 2 C_{thresh} \frac{1}{\sqrt{n (P_l \vee Q_l)} } 
\end{align*}

The first inequality follows because $ \sqrt{1 + x} - 1 \leq x$ for $x \geq 0$ and $\sqrt{ 1 + x} - 1 \geq x$ for $-1 < x < 0$. 
The second inequality follows because $\left| \frac{P_l - Q_l}{Q_l} \right| \leq C_{thresh} \frac{1}{\sqrt{n (P_l \vee Q_l)}} $. 

\end{proof}


The following lemma bounds $\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } Q_l$. 
\begin{lemma}
\label{lem:sqrt_ratio_times_ql}

Let $\eta$ be a sequence that tends to 0. Fix a color $l$ and suppose $P_l \asymp Q_l$. Suppose also that
\[
\frac{|\hat{Q}_l - Q_l|}{Q_l} = \eta \quad 
\frac{|\hat{P}_l - P_l|}{P_l} = \eta
\]

Then, we have that for all small enough $\eta$, 
\[
\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } Q_l \geq \frac{1}{2} \sqrt{\hat{P}_l \hat{Q}_l} \geq
  \frac{1}{8} \sqrt{P_l Q_l}
\]
\end{lemma}

First, we note that the condition that $\frac{|\hat{Q}_l - Q_l|}{Q_l} \rightarrow 0$ is satisfied for $l$ such that $\Delta_l \geq C_{thresh}\sqrt{\frac{P_l \vee Q_l}{n}}$ as well as for $l$ such that $\Delta_l \leq C_{thresh} \sqrt{\frac{P_l \vee Q_l}{n}}$ if we also add the condition that $P_l, Q_l = \frac{C}{n}$ for some constant $C$. This is because, in the latter case, it is known that $\frac{|\hat{P}_l - P_l| }{P_l} \leq \eta \sqrt{\frac{1}{n (P_l \vee Q_l)}}$. 

\begin{proof}


\begin{align*}
& \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \\
&= \sqrt{ \hat{P}_l \hat{Q}_l} \frac{Q_l}{\hat{Q}_l} \\
&= \sqrt{\hat{P}_l \hat{Q}_l} \frac{1}{ \frac{Q_l - \hat{Q}_l}{Q_l} + 1 } \\
&= \sqrt{\hat{P}_l \hat{Q}_l} 
  \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta') \right)  \\
&= \sqrt{\hat{P}_l \hat{Q}_l} (1 - \eta)
\end{align*}

where in the second to last equality, we used the fact that $\frac{\hat{Q}_l - Q_l}{Q_l} = \eta' \rightarrow 0$. 

Clearly then, for small enough $\eta$, we continue the bound as 
\begin{align*}
&\geq \frac{1}{2} \sqrt{ \hat{P}_l \hat{Q}_l} 
\end{align*}

Also, for small enough $\eta$, we have that $\hat{P}_l \geq \frac{1}{2} P_l$ and $\hat{Q}_l \geq \frac{1}{2} Q_l$ and thus, we have the final bound
\[
\frac{1}{8} \sqrt{ P_l Q_l}
\]
as desired. 

\end{proof}



\newpage
\section{Choosing the initial clustering}
\label{sec:initial_clustering}

Recall that the stage 1 of our algorithm is as follows. Let $\tau$ be an input parameter. 

\begin{enumerate}
\item For each color $l$:
  \begin{enumerate}
   \item Perform spectral clustering on $\tilde{A}_{ij} \equiv \mathbf{1}(A_{ij} = l)$ to get $\sigma^l$. 
   \item Estimate $\hat{P}_l, \hat{Q}_l$ via counts from $\sigma^l$. 
   \item Estimate $\sqrt{I_l}$ via 
  $\sqrt{ \hat{I}_l } \equiv \frac{| \hat{P}_l - \hat{Q}_l |}{\sqrt{ \hat{P}_l \vee \hat{Q}_l}}$. 
   \end{enumerate}
\item Discard from $L$ all colors $l$ such that $\sqrt{\hat{I}_l} \leq \tau \sqrt{ \frac{K}{n}}$.
\item Output $l^*$ for which $\sqrt{\hat{I}}$ is maximized
\end{enumerate}

Assuming that $ \frac{n I_{tot}}{K} \rightarrow \infty$, we want to say that the $l^*$ we output satisfies $\frac{n I_{l^*}}{K} \rightarrow \infty $ and that the $l$'s we discard satisfy $\limsup_{n \rightarrow \infty} n \frac{I_l}{K} < \infty$. It must be noted that the output of the initialization algorithm depends on $n$; that is, $l^*$ and the discarded colors $l$ depend on $n$. We will omit the dependence in our notation. 


Both of these claims follow from proposition~\ref{prop:initial_guarantee} below. The second claim follows directly from the second statement of proposition~\ref{prop:initial_guarantee}. To see that the first claim is also true, note that there must exist an $l$ such that $\frac{n I_l}{K}$ by proposition~\ref{prop:consistency_possible} and furthermore, by the first statement of proposition~\ref{prop:initial_guarantee}, $\hat{I}_{l} \asymp I_{l}$. 

\begin{proposition}
\label{prop:initial_guarantee}
Suppose color $l$ satisfies 

$$\Delta_l = \omega \left( \sqrt{ \frac{K (P_l \vee Q_l)}{n}} \right)$$. 

Let $\sigma^l$ be a spectral clustering of the graph based on $\tilde{A}_{ij} = \mathbf{1}(A_{ij}) = l)$ and let $\hat{P}_l, \hat{Q}_l$ be estimates of $P_l, Q_l$ constructed from $\sigma^l$. Then, with probability at least $1 - 2n^{-3+\delta}$ for some $\delta > 0$, there is a sequence $\eta \rightarrow 0$ such that

\[
\frac{ | P_l - Q_l |}{\sqrt{P_l \vee Q_l}} (\frac{1}{\sqrt{2}} - \eta) \leq \frac{ | \hat{P}_l - \hat{Q}_l| }{\sqrt{ \hat{P}_l \vee \hat{Q}_l }} \leq  
 \frac{ | P_l - Q_l | }{\sqrt{ P_l \vee Q_l}} (\sqrt{2} + \eta)
\]

On the other hand, supposing $K$ is fixed and consider a color $l$ such that 

$$\Delta_l = O \left( \sqrt{ \frac{P_l \vee Q_l}{n}} \right)$$ 

(which implies that $\sigma^l$ is inconsistent), then, with probability at least $1 - n^{-3+\delta}$, there exists $\eta \rightarrow 0$ such that 
\[
\frac{\hat{P}_l - \hat{Q}_l}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} \leq  C' \frac{1}{\sqrt{n}} (1 + \eta) 
\]
where $C' \equiv \sqrt{2} (\limsup_{n \rightarrow} \sqrt{n I_l} + 1)$ is a constant. 
\end{proposition}



\begin{proof}

Let color $l$ be fixed and let $\gamma$ be the error rate of the spectral clustering $\sigma^l$. 

From the proof of estimation error of $\hat{P}_l$ (\ref{prop:estimation_consistency})
\[
|\hat{P}_l - P_l| \leq \beta^2 \gamma \Delta_l (1+o(1)) + 2 \sqrt{\frac{P_l \vee Q_l}{n} } \sqrt{ C K \gamma \log \frac{1}{\gamma}} + \frac{K \gamma \log \frac{1}{\gamma}}{n}
\]
with probability at least $1 - n^{-(3+\delta)}$  and likewise for $\hat{Q}_l - Q_l$. 

First, suppose $\Delta = \omega \left( \sqrt{ \frac{ K(P_l \vee Q_l)}{n}} \right)$. Then, by Theorem~\ref{thm:spectral_rate}, we know that, with probability at least $1 - n^{-4}$, $\gamma K \log K \rightarrow 0$. We also assume without loss of generality that $P_l \geq Q_l$. 


In this case, $\sqrt{ \frac{P_l \vee Q_l}{n}} = o( \Delta_l )$. Also, $\Delta_l \geq \frac{1}{n}$ and so we have that $| \hat{P}_l - P_l | = o(\Delta_l)$. Since $\Delta_l \leq P_l$, we also have that $ \frac{1}{2} P_l \leq \hat{P}_l  \leq 2 P_l$. Likewise, we have that $|\hat{Q}_l - Q_l| = o(\Delta_l)$ and $\hat{Q}_l \leq 2 P_l$. 

\begin{align*}
\frac{ | \hat{P}_l - \hat{Q}_l |}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} &\geq 
          \frac{| P_l - Q_l | - o(\Delta_l)}{ \sqrt{2 P_l} } \\
  &\geq \frac{\Delta_l}{\sqrt{2 P_l} } (1 - o(1)) 
\end{align*}

\begin{align*}
\frac{ | \hat{P}_l - \hat{Q}_l |}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} &\leq 
          \frac{| P_l - Q_l | + o(\Delta_l)}{ \sqrt{\frac{1}{2} P_l} } \\
  &\leq \frac{\sqrt{2} \Delta_l}{\sqrt{P_l} } (1 + o(1)) 
\end{align*}

Now, we turn to the second statement of the proposition and suppose  $\Delta_l = O\left( 
   \sqrt{\frac{P_l \vee Q_l}{n} } \right)$. With no assumption on $\gamma$, we must carefully examine the proof of proposition~\ref{prop:estimation_consistency}.

From the proof, it is clear that the bias $|\E \hat{P}_l - P_l| \leq \Delta_l = O\left( 
\sqrt{ \frac{P_l \vee Q_l}{n}} \right)$ and that the variance satisfies 
\begin{align*}
|\hat{P}_l - \E \hat{P}_l | &\leq \frac{\sqrt{2 (P_l \vee Q_l) C_\delta \gamma n \log \frac{1}{\gamma}}}
             {\sqrt{ \sum_k \hat{n}_k (\hat{n}_k - 1)}} + 
          \frac{C_\delta \gamma n \log \frac{1}{\gamma}}
             {\sum_k \hat{n}_k (\hat{n}_k - 1)} \\
  &\leq \frac{\sqrt{2 (P_l \vee Q_l) C_\delta \gamma K \log \frac{1}{\gamma}}}
             {\sqrt{ \max_k \hat{n}_k}}  + 
          \frac{C_\delta \gamma K \log \frac{1}{\gamma}}
             {\max_k \hat{n}_k } \\
  &\leq \sqrt{ \frac{ P_l \vee Q_l}{n}} \sqrt{ C_\delta \gamma K^2 \log \frac{1}{\gamma}} + 
    \frac{C_\delta \gamma K^2 \log \frac{1}{\gamma}}{n} 
\end{align*}
 
Since $\frac{1}{n} \leq \sqrt{ \frac{P_l \vee Q_l}{n}}$ by our assumption that $P_l \vee Q_l = \omega \left(\frac{1}{n} \right)$, we have that

$$| \hat{P}_l - P_l | = O\left( \sqrt{\frac{P_l \vee Q_l}{n} } \right)$$ 

Since $\frac{ P_l \vee Q_l }{n} \rightarrow 0$, we also have that $\hat{P}_l \geq \frac{1}{2} P_l$. Therefore, 

\begin{align*}
\frac{ | \hat{P}_l - \hat{Q}_l |}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} &\leq 
          \frac{| P_l - Q_l | + O \left( \sqrt{ \frac{P_l \vee Q_l}{n}} \right)}{ \sqrt{\frac{1}{2} P_l} } \\
  &\leq \sqrt{2} \frac{|P_l - Q_l|}{\sqrt{P_l}} + \sqrt{2} \sqrt{ \frac{1}{n}} (1 + \eta) \\
  &\leq C' \sqrt{ \frac{1}{n} } (1 + \eta)
\end{align*}

\end{proof}

\begin{theorem}
Suppose $\frac{ n I_{tot}}{K} \rightarrow \infty$ and let $L'$ be the set of colors such that $\limsup_{n \rightarrow \infty} \sqrt{n I_l} < \infty$. \\

Suppose the initialization algorithm is set with $\tau = \max_{l \in L'} \sqrt{2} 
( \limsup_{n \rightarrow \infty} n I_l  + 2)$. Then, the following holds with probability at least $1 - 2Ln^{-(3+\delta)}$.

\begin{enumerate}
\item 
Let $l^*$ be the color outputted by the initialization algorithm. Then, for all large enough $n$, we have that $l^*$ satisfies 
\[
\frac{n I_{l^*} }{K} \rightarrow \infty
\]
\item
A color $l$ is discarded by the initialization algorithm iff $l \in L'$. 
\end{enumerate}

\end{theorem}

\begin{proof}
By taking a union bound, we reason that conclusions of Proposition~\ref{prop:initial_guarantee} hold uniformly for all $l$ with probability at least $1 - 2L n^{-(3+\delta)}$.

By proposition~\ref{prop:consistency_possible}, there must exists a color $l$ such that $\frac{n I_l}{K} \rightarrow \infty$. By proposition~\ref{prop:initial_guarantee}, with probability at least $1 - 2 L n^{-(3 + \delta)}$, 

\begin{align*}
\frac{ |P_{l^*} - Q_{l^*}|}{\sqrt{ P_{l^*} \vee Q_{l^*}}} &\geq 
\frac{1}{\sqrt{2} + \eta} \frac{|\hat{P}_{l^*} - \hat{Q}_{l^*} | }{\sqrt{ \hat{P}_{l^*} \vee \hat{Q}_{l^*} }} \\
& \geq
\frac{1}{\sqrt{2} + \eta} \frac{|\hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} \\
 &\geq \frac{|P_l - Q_l|}{\sqrt{P_l \vee Q_l}} \frac{1/\sqrt{2} - \eta}{\sqrt{2} + \eta}  \\
\end{align*}

where we let $n$ be large enough such that $|\eta| < 1/\sqrt{2}$. Therefore, we see that $ \frac{n I_{l^*} }{K} \rightarrow \infty$. 

Let us turn to the second claim of the theorem. If $l \in L'$, then it is clear by proposition~\ref{prop:initial_guarantee} that $l$ will be discarded.

If $l \notin L'$, then, by proposition~\ref{prop:initial_guarantee}, we have that
\begin{align*}
\frac{|\hat{P}_l - \hat{Q}_l|}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} &\geq 
   (\frac{1}{\sqrt{2}} - \eta) \frac{ | P_l - Q_l|}{\sqrt{P_l \vee Q_l}} 
\end{align*}

For large enough $n$, $\frac{|P_l - Q_l|}{\sqrt{P_l \vee Q_l}} > \tau \sqrt{ \frac{1}{n}}$ and so $l$ would not be discarded. 

\end{proof}

\section{Continuous Distributions}

In this section, we suppose that the weights of within-cluster edges are drawn from a density $p$ and that of between-cluster edges are drawn from a density $q$.
\begin{align*}
A_{ij} &\sim p  \quad \trm{if $\sigma_0(i) = \sigma_0(j)$} \\
A_{ij} &\sim q  \quad \trm{if $\sigma_0(i) = \sigma_0(j)$}
\end{align*}

In this case, the continuous Renyi divergence is 
\[
I = -2 \log \int \sqrt{p(x)q(x)} dx 
\]

Under the oracle setting, the rate of recovery is similar to that of proposition~\ref{prop:weak_recovery_oracle}. 
\begin{proposition} 
\label{prop:weak_recovery_oracle_continuous}
(Oracle Setting Upper Bound for Continuous Distributions)
Assume $\frac{n I}{K \log K} \rightarrow \infty$. The maximum likelihood estimator $\hat{\sigma}$ in the oracle setting achieves:
\[
\sup_{\Theta(n, K, \beta, P, Q)} \E r(\hat{\sigma}, \sigma) \leq \left\{ 
    \begin{array}{cc} 
   \exp \left( - (1 + o(1)) \frac{nI}{2} \right ), \, & K=2, \\
   \exp \left( - (1 + o(1)) \frac{nI}{\beta K} \right ), \,& K\geq 3
  \end{array} \right. 
\]   
\end{proposition}
The proof is identical to that of proposition~\ref{prop:weak_recovery_oracle}, replacing sums with integrals where needed. 


\subsection{Rate Optimal Recovery}

Let us go to the general setting. Our goal is to show that under certain conditions, rate-optimal recovery is possible for continuous distributions.

\subsubsection{Assumptions}
\label{sec:continuous_assumptions}

\begin{enumerate}
\item[A1] $p(x), q(x)$ are supported on $[0,1]$, and $p(x) \geq c > 0$. 
\item[A2] Let $\gamma(x) = \frac{p(x) - q(x)}{\| p - q \|_2}$. Suppose that $|\gamma(x)| \leq M$ for some constant $M$. 
\item[A3] $|p'(x)|, |q'(x)|, |\gamma'(x)| \leq M'$ for some constant $M'$. 
\end{enumerate}


Let us see a specific example in which these generalizations are relevant. 

\begin{example}

Suppose that $f(x)$ is a density supported on $[0, 1-\alpha]$ and $g(x)$ is a density, bounded away from 0, and supported on $[0,1]$. We define $p(x), q(x)$ as mixtures.

\begin{align*}
p(x) &= \lambda f(x) + (1-\lambda) g(x)\\
q(x) &= \lambda f(x - \alpha) + (1-\lambda) g(x)
\end{align*}

where the mixing weight $1 > \lambda > 0$ is a constant. Since $g(x)$ is bounded away from 0, it is clear that assumption A1 is satisfied. 

We suppose that $f$ is twice-differentiable, $g$ is differentiable, and that $|f'(x)|, |g'(x)|, |f''(x)| \leq M$ for some constant $M > 0$. We suppose also that $\int f'(x)^2 dx > 0$.

\begin{align*}
p(x) - q(x) &= \lambda (f(x) - f(x-\alpha)) 
\end{align*}

Also, we have, by Taylor theorem, that
\begin{align*}
f(x - \alpha) &= f(x) + f'(x) \alpha + \frac{f''(x_1)}{2} \alpha^2 \quad \trm{for some $x_1$ in between $x, x-\alpha$} \\
\frac{f(x) - f(x-\alpha)}{\alpha} &= f'(x) + \frac{1}{2} f''(x_1) \alpha
\end{align*}

Therefore,
\begin{align*}
\int (p(x) - q(x))^2 dx &= \lambda^2 \alpha^2 \int 
   \left( \frac{f(x) - f(x - \alpha)}{\alpha} \right)^2 dx \\
  &= \lambda^2 \alpha^2 \int (f'(x) + \frac{1}{2} f''(x_1) \alpha)^2 dx \\
 &= \lambda^2 \alpha^2 \left( \int f'(x)^2 dx  + \int f'(x) f''(x_1) \alpha + f''(x_1)^2 \alpha^2 dx \right) 
\end{align*}

Since $|f'(x)|, |f''(x)| \leq M$, we conclude that, for some constant $C$, we have that
\[
\int (p(x) - q(x))^2 dx = C \alpha^2 + O(\alpha^3) 
\]

For all small enough $\alpha$, it is thus clear that
\[
\gamma(x) = \frac{p(x) - q(x)}{\| p - q \|_2} = C \cdot \frac{p(x) - q(x)}{\alpha} + O(\sqrt{\alpha})
\]
for some constant $C$ independent of $x, \alpha$. To bound $|\gamma(x)|$ and $|\gamma'(x)|$, it is thus sufficient to analyze $\tilde{\gamma}(x) = \frac{p(x) - q(x)}{\alpha}$. 

\begin{align*}
\left| \frac{ p(x) - q(x)}{\alpha} \right| &= \lambda \left| \frac{f(x) - f(x-\alpha)}{\alpha} \right| \\
 &= \left| f'(x) + \frac{1}{2} f''(x_1) \alpha \right| \\
 &\leq 2M  \quad \trm{for small enough $\alpha$}
\end{align*}

Thus, assumption A2 has been verified. 

\begin{align*}
\frac{ \tilde{\gamma}(x) - \tilde{\gamma}(x - c) }{c} &= 
 \frac{\lambda}{c} \left(   \frac{f(x) - f(x-\alpha)}{\alpha} - \frac{f(x-c) - f(x-c - \alpha)}{\alpha}  \right) \\
  &= \frac{\lambda}{\alpha} \left( \frac{f(x) - f(x-c)}{c} - 
                  \frac{ f(x-\alpha) - f(x - \alpha - c)}{c} \right) \\
 &\trm{ take the limit $c \rightarrow 0$} \\
\tilde{\gamma}'(x) &= \frac{\lambda}{\alpha} (f'(x) - f'(x-\alpha) ) 
\end{align*}

Thus, it is clear that $ | \tilde{\gamma}'(x) | \leq \lambda M$ and we have verified assumption A3. 

\end{example}

\subsubsection{Continuous and Discretized Renyi Divergence}

First, we show that, under the assumptions we have listed, the continuous Renyi divergence scales as $\alpha^2$. 

\begin{proposition}
\label{prop:continuous_renyi_order}
Let $I = -2 \log \int \sqrt{p(x)q(x)} dx$ be the continuous Renyi divergence.

Suppose assumptions A1 and A2 are satisfied with constants $M, c$. 

We have that, with $d = \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx$,
\[
I = d \alpha^2 ( 1 + \eta )
\]
where, for any $\alpha < \frac{c}{4M}$, $|\eta| \leq \frac{12 M}{c} \alpha$. 

\end{proposition}

\begin{proof}

First, denote $H = \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx$ as the continuous Hellinger distance and we will first show that $c_1 \alpha^2 \leq H \leq c_2 \alpha^2$ for some constants $c_1, c_2$.\\

\begin{align*}
&\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \\
=& \int ( \sqrt{p(x)} - \sqrt{p(x) - \gamma(x) \alpha} )^2 dx \\
=& \int p(x) \left( 1 - \sqrt{ 1 - \frac{\gamma(x) \alpha}{p(x)}} \right)^2 dx 
\end{align*}

Since $|\gamma(x)| \leq M$ and $p(x) \geq c$, we have that, for any $|\alpha| \leq \frac{c}{2M}$, $\left| \frac{\gamma(x)}{p(x)} \alpha \right| \leq \frac{1}{2}$. Thus, we can take Taylor series expansion of $f(z) = \sqrt{1 - z}$ around 0 and plug in $\frac{\gamma(x) \alpha}{p(x)}$. \\


Therefore, there exists some function $\xi(\alpha, x)$ satisfying $|\xi(\alpha, x)| \leq 
\frac{4M}{c} \alpha$ such that

\begin{align*}
=& \int p(x) \left( 1 - (1 - \frac{1}{2} \frac{\gamma(x) \alpha}{p(x)} (1 + \xi(\alpha, x)) ) \right)^2 dx \\
=& \int p(x) \left( \frac{1}{2} \frac{\gamma(x) \alpha}{p(x)} (1 + \xi(\alpha, x)) \right)^2 dx\\
=& \alpha^2 \int p(x) \left( \frac{1}{2} \frac{\gamma(x) }{p(x)} \right)^2 (1+\xi(\alpha, x))^2 dx \\
=& \alpha^2 \left( \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx \right) (1 + \eta) 
\end{align*}

where $\eta = \frac{ \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 ( 2 \xi(x, \alpha) + \xi(x, \alpha)^2 )  dx }
             { \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx}$. Since $| \xi(\alpha, x) | \leq \frac{4M}{c} \alpha \leq 1$, 
\begin{align*}
|\eta| & 
\leq \frac{1}{ \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx } 
  \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 \frac{12 M}{c} \alpha dx \\
&\leq \frac{12 M}{c} \alpha
\end{align*} 

Now, it remains to bound $I$ in terms of $H$. Since
\begin{align*}
I =& -2 \log \int \sqrt{p(x)q(x)} dx \\
 =& -2 \log \left( 1 - \frac{1}{2} \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right) \\
 =& -2 \log (1 - \frac{1}{2} H) 
\end{align*}

If $\alpha \leq \frac{c}{4M}$, then $H \leq \frac{1}{16}$, thus, we have that

\[
(1 + \eta') H \geq I \geq H
\]

where $|\eta'| \leq \frac{1}{4} H \leq \frac{1}{4} \alpha^2 d (1 + \eta)$.  

This completes the proof with $d = \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx$. 

\end{proof}


Next, we define the \textbf{discretized Renyi divergence}. Let the interval $[0,1]$ be divided into $L$ equally spaced sub-intervals. Let $l$ index each of these sub-intervals and let $B = 1/L$ be the length of each sub-interval.

Let $P_l = \int_{\trm{Bin}_l} p(x) dx$ and $Q_l = \int_{\trm{Bin}_l} q(x)dx$. We define the \emph{discretized Renyi divergence} as $\tilde{I} = -2 \log \sum_{l=1}^L \sqrt{P_l Q_l} $ and $\tilde{H} = \sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2$. 


\begin{proposition}
\label{prop:discrete_renyi_order}
Let $L \geq 1$ be arbitrary and let $\tilde{I}_L = -2 \log \sum_{l=1}^L \sqrt{P_l Q_l}$ be the discretized Renyi divergence. 

Suppose assumptions A1 and A2 hold. 

Let $\gamma_l = \int_{\trm{Bin}_l} \gamma(x) dx$ and $d_L = \sum_l P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 $

Then, we have that,
\[
 \tilde{I}_L = d_L \alpha^2 ( 1 + \eta_L )
\]
where, for any $\alpha < \frac{c}{2M}$, $|\eta_L| \leq \frac{8M}{c} \alpha$.

\end{proposition}

\begin{proof}

Again, we first analyze the discretized Hellinger distance. The discretized Renyi divergence can be bounded in terms of the discretized Hellinger distance in the same fashion as the continuous case. 

\begin{align*}
\tilde{H} &= \sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2 \\
   &= \sum_{l=1}^L P_l \left( 1 - \sqrt{Q_l}{P_l} \right)^2 \\
 &= \sum_{l=1}^L P_l \left( 1 - \sqrt{1 - \frac{P_l - Q_l}{P_l}} \right)^2 
\end{align*}

We simplify the term $P_l - Q_l$.
\begin{align*}
P_l - Q_l &= \int_{\trm{Bin}_l} p(x) - q(x) dx \\
  &= \int_{\trm{Bin}_l} \gamma(x) \alpha dx \\
  &= \gamma_l \alpha 
\end{align*}

where $|\gamma_l| \leq M B$. 

\[
P_l = \int_{\trm{Bin}_l} p(x) dx \leq c B
\]

For $\alpha < \frac{c}{2M}$, we have that $\frac{\gamma_l}{P_l} \alpha < \frac{1}{2}$ and thus, there exists $\eta_l$ satisfying $|\eta_l| \leq \frac{4M}{c} \alpha$ such that

\begin{align*}
\tilde{H} &= \sum_{l=1}^L P_l \left( 1 - \sqrt{ 1 - \frac{\gamma_l \alpha}{P_l}} \right)^2
\\
&= \sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l \alpha}{P_l} (1 + \eta_l) \right)^2 \\
&= \alpha^2 \sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 (1 + \eta_l)^2 \\
&= \alpha^2 \left( \sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 \right) ( 1 + \eta_L) 
\end{align*}

where $\eta_L = \frac{ 
  \sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 (2 \eta_l + \eta_l^2)}
{\sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2} 
$. And since $|\eta_l| \leq \alpha \frac{4M}{c} < 1$, we have that

\begin{align*}
|\eta_L| \leq \frac{12 M}{c} \alpha
\end{align*}

The claim thus follows. 

\end{proof}


Next, we will show that $\lim_{L \rightarrow \infty} \sup_\alpha \left| \frac{d_L}{d} - 1 \right| = 0$. 

\begin{proposition}
\label{prop:convergence_discrete_continuous_renyi}

Let $d = \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx$ and $d_L = \sum_l P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2$. 

Suppose assumptions A1, A3 hold. Then, we have that
\[
\lim_{L \rightarrow \infty} \sup_\alpha \left| \frac{d_L}{d} - 1 \right| = 0
\]

\end{proposition}


Note that $B \rightarrow 0$ is equivalent to $L \rightarrow \infty$ since $B = 1/L$. 

\begin{proof}

Let $\trm{Bin}_l = [a_l, b_l]$. 
\begin{align*}
P_l &= \int_{\trm{Bin}_l} p(x) dx \\
  &= \int_{a_l}^{b_l} p(x) dx \\
 &= \int_{a_l}^{b_l} p(a_l) + p'(c_x) (x - a_l) dx \quad \trm{for some $c_x \in [a_l, b_l]$}\\
 &= B p(a_l) + B^2 \xi_l \quad \trm{where $|\xi_l| \leq M'/2$}
\end{align*}

Likewise, we have that $\gamma_l = B \gamma(a_l) + B^2 \xi'_l$ for some $\xi'_l$ such that $|\xi'_l | \leq M'/2$. 

\begin{align*}
d_L &= \sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 \\
  &= \sum_{l=1}^L B \left(
     (p(a_l) + B \xi_l) \left( \frac{1}{2} \frac{\gamma(a_l) + B \xi'_l}{p(a_l) + B \xi_l} \right)^2 \right) 
\end{align*}

We will simplify this expression step by step, using the fact that $|\xi_l|, |\xi'_l| \leq M'/2$ and that $|\gamma(a_l)| \leq M$ and $|p(a_l)| \geq c$. 

\begin{align*}
d_L = \sum_{l=1}^L B
     p(a_l)  \left( \frac{1}{2} \frac{\gamma(a_l) + B \xi'_l}{p(a_l) + B \xi_l} \right)^2 + \sum_{l=1}^L B^2 \xi_l  \left( \frac{1}{2} \frac{\gamma(a_l) + B \xi'_l}{p(a_l) + B \xi_l} \right)^2 
\end{align*}

The second term is of magnitude at most $B M' \frac{1}{4} \left( \frac{M + B (M'/2)}{c - B (M'/2)} \right)^2$. This is clearly $c_{M, M', c} B$ for some constant $c_{M, M', c}$ independent of $n$. 

We now turn to the first term.
\begin{align*}
& \sum_{l=1}^L B
   p(a_l)  \left( \frac{1}{2} \frac{\gamma(a_l) + B \xi'_l}{p(a_l) + B \xi_l} \right)^2 \\
&= \sum_{l=1}^L B p(a_l) \left( \frac{1}{4} \frac{\gamma(a_l) + B \xi_l'}{p(a_l)} \right)^2
   \left( \frac{1}{ 1 + B \frac{\xi_l}{p(a_l)} } \right)^2 \\
&\leq \sum_{l=1}^L B p(a_l) \left( \frac{1}{4} 
    \frac{ \gamma(a_l)^2 + B \xi'_l \gamma(a_l) + (B \xi'_l)^2}{p(a_l)^2} \right) ( 1 + \xi''_l) 
\end{align*} 

where $\xi''_l$ satisfies $| \xi''_l | \leq \frac{3}{8} B M'/c$ for all $B \leq c / M'$. From here, it is straightforward to verify that the first term is 
\[
\sum_{l=1}^L Bp(a_l) \left( \frac{1}{2} \frac{\gamma(a_l)}{p(a_l)} \right)^2 + \trm{remainder}
\]
where the remainder term is bounded by magnitude by $c_{M, M', c}B$ for some constant $c_{M, M', c}$ independent of $n$. 

Define $d_R = \sum_{l=1}^L B p(a_l) \left( \frac{1}{2} \frac{\gamma(a_l)}{p(a_l)} \right)^2 $ where $R$ stands for Riemann sum. What we have just shown is that

\[
| d_R - d_L | \leq c_{M, M', c} B 
\]

Now, we look at $| d_R - d |$:
\begin{align*}
|d_R - d| &= \frac{1}{4} \left| \int \frac{\gamma(x)^2}{p(x)} dx - \sum_l B \frac{\gamma(a_l)^2}{p(a_l)} \right| \\
 &\leq \sup_x \left| \frac{ d \frac{\gamma(x)^2}{p(x)} }{dx} \right| B^2 \\
 &\leq \sup_x \left| \frac{ 2 \gamma'(x) \gamma(x) p(x) - p'(x) \gamma(x)^2 }{p(x)^2} \right| B^2 \\
 &\leq \frac{2 M^2 M' }{c^2} B^2 
\end{align*}

Thus, we have that
\begin{align*}
\left|\frac{d_L}{d} - 1 \right| &= \frac{| d_L - d |}{d} \\
  &\leq \frac{1}{d} \left( |d_L - d_R| + |d_R - d| \right) \\
  &\leq \frac{1}{d} c_{M, M', c} B 
\end{align*}

Since $\int \gamma(x)^2 dx = 1$, we have that $d = \frac{1}{4} \int \frac{\gamma(x)^2}{p(x)} dx \leq \frac{1}{4c} \int \gamma(x)^2 dx = \frac{1}{4c}$. Thus

\[
\lim_{L \rightarrow \infty} \sup_n  \left| \frac{d_L}{d} - 1 \right| = 0
\]
 
\end{proof}


These propositions together imply the following:
\begin{shaded}
\begin{theorem}
\label{thm:relative_convergence_discrete_continuous_renyi}
Suppose assumptions A1-A3 are satisfied. Let $\tilde{I}_L$ be the discretized Renyi divergence discretized at level $L$.

Let $n \rightarrow \infty$, then, we have that, for \textbf{any sequences} $L_n \rightarrow \infty, \alpha_n \rightarrow 0$,

\[
\lim_{n \rightarrow \infty} \left| \frac{\tilde{I}_L}{I} - 1 \right| \rightarrow 0
\]

\end{theorem}
\end{shaded}

\begin{proof}

By proposition~\ref{prop:continuous_renyi_order} and proposition~\ref{prop:discrete_renyi_order}, we have that, for all $\alpha < \frac{c}{4M}$,

\begin{align*}
| \tilde{I}_L - I | 
 & \leq \left| d_L \alpha^2 ( 1 + \eta_L) - d \alpha^2 ( 1 + \eta) \right| \\
  &\leq d\alpha^2 \left| \frac{d_L}{d} (1 + \eta_L) - (1+\eta) \right|  \\
 &\leq I \left| \frac{d_L}{d} \frac{(1 + \eta_L)}{(1+\eta)} - 1 \right| \\
& \Rightarrow \\
\left| \frac{\tilde{I}_L}{I} - 1 \right| &\leq 
\left | \frac{d_L}{d} \frac{(1+\eta_L)}{1 + \eta} - 1 \right|
\end{align*}

where $|\eta|, |\eta_L| \leq \frac{12M}{c} \alpha$ for all $L$. Thus, it is clear that 
\[
\lim_{\alpha_n \rightarrow 0} \sup_L \left| \frac{1+\eta_L}{1+\eta} - 1 \right| = 0
\]

Furthermore, it has been shown ... that
\[
\lim_{L_n \rightarrow \infty} \sup_\alpha \left| \frac{d_L}{d} - 1 \right| = 0
\]

Let $\epsilon > 0$ be arbitrarily fixed. 
Choose $\alpha$ such that $\sup_L \left| \frac{1+\eta_L}{1+\eta} - 1 \right| \leq \epsilon/3$ and choose $L$ such that $\sup_\alpha \left| \frac{d_L}{d} -1 \right| \leq \epsilon/3$. 

Choose $n_0$ such that $\alpha_n < \alpha$ and $L_n < L$ for all $n > n_0$. 

Then, we have that, for all $n > n_0$:
\begin{align*}
& \left| \frac{d_L}{d} \frac{1+\eta_L}{1+\eta} - 1 \right | \\
&= \left| \frac{d_L}{d} \frac{1 + \eta_L}{1 + \eta} - \frac{d_L}{d} + \frac{d_L}{d} - 1 \right| \\
&= \left| \frac{d_L}{d} \left( \frac{1+\eta_L}{1+\eta} - 1 \right) + \left( \frac{d_L}{d} - 1 \right) \right| \\
&= \left| \left( \frac{1+\eta_L}{1 + \eta} - 1 \right) + \left( \frac{d_L}{d} -1 \right) \left( \frac{1+\eta_L}{1+\eta} - 1 \right) + \left( \frac{d_L}{d} - 1 \right) \right| \\
&\leq  \left| \frac{1+\eta_L}{1 + \eta} - 1 \right| + \left| \left( \frac{d_L}{d} -1 \right) \left( \frac{1+\eta_L}{1+\eta} - 1 \right) \right| + \left| \frac{d_L}{d} - 1  \right| \\
&\leq \epsilon
\end{align*}

The claim thus follows. 

\end{proof}

\subsubsection{Recovery Procedure}

\begin{enumerate}
\item For each edge $(i,j)$, set $\tilde{A}_{ij} = \mathbf{I}(A_{ij} < \tau)$ for some $\tau \in (0, 1)$. Use the $\tilde{A}_{ij}$ labels for the initial rough clustering.
\item Bin the interval $[0,1]$ into $L$ bins and estimate $P_l, Q_l$.
\item Refine clustering with MLE based on $\hat{P}_l, \hat{Q}_l$.
\end{enumerate}


The initial clustering is consistent if we assume that $\left| \int_0^{\tau} \gamma(x) dx \right| = c_1 > 0$. Let $\bar{P} = \int_0^{\tau} p(x) dx, \bar{Q} = \int_0^{\tau} q(x) dx$, $\bar{P} - \bar{Q} = c_1 \alpha$. Furthermore, $\bar{P}, \bar{Q} \geq c$. Thus, 
\[
\bar{I} = -2 \log \left( \sqrt{\bar{P} \bar{Q}} + \sqrt{(1-\bar{P})(1-\bar{Q})} \right) = \Theta( \alpha^2)
\]

The discretization step is fine so long as $L \rightarrow \infty$. 




\newpage
\section{Technical Lemmas}

The following two lemmas give an alternative form to the Renyi-divergence. 

\begin{lemma}
\label{lem:simplify_renyi2}
Let $P = \{ P_l \}_{l = 0,..., \infty}$ and $Q = \{ Q_l \}_{l=0,...,\infty}$ be two discrete distributions and suppose $\sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l} )^2 \rightarrow 0$. 

Let $I = - 2 \log \sum_l \sqrt{ P_l Q_l}$.

Then, we have that $I \rightarrow 0$ and 
\[
I = (1 + o(1)) \sum_{l = 1}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 
\]
\end{lemma}



\begin{lemma}
\label{lem:simplify_renyi}
Let $P = \{ P_l \}_{l = 0,..., \infty}$ and $Q = \{ Q_l \}_{l=0,...,\infty}$ be two discrete distributions and suppose $P_0, Q_0 \rightarrow 1$. 

Let $I = - 2 \log \sum_l \sqrt{ P_l Q_l}$.

Then, we have that $I \rightarrow 0$ and 
\[
I = (1 + o(1)) \sum_{l = 1}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 
\]

\end{lemma}

\begin{proof}
First, it is clear that if $P_0, Q_0 \rightarrow 1$, then 
\begin{align*}
\sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 &= 
   (\sqrt{P_0} - \sqrt{Q_0})^2 + \sum_{l=1}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 \\
 &= (\sqrt{P_0} - \sqrt{Q_0})^2 + \sum_{l=1}^\infty P_l + \sum_{l=1}^\infty Q_l - 
  2 \sum_{l=1}^\infty \sqrt{P_l Q_l} \\
 &\leq  (\sqrt{P_0} - \sqrt{Q_0})^2 + \sum_{l=1}^\infty P_l + \sum_{l=1}^\infty Q_l \\
\end{align*}

Therefore, $\lim_{n\rightarrow \infty} \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 = 0$. 

\begin{align*}
I &= -2 \log \sum_{l=0}^\infty \sqrt{P_l Q_l} \\
  &= -2 \log \left( 1 - \frac{1}{2} \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 \right) \\ 
  &= (1 + o(1)) \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 \quad \trm{(since the sum tends to 0)}
\end{align*}

We will show that $(\sqrt{P_0} - \sqrt{Q_0})^2 = o \left( \sum_{l=1}^\infty (\sqrt{P_l} - \sqrt{Q_l} )^2 \right)$ and the result follows immediately.

Let $P' = 1 - P_0$ and $Q' = 1 - Q_0$. 
\begin{align*}
(\sqrt{P_0} - \sqrt{Q_0})^2 &= (\sqrt{1-P'} + \sqrt{1-Q'})^2 \\
  &= (1-P') \left( 1 - \sqrt{ \frac{1-Q'}{1-P'}} \right)^2 \\
 &= (1-P') \left( 1 - \sqrt{ 1 - \frac{Q' - P'}{1 - P'} } \right)^2 \\
 &\leq (1-P') \left( 1 - (1 - \frac{1}{2} \left( \frac{Q' - P'}{1 - P'} \right) (1+o(1)) ) \right)^2 \\
 &\leq (1 - P') \left( \frac{1}{2} \left( \frac{Q'-P'}{1-P'} \right) (1+o(1)) \right)^2 \\
 &\leq \frac{1}{4} \left( \frac{Q' - P'}{1 - P'} \right)^2 (1 + o(1)) \leq \frac{1}{4} ( Q' - P')^2 (1 + o(1)) 
\end{align*}

\begin{align*}
\sum_{l=1}^\infty (\sqrt{P_l}  - \sqrt{Q_l})^2 &= \sum_{l=1}^\infty P_l + Q_l - 2 \sqrt{P_lQ_l} \\
 &\geq P' + Q' - 2 \sqrt{ \left( \sum_{l=1}^\infty P_l \right) \left( \sum_{l=1}^\infty Q_l \right) } \\
 &= P' + Q' - 2\sqrt{P' Q'} \\
 &= (\sqrt{P'} - \sqrt{Q'})^2 \\
 &= P' \left( 1 - \sqrt{ \frac{Q'}{P'} } \right)^2 \\ 
 &= P' \left( 1 - \sqrt{ 1 - \frac{P' - Q'}{P'} } \right)^2 \\
 &\geq P' \left( 1 - ( 1 - \frac{1}{2} \frac{P' - Q'}{P'} (1 + o(1)) ) \right)^2 \\
 &\geq P' \left( \frac{1}{2} \frac{P' - Q'}{P'} (1 + o(1)) \right)^2 \\
 &\geq \frac{1}{4} \left( \frac{(P' - Q')^2}{P'} \right) (1 + o(1)) 
\end{align*}

Thus, we have shown that 
\begin{align*}
(\sqrt{P_0} - \sqrt{Q_0})^2& \leq \frac{1}{4} (Q' - P')^2 (1 + o(1)) \\
\sum_{l=1}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 & \geq \frac{1}{4} \frac{(P' - Q')^2}{P'} (1 + o(1))
\end{align*}

Since $P' \rightarrow 0$, the proof is complete.

\end{proof}

\section{Reference Results}

\subsection{Existing Results from Literature}

Let $\Theta_0(n, k, p, q, \beta)$ be the parameter space of homogeneous stochastic block model with $p$ as the within-cluster probability and $q$ as the between-cluster probability. The following theorem follows from Theorem 3 and Proposition 1 of \cite{gao2015achieving}. 

\begin{theorem}
\label{thm:spectral_rate}
Assume $p \leq C_1 q$ and that $p, q = \Omega( \frac{1}{n} )$ and suppose that $n \geq 2 \beta k$. Suppose there is some $c \in (0,1)$ such that 

$$ \frac{k^3 p}{(p-q)^2 n^2} \leq c$$. 

Suppose we apply Unnormalized-Spectral-Clustering with trim constant $\tau = C_2 \bar{d}$ and a sufficiently small post-processing constant $\mu > 0$. Then, for any constant $C'$, there exists some $C > 0$ dependent only on $C', C_1, C_2, \mu$ such that

\[
l(\hat{\sigma}, \sigma_0) \leq C \frac{\beta^2 k^2 p}{(p-q)^2 n }
\]

with probability at least $1 - n^{-C'}$. 

\end{theorem}

We note that $\frac{(p-q)^2}{p} = I$. Restated, this theorem says that if $p \asymp q$ and if $ \frac{k}{nI} \rightarrow 0$, then, the error rate $\gamma$ of spectral clustering goes to zero with probability $1 - n^{-C'}$ for any constant $C' > 0$. 

\bibliographystyle{plain}
\bibliography{note}

\end{document}
