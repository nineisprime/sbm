\documentclass{article}
%\documentclass[12pt,pdftex,generic,noinfoline]{imsart}

%TODO
%Modify algorithm to take into account the noise adding state
%Change the tau and other stuff in algorithm descriptions

\RequirePackage[OT1]{fontenc}
\usepackage{amsthm,amsmath,amsfonts,natbib,mathtools,custom_math,amssymb, bbm}
\RequirePackage{hypernat}
\usepackage[ruled,section]{algorithm}
%\usepackage[noend]{algorithmic}
\usepackage{algpseudocode}
\usepackage{graphicx}
%\usepackage[hscale=0.7,vscale=0.8]{geometry}
\usepackage[text={6.0in,8.6in},centering]{geometry}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage{yhmath}
\usepackage[usenames,dvipsnames]{xcolor}
\definecolor{shadecolor}{gray}{0.9}
\definecolor{shadecolor}{gray}{0.9}
\hypersetup{citecolor=blue}
\hypersetup{linkcolor=blue}
\hypersetup{urlcolor=blue}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{color}
%This is for repeating theorems, lemmas
\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
\newenvironment{rep#1}[1]{%
 \def\rep@title{#2 \ref{##1}}%
 \begin{rep@theorem}}%
 {\end{rep@theorem}}}
\makeatother
%end repeat theorems
\newreptheorem{theorem}{Theorem} %repeat theorem
\newreptheorem{lemma}{Lemma}      %repeat lemma
\newreptheorem{proposition}{Proposition} %repeat proposition  

\newcommand{\bin}{\text{bin}}
\allowdisplaybreaks

\renewcommand{\baselinestretch}{1.045}
%\parskip12pt
%\parindent0pt
\setcounter{tocdepth}{2}

\input{macros}

\begin{document}

\begin{center}
{\bf{\Large{Community Recovery on the Weighted Stochastic Block Model and Its Information-Theoretic Limits}}}

\vspace*{.25in}

\begin{tabular}{ccccc}
{\large{Min Xu$^\dagger$}} & \hspace*{.2in} & {\large{Varun Jog$^\ddagger$}} & \hspace*{.2in} & {\large{Po-Ling Loh$^{\ddagger*}$}} \\
{\large{\texttt{minx@wharton.upenn.edu}}} & & {\large{\texttt{vjog@wisc.edu}}} & & {\large{\texttt{loh@ece.wisc.edu}}}
\end{tabular}

\vspace{.2in}

\begin{tabular}{ccc}
Department of Statistics$^\dagger$ & \hspace{.3in} & Departments of ECE$^\ddagger$ \& Statistics$^*$ \\
The Wharton School && Grainger Institute of Engineering \\
University of Pennsylvania && University of Wisconsin - Madison \\ Philadelphia, PA 19104 & & Madison, WI 53706
\end{tabular}
 
\vspace*{.2in}

January 2017

\vspace*{.2in}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Identifying communities in a network is an important problem in many fields, including social science, neuroscience, military intelligence, and genetic analysis. In the past decade, the Stochastic Block Model (SBM) has emerged as one of the most well-studied and well-understood statistical models for this problem. Yet, the SBM has an important limitation: it assumes that each network edge is drawn from a Bernoulli distribution. This is rather restrictive, since weighted edges are fairly ubiquitous in scientific applications, and disregarding edge weights naturally results in a loss of valuable information. In this paper, we study a weighted generalization of the SBM, where observations are collected in the form of a weighted adjacency matrix, and the weight of each edge is generated independently from a distribution determined by the community membership of its endpoints. We propose and analyze a novel algorithm for community estimation in the weighted SBM based on various subroutines involving transformation, discretization, spectral clustering, and appropriate refinements. We prove that our procedure is optimal in terms of its rate of convergence, and that the misclassification rate is characterized by the Renyi divergence between the distributions of within-community edges and between-community edges. In the regime where the edges are sparse, we also establish sharp thresholds for exact recovery of the communities. Our theoretical results substantially generalize previously established thresholds derived specifically for unweighted block models. Furthermore, our algorithm introduces a principled and computationally tractable method of incorporating edge weights to the analysis of network data.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip10pt


\section{Introduction}


The recent explosion of interest in network data has created a need for new statistical methods for analyzing network datasets and interpreting results~\cite{NewEtal06, DavKle10, Jac10, GolEtal10}. One active area of research with diverse applications in many scientific fields pertains to community detection and estimation, where the information available consists of the presence or absence of edges between nodes in the graph, and the goal is to partition the nodes into disjoint groups based on their relative connectivity~\cite{FieEtal85, HarSha00, PriEtal00, ShiMal00, McS01, NewGir04}.

A standard assumption in statistical modeling is that conditioned on the community labels of the nodes in the graph, edges are generated independently according to fixed distributions governing the connectivity of nodes within and between communities in the graph. This is the setting of the stochastic block model (SBM)~\cite{HolEtal83, HarEtal76, WasAnd87}. In the homogeneous case, edges follow one distribution when both endpoints are in the same community, regardless of the community label; and edges follow a second distribution when the endpoints are in different communities. The majority of existing literature on stochastic block models has focused on the case where no other information is available beyond the unweighted adjacency matrix, and much work in the information theory and statistics has focused on  deriving thresholds for \emph{exact} or \emph{weak} recovery of community labels in terms of the underlying probability parameters and the size of the graph (e.g., \cite{MosEtal12, MosEtal14, Mas14, AbbEtal14, abbe2014exact, AbbSan15, HajEtal14, HajEtal15, zhangminimax}).

However, the pairwise connections in many real-world networks possess a natural weighting structure~\cite{New04, BocEtal06}. For example, in social networks, information may be available quantifying the strength of a tie, such as the frequency of interactions between the individuals~\cite{Sad72}; in cellular networks, information may be available quantifying the frequency of communication between users~\cite{BloEtal08}; in gene co-expression networks, edges weights range from -1 to 1 and indicate the correlation between the expression levels of a gene pair; and in neural networks, edge weights may symbolize the level of neural activity between regions in the brain~\cite{RubSpo10}. Of course, the connectivity data could be condensed into an adjacency matrix consisting of only zeros and ones, but this would result in a loss of valuable information that could be used to recover node communities.

In this paper, we analyze the ``weighted" setting of the stochastic block model~\cite{aicher2014learning}, where, after an edge is generated from a Bernoulli distribution, it is given an edge weight generated from one of two arbitrary densities $p(x), q(x)$ depending on whether the edge is between-cluster or within-cluster. The weighted SBM presents a serious challenge in the design of algorithms because $p(x), q(x)$ are unknown and must be estimated. Nonparametric estimation of a density is a difficult problem in its own right and it is made much harder in the weighted SBM because one does not know whether an edge weight is drawn from $p(x)$ and $q(x)$ without the latent cluster structure. There are various approaches to the weighted SBM. For example, Newman \cite{New04} assumes that the edge weights have discrete units and then converts a weighted graph into a multigraph; Aicher et al \cite{aicher2014learning} assumes $p(x), q(x)$ to be from a known exponential family and performs variational Bayesian inference. These approaches can be effective but they rely on strong assumptions to simplify the problems and nothing is known about their theoretical properties. 

Our paper proposes a new discretization based approach that imposes weak assumptions and possesses strong guarantees. In the case of finitely-supported distributions, which correspond to a ``labeled" or ``colored" SBM, we demonstrate a method for choosing an initial label on which we apply a standard SBM estimation method to obtain an initial clustering. We then show how to use this initial rough clustering, together with the full set of edge labels, to obtain more accurate estimates of the true cluster assignments. In the case of continuous weight distributions, we propose a discretization strategy that will allow us to apply a recovery algorithm for the labeled case after appropriate preprocessing. Our method does not rely on prior knowledge of the densities $p(x)$ and $q(x)$ and does not rely on parametric assumptions.

Importantly, we show that the output of our algorithm is optimal, in the sense that under mild regularity assumptions on $p(x)$ and $q(x)$, the misclustering error of our algorithm converges to zero at an optimal rate. Our analysis generalizes the results of Zhang and Zhou~\cite{zhangminimax} and Gao et al~\cite{gao2015achieving}, which show that the optimal rate of convergence of unweighted SBM is driven by the Renyi divergence of order $1/2$ between two Bernoulli distributions, corresponding to the probability of generation for within-community and between-community edges. In fact, a similar phenomenon holds for the weighted SBM setting in our paper: the optimal error rate is also driven by a Renyi divergence of order $1/2$ between two mixed distributions that capture both the divergence between the edge probabilities and the divergence between the edge weight densities $p(x)$ and $q(x)$. Note that in order to achieve the optimal error rate, our discretization strategy must be chosen carefully when $p(x)$ and $q(x)$ are continuous distributions. Our proposed algorithm first transforms the distributions to be supported on $[0,1]$, then bins the interval appropriately; in general, since $p(x)$ and $q(x)$ may vary with the size of the graph, the number of bins used will also need to grow slowly as the number of nodes increases. Our results has an interesting implication: although our algorithm is nonparametric, it is adaptive in the sense that it achieves the same optimal rate even if the edge weight densities $p(x), q(x)$ take on a parametric form such as Gaussian or Laplace. This is in contrast to most problems in statistics where nonparametric methods usually have slower rate of convergence than parametric methods in settings where a parametric form is known. This observation captures an important intuition behind our results, that on the weighted SBM, one do not need to estimate the densities well in order to cluster well.

%Our method first discretizes the continuous edge weights to convert the weighted network into a labeled network~\cite{yun2016optimal}, which can then be clustered by modifying existing techniques for the unweighted stochastic block model. This method is computationally tractable and does not impose any parametric assumption on the weight densities.

%We also explore the related problem of exact recovery for weighted SBMs. Exact recovery refers to the case where the communities are partitioned perfectly, and a corresponding estimator is called \emph{strongly consistent}. We analyze the performance of our algorithm in the case when the average number of edges scales according to $\Theta(\log n)$, known as the \emph{sparse} regime in SBM literature. Again, we show that the thresholds for exact recovery may be expressed in terms of the Renyi divergence between weighted distributions, in the sense that our algorithm exactly recovers the true community labels when the Renyi divergence exceeds a certain threshold, and every algorithm fails with nontrivial probability when the Renyi divergence lies below the threshold.

%% TODO CONTINUE!

The remainder of the paper is organized as follows: Section~\ref{sec:formulation} introduces the mathematical framework of the weighted stochastic block model and defines the problems which we are trying to solve. Section~\ref{sec:method} describes our proposed algorithm for finding communities on the weighted SBM. In Section~\ref{sec:rate} we provide the statements of our main results concerning the behavior of our algorithm in terms of misclassification error rates and exact recovery. Section~\ref{SecProofs} highlights the key technical components employed in the analysis of our algorithm. We close in Section~\ref{sec:conclusion} with further implications of our work and open questions related to our results. %Note that some of the content in this paper overlaps with an earlier version of our manuscript~\cite{JogLoh15}, which focused on finitely-supported edge weight distributions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Model and problem formulation}
\label{sec:formulation}

We begin with a formal definition of the weighted SBM and a description of our error metrics for clustering.

\subsection{Model definition}

Consider a network with $n$ nodes and $K \geq 2$ communities. In this paper, we suppose that the communities are approximately balanced; that is, there exists a \emph{cluster-imbalance constant} $\beta$ such that the cluster size $n_k$ for each cluster $k = 1, \dots, K$ satisfies $\frac{\beta n}{K} \geq n_k \geq \frac{n}{\beta K}$. For each node $u$, we let $\sigma(u) \in \{1,2, \dots, K\}$ denote the community assignment of the nodes. 

\begin{definition} (Homogeneous Stochastic Block Model) An edge random variable $A_{uv}$ has the following distribution:
\[
A_{uv} \sim \left\{ \begin{array}{cc}
 Ber(p) & \trm{ if } \sigma(u) = \sigma(v) \quad \text{ and } \\
 Ber(q) & \trm{ if } \sigma(u) \neq \sigma(v). 
\end{array} \right.
\]
\end{definition}
In the more general case of \emph{heterogenous} SBM, we have a $K \times K$ matrix $P$ where each entry $P_{ij} \in [0,1]$. The edge random variable is drawn from $A_{uv} \sim Ber(P_{\sigma(u), \sigma(v)})$. We focus on the homogeneous case in this paper but discuss how to extend our results to the heterogenous setting.

SBM gives a distribution over the set of all networks whose edges are binary. To adapt to networks with continuous edge weights, we generalize the homogenous SBM by adding a second step to the data generating process: an edge weight is sampled from a continuous distribution after it is generated. 

\begin{definition}
\label{def:weighted_homo_sbm1}
(Weighted Homogeneous SBM) Let $0 < P_0, Q_0 < 1$ and let $p(x), q(x)$ be two densities. We first generate the edge presence indicator $Z_{uv}$:
\[
Z_{uv} \sim 
    \left\{ \begin{array}{cc}
    Ber(1-P_0) & \trm{ if } \sigma(u) = \sigma(v) \quad \text{and} \\
    Ber(1-Q_0) & \trm{ if } \sigma(u) \neq \sigma(v).
   \end{array} \right.
\]
The edge weight random variable is then:
\[
A_{uv} \sim \left\{ \begin{array}{cc} 
     0 & \trm{ if } Z_{uv} = 0 \\
     p(x) & \trm{ if } Z_{uv} = 1 \trm{ and } \sigma(u) = \sigma(v) \quad \text{ and }\\
     q(x) & \trm{ if } Z_{uv} = 1 \trm{ and } \sigma(u) \neq \sigma(v). 
\end{array} \right.
\]
\end{definition}

In this model, an edge is missing with probability either $P_0$ or $Q_0$ depending on whether the potential edge connects two nodes in the same cluster or in different clusters. If the edge is present, then it is given an edge weight drawn from either the density $p(x)$ or $q(x)$, depending again on the nature of the edge. If $p(x)$ and $q(x)$ are Dirac Delta mass at 1, then the weighted homogenous SBM reduces to homogeneous SBM with $p = 1 - P_0$ and $q = 1 - Q_0$. 

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{../figs/weightedSBM.jpg}
\caption{Weighted stochastic block model}
\label{fig:weighted_stochastic_block_model}
\end{figure}


The model defined in \ref{def:weighted_homo_sbm1} is the focus of our method. However, it is useful to note that we can further generalize model~\ref{def:weighted_homo_sbm1} by allowing both weights and labels. 

\begin{definition} \label{def:weighted_homo_sbm2}
(Weighted and Labeled Homogenous SBM) Let $P, Q$ be two general mixed distributions. The edge random variable $A_{uv}$ is drawn as
\[
A_{uv} \sim \left\{ 
   \begin{array}{cc} 
   P & \trm{ if } \sigma(u) = \sigma(v) \\
   Q & \trm{ if } \sigma(u) \neq \sigma(v)
   \end{array} \right.
\]
\end{definition}
In the case where $P, Q$ are mixed distributions with continuous part $(1-P_0) p(x)$ and $(1-Q_0) q(x)$ respectively and a discrete point mass of $P_0, Q_0$ at zero respectively, then we get back the weighted SBM. 

\subsection{Community estimation}

In this paper we aim to find a tractable community recovery algorithm whose misclustering error can be shown to converge to zero at an optimal rate.

\subsubsection{Misclustering error rate}

The goal of a community recovery algorithm is to take as input the adjacency matrix $A$ and try to recover the community assignments. We evaluate a community recovery algorithm by looking at its mis-clustering error rate. To be precise, if $\sigma_0$ is the true clustering and $\hat{\sigma}$ is the clustering generated by a community recovery algorithm, then the misclustering error rate is the following loss function:
\[
l(\hat{\sigma}, \sigma_0) \equiv \min_{\tau \in S_K} \frac{1}{n} \trm{Hamming}(\hat{\sigma},\, \tau \circ \sigma_0 ),
\]
where $\trm{Hamming}(\cdot, \, \cdot)$ denotes the Hamming distance. In the definition of mis-clustering error rate, we minimize over the set of permutations $\tau$ on $K$ objects because clusterings are idenfiable only up to a permutation of their labels. It is important to note that $\hat{\sigma}$ is a random quantity both because the community recovery algorithm may be stochastic and because the network $A$ -- the input to the algorithm -- is random. Thus, we aim to bound $l(\hat{\sigma}, \sigma_0)$ in probability. 

Zhang and Zhou~\cite{zhangminimax} and Gao et al~\cite{gao2015achieving} show that the minimax optimal rate of convergence for the unweighted stochastic block model is of the order $\exp\left( - (1 + o(1)) \frac{n I_{\trm{Ber}}}{K} \right)$. $I_{\trm{Ber}} = -2 \log \sqrt{P_0 Q_0} + \sqrt{(1-P_0)(1-Q_0)}$ is the Renyi divergence of order $1/2$ between $\trm{Ber}(P_0)$ and $\trm{Ber}(Q_0)$, where $P_0, Q_0$ are the probabilities of absence for within-community and between-communities edges. Yun and Proutiere have also characterized, though they present the results differently, the optimal rate of convergence for the labeled stochastic block model. Our work extends these results to the weighted SBM and show that the optimal rate is again governed by a Renyi divergence.

Although Renyi divergence is of central importance in homogenous stochastic block model where the cluster sizes are approximately balanced, it is important to note that, in the case of cluster imbalance or in the case of \emph{heterogenous} stochastic block model, Abbe and Sandon \cite{AbbSan15} and Yun and Proutiere \cite{yun2016optimal} have shown that an information divergence that generalizes the Renyi is what drives the intrinsic difficulty of community recovery -- a generalization that is referred to as the CH-divergence.


\subsubsection{Other notions of recovery}

A closely related problem is that of finding the exact recovery threshold. We say that the weighted stochastic block model has an exact recovery threshold if there is some function of the parameters $\theta(P_0, Q_0, p(x), q(x), K, \beta, n)$ such that exact recovery is asymptotically almost always impossible if $\theta < 1$ and almost always possible if $\theta > 1$. For the homogeneous unweighted stochastic block model, Abbe et al \cite{abbe2014exact} have shown that, when $\beta=1, K=2, 1 - P_0 = \frac{a \log n}{n}$, and $1 - Q_0 = \frac{b \log n}{n}$  (that is, the average degree is of order $\log n$) for some constant $a,b$, then the exact threshold is $\sqrt{a} - \sqrt{b}$ , that is, no exact recovery algorithm can succeed if $\sqrt{a} - \sqrt{b} < 1$ and there exists a recovery algorithm that can succeed with probability tending to one if $\sqrt{a} - \sqrt{b} > 1$.  This result was generalized by Zhang and Zhou \cite{zhangminimax} beyond the $\log n$ degree setting where $ \frac{n I_{\trm{Ber}}}{K \log n}$ was shown to be the threshold. %Our paper again extends these results to the weighted stochastic block model where we show that the exact recovery threshold is a natural generalization of the unweighted analogues. 
Apart from exact recovery (also known as strong consistency) and weak recovery, a notion of partial recovery (also known as weak consistency) has also been considered~\cite{MosEtal14, AminiEtal13, zhangminimax}. This notion lies between the other two notions of recovery, and only requires the fraction of misclassified nodes to converge in probability to 0 as $n$ becomes large. A very general result for the $K=2$ case, characterizing when exact and partial recovery are possible for the unweighted homogeneous stochastic block model, is provided in Mossel et al.~\cite{MosEtal14}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Recovery algorithm}
\label{sec:method}

The weighted stochastic block model presents an extra layer of difficulty on top of the stochastic block model because the densities $p(x), q(x)$ are unknown. For example, one consequence of not knowing $p(x), q(x)$ is that the MLE does not exist. To see this, let us first see the MLE for stochastic block model:
$
\hat{\sigma}^{SBM}_{MLE} = \argmax_{\sigma}  \sum_{\substack{ (u,v) \in E \\ \sigma(u) = \sigma(v)}} \log \frac{p(1-q)}{q(1-p)}.
$
Since $\log \frac{p (1-q)}{q (1-p)} > 0$, the estimator $\hat{\sigma}^{SBM}_{MLE}$ may be computed by searching for the clustering that maximizes the number of within cluster edges. In contrast, one can show, after straightforward algebraic manipulation, that likelihood maximization for wSBM takes on the form
\[
\sup_{\sigma,\, p(x), q(x) \in \mathcal{P} } \sum_{\substack{ (u,v) \in E \\ \sigma(u) = \sigma(v)}} \log \frac{p(A_{uv}) (1-Q_0)}{q(A_{uv}) (1-P_0)},
\]
where $\mathcal{P}$ is the set of all densities. The maximum does not exist here because the maximizer of the likelihood does not exist for nonparametric density estimation. This remains true even if we restrict $\mathcal{P}$ to be the set of all smooth densities with say bounded second derivatives. Our approach therefore is to combine the idea of discretization from nonparametric density estimation with existing clustering techniques on the unweighted stochastic block model. 

\subsection{Algorithm overview}

The key idea behind our method is to convert the edge weights into a finite set of labels by discretization. We then cluster on the labeled network. We first give a broad overview of our algorithm and then describe each steps in detail. Given a weighted network represented as an adjacency matrix $A$, our estimation method has four steps. We summarize the flow of the algorithm below and also in Figure~\ref{fig:method_pipeline1}.


\begin{enumerate}
\item \textbf{Transformation.} We take as input a weighted matrix $A$ and apply an invertible transformation function $\Phi \,:\, \mathbb{R} \rightarrow [0,1]$ to it. The resulting output $\Phi(A)$ is a matrix whose weights are between 0 and 1.

\item \textbf{Discretization.} We divide the $[0,1]$ interval into $l=1,...,L$ equally spaced subintervals, which we call bins. We replace the real-valued weight entries $\Phi(A)$ with a categorical label $l \in \{1,...,L\}$: $[\Phi(A)]_{uv}$ is assigned label $l$ if the value $[\Phi(A)]_{uv}$ falls into bin $l$. We output a network whose edges are colored with $L$ possible colors, whose adjacency matrix we continue to call $A$.

\item \textbf{Add noise.} For a fixed constant $c > 0$, let $\delta = \frac{cL}{n}$. We perform the following process on every edge of the labeled graph, independently of other edges: With probability $1-\delta$, keep an edge as it is, and with probability $\delta$ erase the edge and replace it with an edge with label uniformly drawn from the set of $L$ labels. We continue calling the modified adjacency matrix as $A$.

\item \textbf{Initialization Part 1.} For each color $l$, we create a sub-network by including in it only edges whose color is $l$. For each sub-network, we perform spectral clustering. We output as $l^*$ the color that induces the maximally separated spectral clustering. 

\item \textbf{Initialization Part 2.} Let $A_{l^*}$ be the adjacency matrix for color $l^*$. For each $u \in \{1, \dots, n\}$, we perform spectral clustering on $A_{l^*} \setminus \{u\}$, which is adjacency matrix with vertex $u$ removed. We output $n$ clusters $\tilde{\sigma}_1, \dots, \tilde{\sigma}_n$, where $\tilde \sigma_u$ is a clustering on $\{1, 2, \dots, n\} \setminus \{u\}$ for $1 \leq u \leq n$.

\item \textbf{Refinement.} From each $\tilde \sigma_u$, we generate a $\hat \sigma_u$ which is a clustering on $\{1, 2, \dots, n\}$ which retains the assignments specified by $\tilde \sigma_u$ for $\{1, 2, \dots, n\} \setminus \{u\}$, and assigns $\hat \sigma_u(u)$ by maximizing the likelihood looking at only the neighborhood around $u$. 

\item \textbf{Consensus.} We align the cluster assignments made in the previous step. 
\end{enumerate}

 
\begin{figure}[htp]
\centering
\includegraphics[scale=0.4, trim={0 6.5in 0 0}]{../figs/method_pipeline1.pdf}
\caption{\textcolor{red}{Add a box indicating the add noise step. Pipeline for the our proposed algorithm}}
\label{fig:method_pipeline1}
\end{figure}



\subsection{Transformation and discretization}

These two steps are straightforward. In the transformation step, we apply an invertible CDF function $\Phi \,:\, \mathbb{R} \rightarrow [0,1]$ as the transformation function onto all the edge weights so that the transformed edge weights $\Phi(A)$ is in the interval $[0,1]$. In the discretization step, we divide the interval $[0,1]$ into $L$ equally spaced bins labeled $l=1, \dots, L$. Each bin $l$ is of the form $[a_l, b_l]$ where $a_1 = 0, b_L = 1$ and $b_l - a_l = 1/L$. We give an edge the label $l$ if the weight of that edge falls into bin $l$. 

\begin{algorithm}
\caption{Transformation and Discretization}
\label{alg:transform_and_discretize}
\textbf{Input:} A weighted network $A$, a positive integer $L$, and an invertible function $\Phi \,:\, \mathbb{R} \rightarrow [0,1]$.\\
\textbf{Output:} A labeled network $A$ with $L$ labels\\

\begin{algorithmic}
\State Divide $[0,1]$ into $L$ bins, labeled $Bin_1, \dots, \bin_l$.
\For{every edge $(u,v)$}
   \State let $l$ be the bin in which $\Phi(A_{uv})$ falls.
   \State Give the edge $(u,v)$ the label $l$ in the labeled network $A$
\EndFor
\State Output $A$
\end{algorithmic}
\end{algorithm}

\subsection{Add noise}

This part of the algorithm is primarily for technical reasons. As detailed in the proof of Proposition~\ref{prop:labeled_sbm_rate} in Appendix~\ref{appendix: first}, deliberately forming a noisy version of the graph has a negligible effect on the separation between the distributions specifying within and across community edge labels, but it has the desirable effect of ensuring that all the edge labels occur with probability at least $c/n$. This lower bound is crucial to our analysis in the subsequent steps of the recovery algorithm.

\begin{algorithm}
\caption{Add noise}
\label{alg:noisify}
\textbf{Input:} A labeled network with $L$ labels, a constant $c$\\
\textbf{Output:} A labeled network $A$ with $L$ labels\\

\begin{algorithmic}
\For{every edge $(u,v)$}
   \State With probability $1-\frac{cL}{n}$, do nothing. With probability $\frac{cL}{n}$ replace edge label with a label drawn uniformly at random from $\{1, 2, \dots, L\}$
\EndFor
\State Output $A$
\end{algorithmic}
\end{algorithm}

\subsection{Initialization}

The initialization procedure takes as input a network whose edges are labeled with a color $l \in \{1, ..., L\}$. The goal of the initialization procedure is create a rough clustering $\tilde{\sigma}$ that is sub-optimal but still consistent. As outlined in Algorithm~\ref{alg:initialization1}, the rough clustering is based on a single color $l^*$, which is chosen based on the maximum value of the estimated Renyi divergence between within-community and between-community distributions for the unweighted SBMs based on individual colors.

\begin{algorithm}[h!]
\caption{Initialization}
\label{alg:initialization1}
\textbf{Input:} A labeled network $A$ with $L$ labels \\
\textbf{Output:} A set of clusterings $\{ \tilde{\sigma}_u \}_{u=1,...,n}$, where $\tilde \sigma_u$ is a clustering on $\{1, 2, \dots, n\} \setminus \{u\}$\\

\begin{algorithmic}[1]
\State Separate $A_L$ into $L$ networks $\{ A_l \}_{l=1, \dots, L}$ where $A_l$ contains only edges with label $l$.  \Comment{Stage 1}
\For{each label $l$}
   \State Compute $\bar{d} = \frac{1}{n} \sum_{u=1}^n d_u$ as the average degree.
   \State Perform spectral clustering with $\tau = \bar{d}$ and $\mu \geq C\beta$ to get $\tilde{\sigma}_l$, where $C$ is an appropriately chosen large constant
   \State estimate $\hat{P}_l = 
             \frac{ \sum_{u \neq v \,:\, \tilde{\sigma}_l(u) = \tilde{\sigma}_l(v) } (A_l)_{uv} }
                  { |{u \neq v \,:\, \tilde{\sigma}_l(u) = \tilde{\sigma}_l(v) }| }$ and 
               $\hat{Q}_l = 
            \frac{ \sum_{u \neq v \,:\, \tilde{\sigma}_l(u) \neq \tilde{\sigma}_l(v) } (A_l)_{uv} }
              { |{u \neq v \,:\, \tilde{\sigma}_l(u) \neq \tilde{\sigma}_l(v) }| }$. 
   \State $\hat{I}_l \leftarrow 
               \frac{ (\hat{P}_l - \hat{Q}_l)^2}{\hat{P}_l \vee \hat{Q}_l}$
\EndFor
\State Choose $l^* = \argmax_l \hat{I}_l$. Let $A_{l^*}$ be the network with only edges labeled $l^*$
\For{each node $u$}  \Comment{Stage 2}
   \State Create network $A_{l^*} \setminus \{u\}$ by removing node $u$ from $A_{l^*}$
   \State Perform spectral clustering on $A_{l^*} \setminus \{ u \}$ to get $\tilde{\sigma}_u$
\EndFor 
\State Output the set of clusterings $\{ \tilde{\sigma}_u \}_{u=1, \dots, n}$.
\end{algorithmic}
\end{algorithm}

For technical reasons, we will actually create $n$ separate rough clusterings $\{\tilde{\sigma}_u \}_{u = 1, \dots, n}$ where each $\tilde{\sigma}_u \,:\, [n-1] \rightarrow [K]$ is a clustering of a network of $n-1$ nodes where node $u$ has been removed.

\paragraph{\textbf{Spectral clustering:}} Note that Algorithm~\ref{alg:initialization1} involves several applications of spectral clustering. We describe the spectral clustering algorithm used as a subroutine in Algorithm~\ref{alg:spectral} below:

\begin{algorithm}[h!]
\caption{Spectral clustering}
\label{alg:spectral}
\textbf{Input:} An unweighted network $A$, trim threshold $\tau$, number of communities $K$, tuning parameter $\mu$ \\
\textbf{Output:} A clustering $\sigma$ \\

\begin{algorithmic}[1]
\State For each node $u$ whose degree $d_u \geq \tau$, set $A_{uv} = 0$ to get $T_{\tau}(A)$
\State Let $\hat{A}$ be the best rank-$K$ approximation to $T_{\tau}(A)$ in spectral norm
\State For each node $u$, define the neighbor set $N(u) = \{ v \,:\, \| \hat{A}_u - \hat{A}_v \|_2^2 \leq \mu K^2 \frac{\bar{d}}{n} \}$
\State Initialize $S \leftarrow 0$. Select node $u$ with the most neighbors and add $u$ into $S$ as $S[1]$
\For{$i = 2, \dots, K$}
    \State Among all $u$ such that $|N(u)| \geq \frac{n}{\mu K}$, select 
           $u^* = \argmax_u \min_{v \in S} \| \hat{A}_u - \hat{A}_v \|_2$
    \State Add $u^*$ into $S$ as $S[i]$.
\EndFor
\For{$u = 1,...,n$}
    \State Take $\argmin_i \| \hat{A}_u - \hat{A}_{S[i]} \|_2$ and assign $\sigma(u) = i$
\EndFor
\end{algorithmic}
\end{algorithm}

Importantly, note that we may always choose the parameter $\mu$ sufficiently large such that Algorithm~\ref{alg:spectral} generates a set $S$ with $|S| = K$.

\subsection{Refinement and consensus}

Our refinement and consensus step closely follow the method described by Gao et al \cite{gao2015achieving}. In the refinement step, we use the set of initial clusterings $\{\tilde{\sigma}_u\}_{u=1, \dots, n}$ to generate a more accurate clustering for the labeled network. We do this by locally maximizing an approximate log-likelihood expression for each of the nodes $u=1, \dots, n$. The consensus step is to resolve a technical cluster label consistency problem that arises after the refinement stage. 

\begin{algorithm}
\caption{Refinement}
\label{alg:refinement}
\textbf{Input:} A labeled network $A$ and a set of rough clusterings $\{\tilde{\sigma}_u\}_{u=1,...,n}$, where $\tilde \sigma_u$ is a clustering on the set $\{1, 2, \dots, n\} \setminus \{u\}$ for each $u$ \\
\textbf{Output:} a clustering $\hat{\sigma}$ over the whole network

\begin{algorithmic}[1]
\For{each node $u$}
   \State Estimate $\{ \hat{P}_l, \hat{Q}_l\}_{l=0,...,L}$ from $\tilde{\sigma}_u$
   \State Let $\hat{\sigma}_u : [n] \rightarrow [K]$ where 
       $\hat{\sigma}_u(v) = \tilde{\sigma}_u(v)$ for all $v \neq u$ and 
   \[
    \hat{\sigma}_u(u) = \argmax_k \sum_{v \,:\, \tilde{\sigma}_u(v) = k ,\, v\neq u} 
         \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
     \]    
\EndFor 
\State Let $\hat{\sigma}(1) = \hat{\sigma}_1(1)$  \Comment{Consensus Stage}
\For{each node $u \neq 1$}
\[
\hat{\sigma}(u) = \argmax_k | \{ v \,:\,  \hat{\sigma}_1(v) = k \} \cap
                                 \{ v \,:\, \hat{\sigma}_u(v) = \hat{\sigma}_u(u) \}|
\]
\EndFor
\State Output $\hat{\sigma}$
\end{algorithmic}

\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Analysis of misclustering error}
\label{sec:rate}

On the unweighted stochastic block model, the key information quantity that governs the threshold behavior is $I = -2 \log (\sqrt{p q} + \sqrt{(1-p)(1-q)})$. This is the Renyi divergence of order $\frac{1}{2}$ between the $Ber(p)$ distribution and the $Ber(q)$ distribution. 

The Renyi divergence of order $\frac{1}{2}$ is defined on pairs of general measures as 
\[
I = -2 \log \int \left( \frac{dP}{dQ} \right)^{1/2} dQ
\]
Interestingly, this generalized form of the Renyi divergence is also what governs both the rate of convergence of our proposed algorithm and the threshold behavior of the weighted stochastic block model. In the weighted stochastic block model setting where $P, Q$ have continuous part $p(x), q(x)$ and a point mass of probability $P_0, Q_0$ at zero, the Renyi divergence takes on the form
\[
I = -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0)p(x) q(x)} dx \right)
\]
When $I \rightarrow 0$, which is the scenario that we analyze, then $I$ is also asymptotically equal to the Hellinger distance:
\begin{align}
I &= \left\{ (\sqrt{P_0} - \sqrt{Q_0})^2 + \int (\sqrt{(1-P_0)p(x)} - \sqrt{(1-Q_0)q(x)} )^2 dx \right\} (1 + o(1)) \nonumber \\ 
 &= \left\{ (\sqrt{P_0} - \sqrt{Q_0})^2 + (\sqrt{1-P_0} + \sqrt{1-Q_0})^2 + 
             \sqrt{(1-P_0)(1-Q_0)} \int (\sqrt{p(x)} - \sqrt{q(x)} )^2 dx \right\} \nonumber \\
             %
             & \qquad \cdot (1 + o(1)) \label{eqn:I_decompose} 
\end{align}
Equation~\ref{eqn:I_decompose} shows that the Renyi divergence is driven by both the divergence between the edge probabilities $1-P_0, 1-Q_0$ as well as the divergence between the densities $p(x), q(x)$. This is a novel feature of the weighted stochastic block model. 


When $p(x) = q(x)$, the Renyi divergence $I$ reverts to the unweighted SBM case where it is a divergence between two Bernoulli distributions. This is intuitive because if $p(x) = q(x)$, then the edge weights give no additional information about the cluster structure. When $P_0 = Q_0$, then the Renyi divergence is driven only by the difference between the edge weight densities $p(x), q(x)$. This is also intuitive because if $P_0 = Q_0$, then the presence or absence of an edge offers no information on the cluster structure. 


For the remainder of this paper, we define $H := \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx$. Note that $H \leq 2$. It is important to note that $I \rightarrow 0$ quickly either when $H = o(1)$ or, if $\sqrt{(1-P_0)(1-Q_0)}$ is small, when $H = \Theta(1)$. Both of these are important cases to consider: in the first case, the challenge is to distinguish two densities $p(x), q(x)$ which are becoming increasingly similar; in the second case, the challenge is to estimate the density well when the amount of edges may be very sparse. The algorithm we propose in Section~\ref{sec:method} can handle both of these settings but the theoretical analyses are different. 


\subsection{Rate of convergence}


Our analysis is asymptotic. We characterize the performance of the algorithm as $n \rightarrow \infty$. In our analysis, we treat $p(x), q(x), P_0, Q_0$ all as varying with $n$; we should properly write $p_n(x), q_n(x), P_{0n}, Q_{0n}$ but for the purpose of presentation, we omit the subscript and leave the dependency on $n$ as implicit. All of our results will use the following assumption.

\paragraph{\textbf{Assumption A0:}} There exist absolute constants $c_0, C_0$ such that $c_0 \leq \frac{1-P_0}{1-Q_0} \leq C_0$. \\

Assumption A0 says that the density of edges between the communities is of the same order as the density of edges across communities. This assumption is standard in the existing literature on unweighted stochastic block model. 

%We now state our regularity assumptions on the continuous densities $p(x), q(x)$. Naturally, these depend on the transformation function $\Phi$ that we use. The fact that $\Phi$ appears in the regularity assumptions on $p(x), q(x)$ means that it is difficult to know how to choose $\Phi$, but as we demonstrate in section~\ref{sec:examples}, it is generally sufficient to choose $\Phi$ as the CDF of the log-normal distribution. 

Recall that $\Phi \,:\, \R \rightarrow [0,1]$ and that it must be invertible, differentiable, and a cumulative distribution function. We let $\phi$ denote $\Phi'$ and $\phi$ is thus the density of some distribution. We let $\Phi \{ \cdot \}$ denote the $\Phi$-measure of a set. Intuitively, the additional regularity conditions stated below require $p(x)$ and $q(x)$ to be smooth and the likelihood ratio $\frac{p(x)}{q(x)}$ to be well-behaved. Furthermore, the distribution $\phi$ must be heavier-tailed than $p(x)$ and $q(x)$. Note that $H = \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx$ may either be $o(1)$ or $\Theta(1)$. Since these two cases lead to significant differences in their respective analyses, we require different sets of assumptions for $p(x)$ and $q(x)$ for each case.

\subsubsection{The case $H = o(1)$}

We state the assumptions for the case of $H = o(1)$, and then present our main result for this sub-problem.\\

\noindent \textbf{Regularity conditions:} 
\begin{enumerate}
\item[A1] There exists a constant $C >0$ such that  $0 < p(x), q(x) \leq C$, and $p(x)$ and $q(x)$ are absolutely continuous.  Moreover, the transformation density $\phi$ satisfies 
$$\lim_{|x| \rightarrow \infty} \sup_n \frac{p(x) \vee q(x)}{\phi(x)} < \infty.$$

\item[A2] There exists $R$ a subinterval of $\R$ such that: (a) $\Phi\{R^c\} = o(H)$, and (b)  $\frac{1}{\rho} \leq \frac{p(x)}{q(x)} \leq \rho $  for all $x \in R$ and for a constant $\rho$. (Recall that we define $H \equiv \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx$.)

\item[A3] Let $\alpha^2 = \int_R q(x) \left( \frac{p(x) - q(x)}{q(x)} \right)^2 dx$ and $\gamma(x) = \frac{q(x) - p(x)}{\alpha}$. There exists constants $M, r \geq 4$ such that  
$$\int_R q(x) \left| \frac{\gamma(x)}{q(x)} \right|^r dx  \leq M.$$

\item[A4] Let $h(x) \geq \sup_n \max \left\{  \left|\frac{\gamma'(x)}{q(x)} \right|, 
 \left|\frac{q'(x)}{q(x)}\right|, \left| \frac{\phi'(x)}{\phi(x)}\right|, \left | \frac{\gamma(x)}{q(x)} \right|  \right\} $. Let $\int_R |h(x)|^{4t/(1-t)} \phi(x) dx \leq M'$ for some constant $M'$ and $1 \geq t \geq 2/r$. Suppose also that the level set $\{x \,:\, |h(x)| \geq \kappa\}$ is a union of at most $K_h$ intervals for all large enough $\kappa$. Suppose $\int \phi(x)^{\frac{1-t}{1+t}} dx < \infty$.
Additionally, we require that the level set $\{x \,:\, |h(x)| \geq \kappa\}$ is a union of at most $K_h$ intervals for all large enough $\kappa$ where $K_h$ is a constant. We also assume $\int \phi(x)^{\frac{1-t}{1+t}} dx < \infty$.

\item[A5]  There exists a constant $c' > 0$ such that $(\log p)'(x), (\log q)'(x) \geq (\log \phi)'(x) \geq 0$ for all $x < -c'$ and $ (\log p)'(x), (\log q)'(x) \leq (\log \phi)'(x) \leq 0$ for all $x > c'$.
\end{enumerate}

The simplest setting for which the assumptions are satisfied is when $p(x)$ and $q(x)$ are compactly supported (so the transformation $\Phi$ is not even necessary), have bounded first derivatives, likelihood ratio $\frac{p(x)}{q(x)}$ bounded away from 0 and infinity, and uniform convergence of $p(x) - q(x) \rightarrow 0$. However, this simple setting excludes many interesting cases, for example when $p(x)$ and $q(x)$ are Gaussian. To include such cases (cf.\ Section~\ref{SubsecExa} below), we require the more technical conditions. We then have the following result:
\begin{theorem}
\label{thm:weighted_sbm_rate1}
Suppose $\hat{\sigma}$ is the output of the algorithm in Section~\ref{sec:method} with transformation $\Phi$ and discretization level $L$ chosen such that $L \rightarrow \infty$, $L = o(\frac{1}{H})$, and $L = o(nI)$. Suppose that $P_0, Q_0$ satisfy assumption A0 and that $p(x), q(x)$ satisfy assumptions A1-A5 with respect to $\Phi$. Suppose $\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = o(1)$, that $K$ is fixed, and $I = o(1)$. Then, we have that
\[
\lim_{n \rightarrow \infty} P \left\{
     l(\hat{\sigma}, \sigma_0) \leq \exp\left( - \frac{nI}{\beta K} (1 + o(1)) \right)
    \right\} \rightarrow 1.
\]
\end{theorem}
The proof of Theorem~\ref{thm:weighted_sbm_rate1} is outlined in Appendix~\ref{AppThmRate1}. 

\subsubsection{Examples}
\label{SubsecExa}

Since conditions A1-A5 are rather technical, we illustrate with several concrete examples. Although we do not in general require $p(x)$ and $q(x)$ to belong to a parametric family, we will discuss cases where $p(x) = \exp( f_{\theta_1}(x))$ and $q(x) = \exp( f_{\theta_0}(x))$ where $f_{\theta}(x)$ is a set of functions indexed by $\theta$ where  $\theta \in \Theta \subset \R^{d_{\Theta}}$ and where $\Theta$ is some compact subset of the Euclidean space.
Although there is not a universal function $\Phi$ that works in all situations, it is generally sufficient, when $p(x), q(x)$ have subexponential tails, to take $\Phi$ as the CDF of the log-normal distribution. That is, we take $\phi(x) = \frac{1}{x \sigma \sqrt{2\pi}} \exp\left\{ - \frac{\ln^2 (x)}{2} \right\}$.

\begin{example} (Gaussian with varying mean and variance)\\
Suppose $p(x) = N(\mu_1, \sigma_1^2)$ and $q(x) = N(\mu_0, \sigma_0^2)$ are both Gaussian with different mean and variance. Then, $\theta = (\mu, \sigma^2)$. We can take $\Theta$ to be any compact set where $\sigma^2$ is bounded away from 0. For example, we can let $\Theta = [-1,1] \times [0.1, 2]$. Then, we have
\begin{align*}
f_\theta(x) &= - \frac{1}{2} \frac{(x - \mu)^2 }{\sigma^2} - \frac{1}{2} \log (2\pi \sigma^2 ).
%\nabla_{\theta} f_\theta(x) &= \left( - \frac{(\mu - x) }{\sigma^2}, \frac{1}{2} \frac{(x- \mu)^2}{\sigma^4} - \frac{1}{2} \frac{1}{\sigma^2} \right)
\end{align*}
Since the log-normal distribution has all moments and has heavier tails than Gaussians, one can verify (through proposition~\ref{prop:theta_rate} that the conditions A1-A5 are always satisfied.
%all conditions B1-B5 are satisfied. 
\end{example}

\begin{example} (Laplace  with varying location and scale)\\
Suppose $p(x) = \frac{1}{2b_1} \exp\left( - \frac{|x - \mu_1|}{b_1} \right)$ and $q(x) = \frac{1}{2b_0} \exp\left( - \frac{|x - \mu_0|}{b_0} \right)$. Then $\theta = (\mu, b)$ and we can take $\Theta$ to be any compact set where $b$ is bounded away from 0.
\begin{align*}
f_{\theta}(x) &= - \frac{|x - \mu|}{b} - \log 2b.
%\nabla f_{\theta}(x) &= \left( -\frac{\trm{sign}(x - \mu)}{b}, -\frac{| x - \mu|}{b^2} \right)
\end{align*}
Again, one can (through proposition~\ref{prop:theta_rate}) show that conditions A1-A5 are always satisfied.
\end{example}

\begin{example} (Location Family)
For a given density $\exp f(x)$ with mean zero, we can define a location family parametrized by $\mu$ as $\exp( f(x - \mu))$. We let $p(x) = \exp( f( x - \mu_1))$ and $q(x) = \exp( f ( x - \mu_0))$. In this case, $\theta = \mu$ and we can let $\Theta = [-c, c]$ for some constant $c$. 

In this case, 
\begin{align*}
f_{\theta} &= f( x - \mu).
%\nabla_\theta f_{\theta} &= d_{\mu} f(x - \mu) = -f'(x - \mu) \\
%\nabla_\theta f_{\theta}' &= - f''(x-  \mu)
\end{align*}
One can show (through proposition~\ref{prop:theta_rate}) that conditions A1-A5 are always satisfied when $\sup_{\mu \in [-c, c]} | f'(x-  \mu) |$ and $| f''(x - \mu) |$ are bounded by a polynomial of $x$.

%Take $\Phi$ as the CDF of the log-normal distribution. Because all moments of the log-normal distributions are finite, we have that $p(x)$ and $q(x)$ satisfy assumption B1-B5 so long as $\sup_{\mu \in [-c, c]} | f'(x-  \mu) |$ and $| f''(x - \mu) |$ are bounded by a polynomial of $x$. 
\end{example}


\subsubsection{The case $H = \Theta(1)$}

We now state the assumptions we require in the case $H = \Theta(1)$.

\paragraph{\textbf{Regularity conditions:}} 

\begin{enumerate}
\item[A1'] There exists a constant $C >0$ such that $0 \leq p(x), q(x) \leq C$, and $p(x)$ and $q(x)$ are absolutely continuous.  Moreover, we assume that 
$$\lim_{|x| \rightarrow \infty} \sup_n \frac{p(x) \vee q(x)}{\phi(x)} < \infty.$$
\item[A2'] For some $r > 2$, $\sup_n \int \left| \log \frac{p(x)}{q(x)} \right|^r \phi(x) dx < \infty$.
\item[A3'] Let $h(x) \geq \sup_n \max \left\{  \left|\frac{\gamma'(x)}{q(x)} \right|, 
 \left|\frac{q'(x)}{q(x)}\right|, \left| \frac{\phi'(x)}{\phi(x)}\right|  \right\} $. Let $\int |h(x)|^{2t/(1-t)} \phi(x) dx \leq M'$ for some constant $M'$ and $1 \geq t \geq 2/r$. Suppose also that the level set $\{x \,:\, |h(x)| \geq \kappa\}$ is a union of at most $K_h$ intervals for all large enough $\kappa$. Suppose $\int \phi(x)^{\frac{1-t}{1+t}} dx < \infty$.
\item[A4']  There exists a constant $c'>0$ such that $(\log p)'(x), (\log q)'(x) \geq (\log \phi)'(x) \geq 0$ for all $x < -c'$ and $ (\log p)'(x), (\log q)'(x) \leq (\log \phi)'(x) \leq 0$ for all $x > c'$.
\end{enumerate}

These assumptions are similar in nature to conditions A1-A5, and one can show that the examples in Section~\ref{SubsecExa} also satisfy conditions A1'-A4'. Our main result here is as follows:

\begin{theorem}
\label{thm:weighted_sbm_rate2}
Suppose $\hat{\sigma}$ is the output of the algorithm in section~\ref{sec:method} with transformation $\Phi$ and discretization level $L$ chosen such that $L \rightarrow \infty$ and $\frac{n I}{ L \exp(L^{1/r}) } \rightarrow \infty$. Suppose that $P_0, Q_0$ satisfy assumption A0 and that $p(x), q(x)$ satisfy assumptions A1'-A4' with respect to $\Phi$. Suppose $\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = \Theta(1)$, that $K$ is fixed, and that $I = o(1)$. Then, we have that

\[
\lim_{n \rightarrow \infty} P \left\{
     l(\hat{\sigma}, \sigma_0) \leq \exp\left( - \frac{nI}{\beta K} (1 + o(1)) \right)
    \right\} \rightarrow 1.
\]
\end{theorem}
For the proof of Theorem~\ref{thm:weighted_sbm_rate2}, see Appendix~\ref{AppThmRate2}.

\subsubsection{Additional discussion of assumptions}

It is crucial to note that our algorithm does not require any knowledge about the form of $p(x), q(x)$. The same algorithm and the same guarantees apply whether $p(x)$ and $q(x)$ are Gaussian, Laplace, or any other (possibly non-parametric) distributions, as long as they satisfy conditions A1-A5 in conjunction with the transformation function $\Phi$. To aid the reader, we provide a brief non-technical interpretation of the regularity conditions.

\paragraph{\textbf{Interpretation of assumptions A1-A5:}}

\begin{enumerate}
\item[A1] Assumption A1 is simple; the second part states that $\Phi$ must have a tail just as heavy as that of $p(x)$ and $q(x)$. 
\item[A2] In Assumption A2, we require that the likelihood ratio $\frac{p(x)}{q(x)}$ be bounded away from 0 and $\infty$ except on a region $ R^c \subset \R$. Since $H \rightarrow 0$, we have that $p(x), q(x)$ are becoming more similar and thus $R^c$ is shrinking. We require that the measure of $R^c$, with respect to $\Phi$, shrinks faster than $H$. This condition intuitively states that $|\frac{p(x)}{q(x)}|$ and its reciprocal tend to infinity slowly with respect to $x$. If $\Phi$ has a heavier tail, then A2 is a stronger condition on $\frac{p(x)}{q(x)}$. If $\Phi$ has a lighter tail, then A2 is a looser condition.
\item[A3] In Assumption A3, note that because $H \rightarrow 0$, $\alpha \rightarrow 0$ as well. $\gamma(x) = \frac{p(x) - q(x)}{\alpha}$ is thus a function of constant order. The integrability condition on $\gamma(x)$ effectively says that $p(x) - q(x)$ must converge to 0 almost uniformly for all $x$ in the region $R$. (Having an $L_\infty$ bound on $\gamma$ would imply uniform convergence.) 
\item[A4]  Assumption A4 imposes smoothness on $q(x)$ as well as $\gamma(x)$. The second part of A4 is a weak condition that says $h(x)$ cannot oscillate with infinite frequency. 
\item[A5] Assumption A5 is another way of saying that $\phi$ must have a tail as heavy as that of $p(x)$ and $q(x)$. 
\end{enumerate}
Note that an analogous interpretation may be used to describe to the conditions A1'-A4'.

An alternative way to interpret these assumptions is that, for a given transformation $\Phi$, there is a space $\mathcal{P}_{\Phi}(C, \rho, r, M, t, M', K_h, c')$ of densities that satisfy assumptions A1 to A5. We again emphasize that $\mathcal{P}_\Phi$ is actually a sequence of function spaces indexed by $n$; we make the dependence implicit in our notation. For a given $\Phi$, assumption A1-A5 imposes a set of constraints on the densities $p(x), q(x)$. But suppose that $p(x), q(x)$ are given, it is difficult unfortunately to know how to choose an appropriate $\Phi$ from the statement of the assumptions.
%Section~\ref{sec:examples} show however that choosing $\Phi$ as the CDF of the log-normal distribution suffices for a broad family of subexponential densities. 

%%%%%%

\paragraph{\textbf{Assumptions for parametric families:}} When $p(x)$ and $q(x)$ belong to a parametric family, as in the examples discussed in Section~\ref{SubsecExa}, it is helpful to consider a simpler set of assumptions. Suppose $p(x) = \exp( f_{\theta_1}(x))$ and $q(x) = \exp( f_{\theta_0}(x))$ where $f_{\theta}(x)$ is a set of functions indexed by $\theta$ where  $\theta \in \Theta \subset \R^{d_{\Theta}}$ and where $\Theta$ is some compact subset of the Euclidean space. Consider the following conditions:
\textcolor{red}
{
\begin{enumerate}
\item[B1] For all $\theta \in \Theta$, $\liminf_{|x| \rightarrow \infty} \left( (\log \phi)(x) - f_\theta(x) \right) > -\infty$. 
\item[B2] Define the Fisher information matrix $G_\theta$ as
$$G_{\theta} = \int_{-\infty}^{\infty} (\nabla_{\theta} f_{\theta}(x)) 
                                    (\nabla_{\theta} f_{\theta}(x))^\tran 
                      \exp( f_{\theta}(x)) dx.$$
 We assume that this matrix is full-rank:
   \[
 0<   c_{\min} <  \inf_{\theta \in \Theta} \lambda_{min}(G_\theta) \leq \sup_{\theta \in \Theta} \lambda_{max}(G_{\theta}) < c_{\max} < \infty
  \]
\item[B3] There is some constant $c$ such that $\sup_\theta$ $\| \nabla_{\theta} f_{\theta} (x) \|$ is monotonically non-decreasing in $|x|$ for $|x| \geq c$.
\item[B4] The following four integrability conditions hold:
  \begin{align*}
  \int_{-\infty}^\infty \sup_{\theta \in \Theta} \| \nabla_{\theta} f_\theta(x) \|^{r \vee \frac{8t}{1-t}} \phi(x) dx &< \infty\\
   \sup_{\theta \in \Theta} \int_{-\infty}^\infty \| \nabla_{\theta} f'_\theta(x) \|^{4t/(1-t)} \phi(x) dx &< \infty\\
  \sup_{\theta \in \Theta} \int_{-\infty}^\infty | f'_{\theta}(x) |^{8t/(1-t)} \phi(x) &< \infty   \\
        \int_{-\infty}^\infty | (\log \phi)'(x) |^{2t/(1-t)} \phi(x) &< \infty.  
  \end{align*}
\item[B5] There is some constant $c' > 0$ such that, for all $\theta$, $f'_\theta(x) \geq (\log \phi)'(x) > 0$ for all $x \leq -c'$ and 
          $f'_\theta(x) \leq (\log \phi)'(x) < 0$ for all $x \geq c'$.
\end{enumerate}
}
Note that B4 translates to a moment condition on the transformation distribution $\Phi$. We then have the following result, proved in Appendix~\ref{sec:theta_rate_proof}:
\begin{proposition}
\label{prop:theta_rate}
Suppose assumptions B1-B5 hold. Then the following statements are true:
\begin{enumerate}
\item[(a)] If $\| \theta_1 - \theta_0 \| \rightarrow 0$, then $\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \rightarrow 0$ and that assumptions A1-A5 are satisfied. 
\item[(b)] In the case where $\| \theta_1 - \theta_0\| = \Theta(1)$, then $\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = \Theta(1)$ and that assumptions A1'-A4' are satisfied.
\end{enumerate}
\end{proposition}

%%%%%%

\subsection{Lower bound}


In this section, we give a lower bound on the performance of any clustering algorithms on the weighted stochastic block model. For technical reasons, we require the likelihood ratio $\frac{p(x)}{q(x)}$ to be bounded instead of approximately bounded as in Assumption A2 or A2'; we conjecture that the bounded likelihood ratio condition can be relaxed but leave its verification to future works. We also take the true clustering $\sigma_0$ to be uniformly at random in the sense that $\sigma_0 = \sigma'_0 \circ \pi$ where $\sigma'_0 : [n] \rightarrow [K]$ is any fixed clustering and $\pi \in S_n$ is a random permutation on $[n]$. We take $\sigma_0$ to be random so that a clustering algorithm cannot use any prior information on $\sigma_0$; if $\sigma_0$ were fixed, then the algorithm that trivially outputs $\sigma_0$ would have error 0 for example.


\begin{theorem}
\label{thm:lower_bound}
Suppose we have $K$ clusters at least two of which are of size $\frac{n}{\beta K}$ for some constant $\beta \geq 1$. Let the true clustering $\sigma_0$ be drawn uniformly at random. Suppose $I \rightarrow 0$. Suppose $P_0, Q_0$ satisfy assumption A0 and that $p(x), q(x)$ are two densities such that $\left| \log \frac{p(x)}{q(x)} \right| \leq C$ for some constant $C$. Then, we have that, for any community recovery algorithm $\hat{\sigma}$, 
\[
\trm{if $\frac{nI}{K} \rightarrow \infty$, } \quad \E l(\hat{\sigma}, \sigma_0) \geq \exp \left( - (1 + o(1)) \frac{ n I}{K} \right).
\]
and if $\frac{nI}{K} \rightarrow c < \infty$ for some constant $c$, then $\E  l(\hat{\sigma}, \sigma_0) \geq c' > 0$ for some constant $c'$. 
\end{theorem}

The proof of Theorem~\ref{thm:lower_bound} is provided in Appendix~\ref{sec:lower_bound_proof}. The proof employs the same change of measure technique used by Yun and Proutiere to prove a similar lower bound on labeled stochastic block model~\cite{yun2016optimal}. We note that theorem~\ref{thm:lower_bound} applies to any $p(x), q(x)$ that satisfy the assumptions; it does not take the supremum over a function space as with minimax lower bounds. 

It is interesting to observe Theorem~\ref{thm:lower_bound} in conjunction with Theorem~\ref{thm:weighted_sbm_rate1} show that, in terms of rate of convergence, one does not have to pay a price for making nonparametric assumptions. That is, our nonparametric method achieves the same optimal rate even if the densities $p(x), q(x)$ take on a parametric form. This seemingly counter-intuitive phenonmenon arises because the cost of discretization is reflected in the $o(1)$ term in the exponent and is thus of lower order. 


\section{Proof sketch: Recovery algorithm}
\label{SecProofs}

A large portion of the appendix is devoted to proving that our recovery algorithm succeeds and achieves the optimal error rates. Since this also constitutes the a significant part of the novel technical contribution of our paper, we provide an outline of the proof here. 

We divide our argument into propositions that focus on successive stages of our algorithm. If we take a bird-eye view of our method, we find that it consists of two major components: first convert a weighted network into a labeled network, and then second, run community recovery algorithm on the labeled network. The first component has two steps: transformation and discretization. 

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4, trim={0 6.5in 0 .8in}]{../figs/method_pipeline2.pdf}
\caption{\textcolor{red}{The add noise component also goes into the blue section. Analysis of the right-most blue region is in subsection~\ref{sec:labeled_sbm_analysis}, of the middle green region in subsection~\ref{sec:discretization_analysis}, and of the left-most red region in subsection~\ref{sec:transformation_analysis}}}
\label{fig:method_pipeline1}
\end{figure}


\subsection{Analysis of community recovery on a labeled network}
\label{sec:labeled_sbm_analysis}

The workhorse behind our algorithm is a subroutine (right-most blue region in Figure~\ref{fig:method_pipeline1}) for recovering communities on a network where the edges have a discrete label $l=1,...L$. The following proposition characterizes the rate of convergence of the subroutine on the labeled stochastic block model where an edge within a community receives a label $l$ with probability $P_l$ and an edge between communities receives a label $l$ with probability $Q_l$. 


\begin{proposition}
\label{prop:labeled_sbm_rate}
Suppose we have $l=1,...,L$ edge labels and suppose that the label probabilities satisfy $\frac{1}{\rho_L} \leq \frac{P_l}{Q_l} \leq \rho_L$ for a sequence $\rho_L = \Omega(1)$. Define $I_L = -2 \log \sum_{l=0}^L \sqrt{P_l Q_l}$ and suppose $I_L \rightarrow 0$.  Suppose $L = \Omega(1)$ and satisfies $\frac{n I_L}{L \rho^4_L} \rightarrow \infty$. Let $\hat{\sigma}$ be the output of our algorithm. Then, we have that
\[
\lim_{n \rightarrow \infty} P \left( l(\hat{\sigma}, \sigma_0) \leq \exp \left( - \frac{ n I_L}{ \beta K} (1 + o(1)) \right) \right) \rightarrow 1.
\]
\end{proposition}

Yun and Proutiere \cite{yun2016optimal} have proposed an algorithm for the labeled SBM that achieves the same rate of convergence. Proposition~\ref{prop:labeled_sbm_rate} is more general in that we allow the number of labels $L$ and the bound on the ratio $\frac{P_l}{Q_l}$ to both go to infinity. This extension is critical for weighted SBM because to achieve consistency, we must let the discretization level $L$ increase with $n$. 


\subsection{Discretization of the Renyi divergence}
\label{sec:discretization_analysis}

The rate of Proposition~\ref{prop:labeled_sbm_rate} looks similar to that of Theorems~\ref{thm:weighted_sbm_rate1} and \ref{thm:weighted_sbm_rate2}, except that instead of the actual Renyi divergence $I$, we have the discretized Renyi divergence $I_L$. A way to prove Theorems~\ref{thm:weighted_sbm_rate1} and \ref{thm:weighted_sbm_rate2} then is to show that the two quantities are close to each other; the following propositions do exactly that for distributions supported on $[0,1]$ and satisfying some additional assumptions. It is easy to show that $I_L \leq I$ because discretization always loses information. If $p(x), q(x)$ are sufficiently regular in that they can be well approximated by discretization, then one might expect that $I_L$ is not much smaller than $I$. Proposition~\ref{prop:discretization1} and \ref{prop:discretization2} shows exactly that. 
\medskip

\noindent The following proposition is useful for proving Theorem \ref{thm:weighted_sbm_rate1}:
\begin{proposition} 
\label{prop:discretization1}
Let $p(z), q(z)$ be two densities supported on $[0,1]$. Suppose that $H \equiv \int (\sqrt{p(z)} - \sqrt{q(z)})^2 dz = o(1)$. Let $L$ be a sequence such that $L \rightarrow \infty$.


Suppose the following assumptions are satisfied:
\begin{enumerate}
\item[C1] Suppose $p(z), q(z) \leq C$ on $[0,1]$ and are absolutely continuous.
\item[C2] There exists $R$ a subinterval of $[0,1]$ such that $\frac{1}{\rho} \leq \left| \frac{p(z)}{q(z)} \right| \leq \rho$ and $\mu\{R^c\} = o(H)$ where $\mu$ is the Lebesgue measure.

\item[C3] Define $\alpha^2 = \int_R \frac{(p(z) - q(z))^2}{q(z)} dz$ and $\gamma(z) = \frac{q(z) - p(z)}{\alpha}$. Suppose $\int_R q(z) \left| \frac{\gamma(z)}{q(z)} \right|^r dz  \leq M$ for constants $M, r \geq 4$.
\item[C4] Let $h(z) \geq \sup_n \max \left\{  \left|\frac{\gamma'(z)}{q(z)} \right|, 
 \left|\frac{q'(z)}{q(z)}\right|  \right\} $. Suppose $\int_R |h(z)|^t dz \leq M'$ for some constant $M'$ and $1 \geq t \geq 2/r$. Suppose also that the level set $\{z \,:\, |h(z)| \geq \kappa\}$ is a union of at most $K_h$ intervals for all large enough $\kappa$, and for a constant $K_h$.
\item[C5] For all $z \leq \frac{1}{L}$, $p'(z), q'(z) \geq 0$ and for all $z \geq 1 - \frac{1}{L}$, we have that $p'(z), q'(z) \leq 0$.
\end{enumerate}

Suppose $\frac{1}{c_0} \leq \frac{1 - P_0}{1-Q_0} \leq c_0$. Let $\bin_l = [a_l, b_l]$ for $l=1,...,L$ be a uniformly spaced binning of the interval $[0,1]$ and let $P_l = (1- P_0) \int_{a_l}^{b_l} p(z) dz$ and $Q_l = (1-Q_0)\int_{a_l}^{b_l} q(z) dz$. Suppose $L \rightarrow \infty$ but that $L \leq \frac{2}{H}$. Define $I = -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0) p(z) q(z)} dz \right)$ and $I_L = -2 \log \left( \sqrt{P_0 Q_0} + \sum_{l=1}^L \sqrt{P_l Q_l} \right)$. Then, we have that
 $$\left| \frac{I - I_L}{I} \right| = o(1),$$ 
and that $\frac{1}{4\rho c_0} \leq \frac{P_l}{Q_l} \leq 4\rho c_0$ for all $l$. 
\end{proposition}

\medskip

\noindent The following proposition is useful for proving Theorem \ref{thm:weighted_sbm_rate2}:

\begin{proposition}
\label{prop:discretization2}
Let $p(z), q(z)$ be two densities supported on $[0,1]$. Suppose that $H \equiv \int (\sqrt{p(z)} - \sqrt{q(z)})^2 dz = \Theta(1)$.
\begin{enumerate}
\item[C1'] Suppose $p(z), q(z) \leq C$ on $[0,1]$ and are absolutely continuous.
\item[C2'] For some $r > 2$, $sup_n \int \left| \log \frac{p(z)}{q(z)} \right| dz < \infty$.
\item[C3'] Let $h(z) \geq \sup_n \max \left\{  \left|\frac{p'(z)}{p(z)} \right|, 
 \left|\frac{q'(z)}{q(z)}\right|  \right\} $. Suppose $\int |h(z)|^t dz \leq M'$ for some constant $M'$ and $1 \geq t \geq 2/r$. Suppose also that the level set $\{z \,:\, |h(z)| \geq \kappa\}$ is a union of at most $K_h$ intervals for all large enough $\kappa$.  
\item[C4']  $p'(z), q'(z) \geq 0$ for all $z < c'$ and $p'(z), q'(z) \leq 0$ for all $z > 1-c'$. 
\end{enumerate}
Let $L$ be a sequence such that $L \rightarrow \infty$. Suppose $\frac{1}{c_0} \leq \frac{1 - P_0}{1-Q_0} \leq c_0$. Let $\bin_l = [a_l, b_l]$ for $l=1,...,L$ be a uniformly spaced binning of the interval $[0,1]$ and let $P_l = (1- P_0) \int_{a_l}^{b_l} p(z) dz$ and $Q_l = (1-Q_0)\int_{a_l}^{b_l} q(z) dz$. Define $I = -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0) p(z) q(z)} dz \right)$ and $I_L = -2 \log \left( \sqrt{P_0 Q_0} + \sum_{l=1}^L \sqrt{P_l Q_l} \right)$. Then, we have that
 $$ \left| \frac{I - I_L}{I} \right| = o(1),$$ 
and that $\frac{1}{4c_0} \exp(-L^{1/r}) \leq \frac{P_l}{Q_l} \leq 4 c_0 \exp(L^{1/r})$ for all $l$. 
\end{proposition}


\subsection{Analysis on the transformation function}
\label{sec:transformation_analysis}

Proposition~\ref{prop:discretization1} and \ref{prop:discretization2} considers densities supported on $[0,1]$. This is enough for us because once we transform the densities by an application of $\Phi$, the new densities are compactly supported and, importantly, the Renyi divergence $I$ and the Hellinger divergence $H$ are invariant with respect to the transformation $\Phi$.

To see this, let $p(x), q(x)$ denote densities over $\R$ and let $p_{\Phi}(z)$ and $q_{\Phi}(z)$ denote the transformed densities over $[0,1]$. It is easy to see that $p_{\Phi}(z) = \frac{p(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$ and $q_{\Phi}(z) = \frac{q(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$. Therefore, by a change of variables $z = \Phi^{-1}(x)$, we have that the following integrals are equal:
\begin{align*}
\int_\R \sqrt{p(x)q(x)} dx &= \int_0^1 \sqrt{p_{\Phi}(z) q_{\Phi}(z)} dz \\
\int_\R (\sqrt{p(x)} - \sqrt{q(x)})^2 dx &= \int_0^1 (\sqrt{p_{\Phi}(z)} -\sqrt{q_{\Phi}(z)})^2 dz 
\end{align*}
Therefore, the divergences $I$ and $H$ between $p(x), q(x)$ are the same as the divergences between $p_{\Phi}(z)$ and $q_{\Phi}(z)$.

To prove Theorems~\ref{thm:weighted_sbm_rate1} and \ref{thm:weighted_sbm_rate2}, we then have to show that if the densities $p(x), q(x)$ satisfy assumptions A1-A5 (or A1'-A4'), then the transformed densities $p_{\Phi}(z), q_{\Phi}(z)$ satisfy assumptions C1-C5 (or C1'-C4') in Proposition~\ref{prop:discretization1} (or Proposition~\ref{prop:discretization2}). This is done through Proposition~\ref{prop:transformation1} and \ref{prop:transformation2}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Conclusion}
\label{sec:conclusion}

We have provided a rate-optimal community estimation algorithm for the homogeneous weighted stochastic block model. In the setting where the average degree is of order $\log n$ and the edge weight densities $p(x)$ and $q(x)$ are fixed, we have also characterized the exact recovery threshold. Our algorithm includes a preprocessing step consisting of transforming and discretizing the (possibly) continuous edge weights to obtain a simpler graph with edge weights supported on a finite discrete set. This approach may be useful for other network data analysis problems involving continuous distributions, where discrete versions of the problem are simpler to analyze.

Our paper is a first step toward understanding the weighted SBM under the same mathematical framework that has been so fruitful for the unweighted SBM. It is far from comprehensive, however, and many open questions remain. We describe a few here:
\begin{enumerate}
\item An important problem is to extend our analysis to the case of a \emph{heterogenous} stochastic block model, where edge weight distributions depend on the exact community assignments of both endpoints. In such a setting, Abbe and Sandon~\cite{AbbSan15} and Yun and Proutiere~\cite{yun2016optimal} have shown that a generalized information divergence---the CH divergence---governs the intrinsic difficulty of community recovery. We believe that a similar discretization-based approach should lead to analogous results in the case of a heterogeneous weighted SBM. The key challenge would be to show that discretization does not lose much information with respect to the CH-divergence.
\item Real-world networks often have nodes with very high degrees, which may adversely affect the accuracy of recovery methods for the stochastic block model. To solve this problem, degree-corrected SBMs \cite{zhao2012consistency, gao2016community} have been proposed as an effective alternative to regular SBMs. It is straightforward to extend the concept of degree-correction to the weighted SBM, but it is unclear whether our discretization-based approach would be effective in obtaining optimal error rates.
\item It is easy to extend our results to the weighted \emph{and} labeled SBMs if the number of labels is finite or assumed to be slowly growing. However, this excludes some interesting cases, including the setting where edge labels represent counts  from a Poisson distribution. We suspect that in such a situation, it may be possible to combine low-probability labels in a clever way to obtain a discretization that is again amenable to our approach. 
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\bibliographystyle{plain}
\bibliography{paper}


\appendix

\section{Proof of Proposition~\ref{prop:labeled_sbm_rate}}
\label{appendix: first}
The proof of Proposition~\ref{prop:labeled_sbm_rate} is quite involved. We structure the proof to follow the flow of our algorithm.  Algorithm~\ref{alg:noisify} is where we deliberately add noise to the graph by changing edge colors at random. This may seem like a suboptimal step to take: after all, adding such random noise is only going to destroy information and make it harder to cluster the communities. However, we show in Lemma~\ref{lemma: first} that this process does not significantly affect the Renyi divergence term $I_L$, and ensures that the new probabilities of edge labels are at least $c/n$ for a constant $c$. This lower bound is crucial to the rest of our analysis. The proof of Lemma~\ref{lemma: first} may be found in Appendix~\ref{appendix: lemmas for labeled_sbm_rate}. To simplify notation, we continue to refer the new edge label probabilities as $P_l$ and $Q_l$ throughout the proof.

Next, our algorithm performs a spectral clustering using only the edges with label $l$, and calculates $\hat I_l := \frac{(\hat P_l - \hat Q_l)^2}{\hat P_l \vee \hat Q_l}$, where $\hat P_l$ and $\hat Q_l$ are the estimated probabilities obtained by using the output of spectral clustering, defined as follows: 
\begin{align*}
\hat P_l &= \frac{ \sum_{u \neq v \,:\, \tilde{\sigma}_l(u) = \tilde{\sigma}_l(v) } (A_l)_{uv} }{ |{u \neq v \,:\, \tilde{\sigma}_l(u) = \tilde{\sigma}_l(v) }| }, \quad\text{ and }\\
\hat{Q}_l &=  \frac{ \sum_{u \neq v \,:\, \tilde{\sigma}_l(u) \neq \tilde{\sigma}_l(v) } (A_l)_{uv} }{ |{u \neq v \,:\, \tilde{\sigma}_l(u) \neq \tilde{\sigma}_l(v) }| },
\end{align*}
where $A_l$ is the adjacency matrix for label $l$. We then select $l^*$ to be the color that has the largest value of $\hat I_l$. Notice that if $\hat P_l$ and $\hat Q_l$ closely approximate the true $P_l$ and $Q_l$, then $\hat I_l$ is a measure of how good a color is for clustering: if $\hat I_l$ is large, then the true probabilities are well-separated and therefore provide more information about the community structure than if $\hat I_l$ is small. Naturally, the estimated edge probabilities are reasonably good only if the spectral clustering is reasonably good. Our first proposition makes this statement rigorous. Before stating the proposition, we classify the colors into two sets $L_1$ and $L_1^c$ as follows:
$$L_1 = \left\{ l : \frac{n(P_l-Q_l)^2}{P_l \vee Q_l} := \frac{\Delta_l^2}{P_l \vee Q_l} \geq 1 \right\}.$$
Notice that  a color is ``good" or ``bad" for clustering depending on the value $\frac{n(P_l-Q_l)^2}{P_l \vee Q_l}$, since it captures how well-separated the edge probabilities are. We may now bound difference between the estimated probabilities and the true probabilities for good and bad colors using Proposition~\ref{prop:estimation_consistency}, the formal statement and proof of which may be found in Appendix~\ref{appendix: hats galore}.
\begin{repproposition}{prop:estimation_consistency}
Suppose $\sigma$ is a clustering with error rate at most $\gamma$ i.e., $l(\sigma, \sigma_0) \leq \gamma $. Then with probability at least $1 - L n^{-(3 + \delta_p)}$ for a small $\delta_p > 0$, the following event happens for all small enough $\gamma$:
\begin{enumerate}
\item For $l \in L_1$, we have $| \hat{P}_l - P_l | \leq \eta \Delta_l$ and $| \hat{Q}_l - Q_l | \leq \eta \Delta_l.$
\item For $l \in L_1^c$, we have $| \hat{P}_l - P_l | \leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}$ and $| \hat{Q}_l - Q_l | \leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}$.
\end{enumerate}
In both cases,  $\eta = C \sqrt{\gamma \log \frac{1}{\gamma}}$ for an absolute constant $C$.
\end{repproposition}


We will now work towards getting a good initial clustering with a small error rate $\gamma$. In Proposition~\ref{prop:spectral_analysis}, we show that if the edge probabilities for a particular label are well-separated, then the spectral clustering output of Algorithm~\ref{alg:spectral} is indeed reasonably good. We give a rough statement of the proposition here, and refer to Appendix~\ref{appendix: spectral} for the precise statement and its proof.
\begin{repproposition}{prop:spectral_analysis}
If $P_l$ and $Q_l$ satisfy $C_1 \frac{(P_l \vee Q_l)}{n (P_l-Q_l)^2} \leq 1$ for an absolute constant $C_1$,  the output $\sigma^l$ of the spectral clustering Algorithm~\ref{alg:spectral} satisfies the inequality
\[
l(\sigma^l, \sigma_0) \leq C_2 \frac{(P_l \vee Q_l)}{n (P_l-Q_l)^2},
\]
for a constant $C_2$ with probability at least $1 - n^{-4}$.
\end{repproposition}
Thus, if we want to cluster with arbitrarily small error rates $\gamma$, we need $ \frac{(P_l \vee Q_l)}{n (P_l-Q_l)^2} \to 0$ for at least one well-separated color $l$. This is precisely what Algorithm~\ref{alg:initialization1} does when it chooses $l^*$. We show that $l^*$ satisfies $ \frac{(P_{l^*} \vee Q_{l^*})}{n (P_{l^*}-Q_{l^*})^2} \to 0$ in two steps. 

\medskip

First,  we combine the results of Propositions~\ref{prop:estimation_consistency} and \ref{prop:spectral_analysis} and show that for sufficiently well-separated colors, the estimated $\hat I_l$ is close to $\frac{( P_l -  Q_l)^2}{P_l \vee Q_l}$. If the probabilities are not well-separated, then we claim that $\hat I_l$ is negligibly small. We give a rough statement of Proposition~\ref{prop:initial_guarantee} here, and refer to Appendix~\ref{appendix: init} for the precise statement and its proof.
\begin{repproposition} {prop:initial_guarantee}
There is a positive constant $C_{test}$ such that with probability at least $1 - Ln^{-3 + \delta_p}$, for a small $\delta_p > 0$, we have
\begin{enumerate} 
\item If $\Delta_l \geq \sqrt{C_{test}} \sqrt{ \frac{P_l \vee Q_l}{n}},$ then $\frac{ | \hat{P}_l - \hat{Q}_l| }{\sqrt{ \hat{P}_l \vee \hat{Q}_l }} = \Theta \left(\frac{ | P_l - Q_l | }{\sqrt{ P_l \vee Q_l}} \right).$
\item If $\Delta_l <  \sqrt{C_{test}} \sqrt{ \frac{P_l \vee Q_l}{n} }$, then
$\frac{ | \hat{P}_l - \hat{Q}_l|}{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} \leq O\left( \sqrt{ \frac{1}{n} } \right).$ 
\end{enumerate}
\end{repproposition}
Next, we then argue that 
$$I_L = \Theta\left( \sum_{l \in L_1} \frac{\Delta_l^2} {P_l \vee Q_l} \right),$$
and use this fact to conclude the existence of a color $l$ for which $\frac{n( P_l -  Q_l)^2}{P_l \vee Q_l}$ is arbitrarily large. Using Proposition~\ref{prop:initial_guarantee}, we then conclude that Algorithm~\ref{alg:initialization1}  succeeds in choosing a color $l^*$ for which $\frac{n( P_{l^*} -  Q_{l^*})^2}{P_{l^*} \vee Q_{l^*}}$ is arbitrarily large. This is stated and proved in the following result:
\begin{repproposition}{prop:initialization_correctness}
With probability at least $1 - 2L n^{-(3+\delta_p)}$, we have that $\frac{n (P_{l^*}-Q_{l^*})^2}{\rho_L^4(P_{l^*} \vee Q_{l^*})} \geq a_n$ for suitable sequence $a_n \to \infty$. 
\end{repproposition}
Again, we refer to Appendix~\ref{appendix: starry night} for the precise statement and its proof. Let event $E_1$ denote the successful selection of $l^*$ satisfying the inequality in Proposition~\ref{prop:initialization_correctness}. Having selected $l^*$, we now perform spectral clustering $n$ times, by leaving one vertex out and clustering on the remaining graph. Denote the community assignments obtained in this manner by $\tilde \sigma_u$, for $1 \leq u \leq n$. Note that Proposition~\ref{prop:spectral_analysis} combined with a simple union bound gives that with probability at least $1-n^{-3}$, all of these clusterings have error rate at most $\gamma$, where $\gamma$ satisfies
\[
\gamma \leq  C\frac{ P_{l^*} \vee Q_{l^*}}{ n ( P_{l^*} - Q_{l^*} )^2}, 
\]
for some constant $C$. Let us denote this event by $E_2$. Conditioned on $E_1$ and $E_2$ described above, we have that $\gamma \rho_L^4 \rightarrow 0$. Thus, we can apply Proposition~\ref{prop:estimation_consistency} on each of the rough clustering $\tilde{\sigma}_u$'s and show that the conclusion of Proposition~\ref{prop:estimation_consistency} holds simultaneously for all $\tilde{\sigma}_u$ with probability at least $1 - Ln^{2 + \delta_p}$. Furthermore, the $\eta$ that appears in Proposition~\ref{prop:estimation_consistency} is $\Theta \left( \sqrt{\gamma \log \frac{1}{\gamma} } \right)$, and thus it satisfies satisfies $|\eta \rho_L| \rightarrow 0$. Let us denote this event by $E_3$. Having obtained the $n$ clusterings $\tilde \sigma_u$, we now create $\hat \sigma_u$ by assigning vertex $u$ to an appropriate community in $\tilde \sigma_u$, using the relation from Algorithm~\ref{alg:refinement}
 \[
    \hat{\sigma}_u(u) = \argmax_k \sum_{v \,:\, \tilde{\sigma}_u(v) = k ,\, v\neq u} 
         \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l). 
\]    
In Proposition~\ref{prop:single_node_error_bound}, we show that with high probability, the assignment $\hat \sigma_u(u)$ is ``correct". We give a rough statement of  the proposition here, and defer the exact statement and its proof to Appendix~\ref{appendix: single node}. We may check that all that all the necessary pre-conditions for applying Proposition~\ref{prop:single_node_error_bound}, in particular $|\eta \rho_L| \to 0$ are satisfied when event $E_3$ occurs.
\begin{repproposition}{prop:single_node_error_bound}
Let $\pi_u \in S_K$ be a permutation such that $l(\sigma_0, \tilde \sigma_u) = d(\sigma_0, \pi_u(\tilde \sigma_u)).$ Then with probability at least $1 - \exp \left( - (1 - o(1)) \frac{n}{\beta K} I_L \right)$, we have $\pi_u^{-1}(\sigma_0(u)) = \hat \sigma_u(u).$
\end{repproposition}
Since we haven't yet addressed what $\pi_u$ is and whether it is unique, we elaborate a bit more on the topic. By construction of $\hat{\sigma}_u$, the error rate of $\hat{\sigma}_u$ is at most $ \gamma + \frac{1}{n}$ and thus, $l(\hat{\sigma}_u, \sigma_0) < \frac{1}{8 \beta K}$ for small enough $\gamma$. Let $\pi_u \in S_K$ denote a permutation such that $d(\pi_u(\hat{\sigma}_u), \sigma_0) < \frac{n}{8\beta K}$. We argue that $\pi_u$ is the unique permutation that satisfies $l(\hat \sigma_u, \sigma_0) = d(\pi_u(\hat{\sigma}_u), \sigma_0)$. We show this by first observing that for any $u$, the minimum cluster size of the clustering $\hat{\sigma}_u$ is at least $\frac{n}{\beta K} - (n \gamma + 1) \geq \frac{ n(1 - \beta K \gamma  - \beta K/n)}{\beta K} \geq \frac{n}{2\beta K}$ for small enough $\gamma$. Now we use a simple argument in  Lemma~\ref{lem:consensus_uniqueness} which shows that if the Hamming distance $d(\pi(\sigma'), \sigma_0)$ for some permutation $\pi \in S_K$ and some assignment $\sigma'$ is at most half the size of the smallest cluster ($n/2\beta K$ in this case), then $\pi$ is the unique permutation that satisfies $l(\sigma', \sigma_0) = d(\pi(\sigma'), \sigma_0)$. 

Restating Proposition~\ref{prop:single_node_error_bound}, we have
\[
 P( \hat{\sigma}_u(u) \neq \pi^{-1}_u(\sigma_0(u)) \given E_3) \leq \exp\left( - (1+o(1)) \frac{n I_L}{\beta K} \right).
\]
The final stage of our algorithm is the consensus stage, where we combine the $\hat \sigma_u$'s to produce a $\hat \sigma$ according to the rule
\[
\hat{\sigma}(u) = \argmax_k | \{ v \,:\,  \hat{\sigma}_1(v) = k \} \cap
                                 \{ v \,:\, \hat{\sigma}_u(v) = \hat{\sigma}_u(u) \}|.
\]
This means that if the cluster containing $u$ in $\hat \sigma_u$ has the largest overlap with some cluster $k$ in $\hat \sigma_1$, then $u$ is assigned to cluster $k$; i.e. we put $\hat \sigma (u) = k.$ Note that $\hat \sigma_1$ and $\hat \sigma_u$ both have an error rate of at most $\gamma + 1/n$, and since $\gamma$ is small, we may conclude that these two assignments are close to each other. Intuitively, one may guess that the permutation $\xi_u \in S_K$ that minimizes $d(\hat \sigma_1, \xi_u(\hat \sigma_u))$ is the consensus function; i.e. $\hat \sigma(u) = \xi_u(\hat \sigma_u(u))$. This is indeed the case, and we show this rigorously. Using the triangle inequality, we have that $l(\hat{\sigma}_u, \hat{\sigma}_1) \leq 2\gamma + 2/n < \frac{1}{4\beta K}$ for small enough $\gamma$. Thus, the assignments $\hat \sigma_1$ and $\hat \sigma_u$ are very close to each other. We then apply Lemma~\ref{lem:consensus_analysis} on the pair $(\hat{\sigma}_1, \hat{\sigma}_u)$ to show that the consensus function $\xi_u$ is the permutation that minimizes $d(\hat{\sigma}_1, \xi_u(\hat{\sigma}_u))$.

Observe that $\sigma_0, \hat \sigma_1$, and $\hat \sigma_u$ are all close to each each other, and therefore it is plausible that the permutations $\pi_1$ and $\pi_u$ that minimize $d(\sigma_0, \pi_1(\hat \sigma_1))$ and $d(\sigma_0, \pi_u(\hat \sigma_u))$ respectively, must be related by $\xi_u$, the permutation that minimizes $d(\hat \sigma_1, \xi_u(\hat \sigma_u))$, by the relation $\xi_u = \pi_1^{-1} \circ \pi_u$. We prove this relation rigorously as well. We know that $d(\sigma_0, \pi_1(\hat{\sigma}_1)) < \frac{n}{8 \beta K}$ and that $d(\sigma_0, \pi_u(\hat{\sigma}_u)) < \frac{n}{8 \beta K}$. Therefore, $d(\hat{\sigma}_1, \pi_1^{-1}( \pi_u(\hat{\sigma}_u))) < \frac{n}{4 \beta K}$. Since the minimum cluster size of both $\hat{\sigma}_1$ and $\hat{\sigma}_u$ is $\frac{n}{2 \beta K}$, we may apply Lemma~\ref{lem:consensus_uniqueness} to conclude that $\pi_1^{-1} \circ \pi_u = \xi_u$. Thus, we have that
\begin{align*}
P( \hat{\sigma}(u) \neq \pi_1^{-1} ( \sigma_0(u)) \given E_3) &= P( \hat{\sigma}(u) \neq \xi_u \circ \pi_u^{-1} (\sigma_0(u)) \given E_3)\\
 &=  P( \xi_u^{-1}(\hat{\sigma}(u)) \neq \pi_u^{-1} (\sigma_0(u)) \given E_3)\\
 &= P( \hat{\sigma}_u(u) \neq \pi_u^{-1} (\sigma_0(u)) \given E_3). 
\end{align*}
Using Proposition~\ref{prop:single_node_error_bound}, 
\begin{align*}
P( \hat{\sigma}(u) \neq \pi_1^{-1} ( \sigma_0(u)) ) &\leq  \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) + P(E_3^c \given E_2, E_1) + P(E_2^c) + P(E_3^c) \\
    &\leq \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) + Ln^{-(2 + \delta_p)} + n^{-3} + Ln^{-3} \\
    &\leq  \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) + n^{-(1 + \delta_p)} + n^{-3} + n^{-2} \\
    &\leq  \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) + n^{-(1 + \delta_p)}
\end{align*}
where $\eta'$ is some $o(1)$ sequence. We take then $\eta'' = \eta' + \beta \sqrt{ \frac{K}{ n I_L} } = o(1)$. First, suppose that $$\exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) \geq n^{-(1+\delta_p/2)}.$$
In this case, we argue as follows:
\begin{align*}
& P\left\{ l(\hat{\sigma}, \sigma_0) >   \exp\left( -(1 - \eta'') \frac{n I_L}{\beta K} \right) \right\} \\
&\leq \frac{\E l(\sigma_0, \hat \sigma)}{ \exp\left( -(1 - \eta'') \frac{n I_L}{\beta K} \right)}\\
 &\leq \frac{1}{ \exp\left( -(1 - \eta'') \frac{n I_L}{\beta K} \right) } \frac{1}{n} \sum_{u=1}^n P(\hat{\sigma}(u) \neq \pi_1^{-1}(\sigma_0(u))) \\
 &\leq \exp\left\{ -(\eta'' - \eta') \frac{n I_L}{\beta K} \right\} + \frac{ C n^{-(1 + \delta_p)} }{ \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right)  }\\
 &\leq \exp \left\{ - \sqrt{ \frac{n I_L}{K} } \right\} + n^{-\delta_p/2} = o(1).
\end{align*}
On the other hand, if we have 
$$ \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) \leq n^{-(1 + \delta_p/2)},$$
then
\begin{align*}
& P \left\{ l(\hat{\sigma}, \sigma_0) >  \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right)  \right\} \\
&\leq P( l(\hat{\sigma}, \sigma_0) > 0 ) \\
& \leq \sum_{u=1}^n P( \hat{\sigma}(u) \neq \pi_1^{-1}(\sigma_0(u))) \\
&\leq n \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right)  + n^{-\delta_p} \leq n^{-\delta_p /2} = o(1).
\end{align*}
This completes the proof of Proposition~\ref{prop:labeled_sbm_rate}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Appendix for Proposition~\ref{prop:labeled_sbm_rate}}
\label{appendix: labeled_sbm_rate}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis of estimation error of $\hat{P}_l$ and $\hat{Q}_l$}
\label{appendix: hats galore}

\begin{proposition}
\label{prop:estimation_consistency}
Let $A_L$ be a labeled network with true clustering $\sigma_0$. Suppose $\sigma$ is a random initial clustering with error rate at most $\gamma$ i.e., $l(\sigma, \sigma_0) \leq \gamma $. Let $\Delta_l = | P_l - Q_l |$. Let $\hat{P}_l = \frac{\sum_{u \neq v \,:\, \sigma(u)=\sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {\sum_{u \neq v \,:\, \sigma(u) = \sigma(v)} 1}$ and
    $\hat{Q}_l = \frac{\sum_{u \neq v \,:\, \sigma(u) \neq \sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {\sum_{u \neq v \,:\, \sigma(u) \neq \sigma(v)} 1}$ be the MLE of $P_l$ and $Q_l$ based on $\sigma$. Let $\delta_p$ be a positive, fixed, and arbitrarily small real number. Let $c$ be an absolute positive constant. Then with probability at least $1 - L n^{-(3 + \delta_p)}$, the following event happens for all small enough $\gamma$:
\begin{enumerate}
\item For all $l$ such that $P_l \vee Q_l \geq \frac{c}{n}$, if $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq 1$, then
\begin{align*}
 | \hat{P}_l - P_l | &\leq \eta \Delta_l, \quad \text{ and } \\
 | \hat{Q}_l - Q_l | &\leq \eta \Delta_l. 
\end{align*}
\item For all $l$ such that $P_l \vee Q_l \geq \frac{c}{n}$, if $\frac{n \Delta_l^2}{P_l \vee Q_l} \leq 1$, then
\begin{align*}
 | \hat{P}_l - P_l | &\leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}, \quad \text{ and } \\
 | \hat{Q}_l - Q_l | &\leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}.
\end{align*}
\end{enumerate}
In both cases,  $\eta = C \sqrt{\gamma \log \frac{1}{\gamma}}$ for an absolute constant $C$.
\end{proposition}

\begin{proof}
Our proof strategy proceeds by showing that for almost all random graphs generated according to the labeled SBM model, the event described in Proposition~\ref{prop:estimation_consistency} holds for every single assignment in set of all possible assignments with error rate at most $\gamma$. For a fixed assignment $\sigma$ with error rate $\gamma$, we call a random graph as ``bad graph for $\sigma$" if the event in Proposition \ref{prop:estimation_consistency} does not hold for that graph and that $\sigma$. For such a fixed $\sigma$, we upper-bound the probability of the set of bad graphs for $\sigma$. We then use the union bound over all possible choices of $\sigma$ and show that the set of bad graphs has probability at most $Ln^{-(3+\delta_p)}$, and thus conclude the proof. 

Note that there are at most $\binom{n}{\gamma n} K^{\gamma n}$ possible assignments $\sigma$'s that satisfy the error rate constraint. 
\begin{align*}
\log \binom{n}{\gamma n} K^{\gamma n} & =
  \log \left( \frac{ n(n-1) ...(n-\gamma n+1) }{(\gamma n)!} \right) + \gamma n \log K \\
 & \leq \log \left( \frac{ n^{\gamma n} e^{\gamma n} }
     { (\gamma n)^{\gamma n} } \frac{1}{\sqrt{2\pi \gamma n}} \right) + \gamma n \log K \\
 & \leq \log \left( \frac{ e^{\gamma n} }{\gamma^{\gamma n}} \right) - \frac{1}{2} \log 2 \pi \gamma n + \gamma n \log K \\
 & \leq \gamma n \log \frac{e}{\gamma}  + \gamma n \log K \\
 & \leq  \gamma n \log \frac{Ke}{\gamma}\\
 &\leq C_1 n\gamma \log \frac{1}{\gamma},
\end{align*}
for a suitable constant $C_1$ and for all small enough $\gamma$.
Next, we bound the bias of $\hat{P}_l$.
Our estimator of $P_l$ is 
\[
\hat{P_l} = \frac{ \sum_{u \neq v \,:\, \sigma(u) = \sigma(v)} \mathbf{1}(A_{uv} = l) }{
                   \sum_{u \neq v \,:\, \sigma(u) = \sigma(v)} 1 }.
\]
The expected value $\E \hat{P}_l$ is a convex combination of $P_l, Q_l$, given by
\begin{align}
\E \hat{P_l} &= 
   \frac{ \sum_{u \neq v \,:\, \sigma(u) = \sigma(v)} 
             \mathbf{1}(\sigma_0(u) = \sigma_0(v) ) P_l + 
               \mathbf{1}(\sigma_0(u) \neq \sigma_0(v)) Q_l }{
                   \sum_{u \neq v \,:\, \sigma(u) = \sigma(v)} 1 } \nonumber \\
  &= (1 - \lambda) P_l + \lambda Q_l  = P_l + \lambda (Q_l - P_l). \label{eqn:bias_simple_bound}
\end{align}
for $\lambda = \frac{\sum_{u \neq v \,:\, \sigma(u) = \sigma(v)} 
     \mathbf{1}(\sigma_0(u) \neq \sigma_0(v)) }{\sum_{u \neq v \,:\, \sigma(u) = \sigma(v)} 1}$. Thus, we have that 
\[
|\E \hat{P}_l - P_l | \leq \lambda |Q_l - P_l|.
\]
Observe that $\lambda$ may be bounded from above as follows:
\begin{align*}
\lambda 
  &= \frac{\sum_{u \neq v \,:\, \sigma(u) = \sigma(v) }\mathbf{1}(\sigma_0(u) \neq \sigma_0(v)) }{\sum_{u \neq v} \mathbf{1}(\sigma(u) = \sigma(v)) } 
      \\
  &= 
   \frac{\sum_k \sum_{u \neq v \,:\, \sigma(u)=\sigma(v)=k} \mathbf{1}(\sigma_0(u) \neq \sigma_0(v))}{\sum_k \hat{n}_k (\hat{n}_k-1)} 
      \\
  &\leq \frac{\sum_k \sum_{u \neq v \,:\, \sigma(u)=\sigma(v)=k} 
       \mathbf{1}( \neg (\sigma_0(u) = \sigma_0(v) =k) )}{\sum_k \hat{n}_k (\hat{n}_k-1)} 
      \\ 
  &\leq \frac{ \sum_k \sum_{u \neq v \,:\, \sigma(u)=\sigma(v)=k} \mathbf{1}(\sigma_0(v)) \neq k) + \mathbf{1}(\sigma_0(u) \neq k)}
            {\sum_k \hat{n}_k (\hat{n}_k - 1)}.
\end{align*}
Define $\gamma_k = \frac{1}{n} \sum_{u \,:\, \sigma(u)=k} \mathbf{1}(\sigma_0(u) \neq k)$ as the error rate within the estimated cluster $k$, and define $\hat{n}_k = \sum_u \mathbf{1}(\sigma(u) = k)$. Then, we have that $\sum_k \gamma_k = \gamma$ and also $\sum_{u \,:\, \sigma(u) = k} \sum_{v \,:\, \sigma(v) = k} \mathbf{1}(\sigma_0(v) \neq k) = \gamma_k n \hat{n}_k$. We continue the bound: 
\begin{align*}
\lambda  
  &\leq \frac{ \sum_k 2 \gamma_k n \hat{n}_k }{\sum_k \hat{n}_k(\hat{n}_k - 1)} 
     \\
  &= \frac{n}{\sum_k \hat{n}_k (\hat{n}_k - 1) } \sum_k 2 \gamma_k \hat{n}_k 
     \\
  &\leq \frac{K}{n-K} n \sum_k 2 \gamma_k \frac{\hat{n}_k}{n} \\
  &\leq 4 \gamma K.
\end{align*}
In the first inequality, we used the fact that $\sum_k \frac{\hat{n}_k }{n} (\hat{n}_k - 1) = n \sum_k \left( \frac{\hat{n}_k}{n} \right)^2 - 1 \geq \frac{n}{K} - 1$ since $\sum_k \frac{\hat{n}_k}{n} = 1$. In the last inequality, we used the assumption that $K < \frac{n}{2}$. 
We then have an upper bound for $\lambda \leq 4 \gamma K$. Therefore, we have that
\[
|\E \hat{P}_l - P_l | \leq 4 \gamma K \Delta_l,
\]
where $\Delta_l  = |Q_l - P_l|$. A similar calculation may also be performed for the bias of $\hat Q_l$, and taking the worse of the two bounds for $\lambda$ obtained in each case, we conclude that there exists a constant $C_2$ such that 
\begin{align*}
|\E \hat{P}_l - P_l | &\leq C_2 \gamma \Delta_l \quad \text{ and }\\
|\E \hat{Q}_l - Q_l | &\leq C_2 \gamma \Delta_l.
\end{align*}
To simplify presentation, we define $\eta_1 = C_2 \gamma$ so that
\begin{equation} \label{eq: A1.5}
|\E \hat{P}_l - P_l | \leq \eta_1 \Delta_l.
\end{equation}
From this it is clear that $\eta_1$ becomes arbitrarily small if $\gamma$ is made arbitrarily small. 

Having bounded the bias, we can now bound the variance. Let $ \tilde{A}_{uv} = \mathbf{1}(A_{ij} = l)$. Then, by Bernstein's inequality,
\[
P\left( \left| \sum_{u,v\,:\, \sigma(u) = \sigma(v)} (\tilde{A}_{uv} - \E \tilde{A}_{uv} ) \right|  > t 
 \right) \leq 2 \exp\left( 
    - \frac{t^2}{ 2 \sum_{u,v \, \sigma(u) = \sigma(v)} \E \tilde{A}_{uv}  + \frac{2}{3}t }
\right).
\]
We first bound $\sum_{u,v \, \sigma(u) = \sigma(v)} \E \tilde{A}_{uv}$:
\begin{align*}
\sum_{u,v \, \sigma(u) = \sigma(v)} \E \tilde{A}_{uv} &=
  \sum_k \hat{n}_k (\hat{n}_k - 1) \E \hat{P}_l \\
 &\leq (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1). \quad 
  \trm{(by Equation~\ref{eqn:bias_simple_bound})}
\end{align*}
Therefore,
\[
P\left( \left| \sum_{u,v\,:\, \sigma(u) = \sigma(v)} (\tilde{A}_{uv} - \E \tilde{A}_{uv} ) \right|  > t 
 \right) \leq 2 \exp\left( 
    - \frac{t^2}{ 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1)  + \frac{2}{3}t } 
\right).
\]
Our goal is to choose an appropriate $t$ such that the probability is upper bounded by $2\exp( - C_1 \gamma n \log \frac{1}{\gamma} - (3+\delta_p) \log n )$. We choose the following $t$
\begin{align*}
t^2 &= 4 \left\{  \left( 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) \right) 
      \left( 
     C_1\gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right) \right\} 
      \vee 
   4  \left\{
   \left(  C_1\gamma n \log \frac{1}{\gamma} + (3 + \delta_p) \log n \right)^2 \right\} 
\end{align*}
We now verify that regardless of which term among $\{ 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) ,\,   C_1\gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n\}$ is larger, the probability term is at most $2 \exp\left( -  \left(  C_1\gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right) \right)$. Let $A = 2(P_l \vee Q_l)\sum_k \hat{n}_k (\hat{n}_k - 1)$ and
 $B =   C_1\gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n$. We verify our claim by a direct calculation in each of the two cases:
\begin{enumerate}
\item Suppose $A \geq B$, then $t^2 = 4AB$ and the probability term is at most
    $2 \exp \left( - \frac{4 AB}{A + \frac{4}{3} \sqrt{AB}} \right)  \leq 2 \exp \left( - \frac{4 AB}{A + \frac{4}{3} A} \right) \leq  2 \exp( - B )$. 
\item Suppose $A \leq B$, then $t^2 = 4B^2$ and the probability term is at most
       $2 \exp \left( - \frac{4 B^2}{A + \frac{4}{3} B} \right) \leq 2 \exp \left( - \frac{4 B^2}{ B + \frac{4}{3} B} \right) \leq 2 \exp( - B)$. 
\end{enumerate}
Thus, with probability at least $1- 2 \exp\left( -  \left(  C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right) \right)$,
\begin{align*}
| \hat{P}_l - \E \hat{P}_l | =
\frac{\sum_{u,v \, \sigma(u) = \sigma(v)} (\tilde{A}_{uv} - \E \tilde{A}_{uv} ) }{
  \sum_{u,v} \mathbf{1}( \sigma(u) = \sigma(v) ) } &<
  \frac{t}{\sum_{u,v} \mathbf{1}(\sigma(u) = \sigma(v)) }. \\
\end{align*}
Now we derive a more manageable upper bound for $t$. Note that $t^2 = \max(4AB, 4B^2) \leq 4(\sqrt{AB} +B)^2$; i.e.,
\[
t^2 \leq 4 \left\{ \sqrt{  \left(2 (P_l \vee Q_l)  \sum_k \hat{n}_k (\hat{n}_k - 1) \right) 
               \left( C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right)} 
           + 
          \left(   C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right) \right\}^2.
\]
Note that we can without loss of generality assume $\gamma \geq \frac{1}{n}$, since if $\gamma = 0$ the result follows trivially. We then have that $\gamma n \log \frac{1}{\gamma} \geq \log n$. Thus, $ C_1\gamma n \log \frac{1}{\gamma} + (3 + \delta_p) \log n \leq \tilde C_1 \gamma n \log \frac{1}{\gamma}$.


\begin{align*}
 \frac{t}{\sum_{u,v} \mathbf{1}(\sigma(u) = \sigma(v)) } &\leq
  2 \frac{  \sqrt{  \left( 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) \right) 
                 \left(  C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right) }  
          + 
               \left(  C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n\right)  }
        { \sum_{u,v} \mathbf{1}(\sigma(u) = \sigma(v) ) }  \\
        %
  &\leq 2 \frac{\sqrt{2 (P_l \vee Q_l) \tilde C_1 \gamma n \log \frac{1}{\gamma}}}
             {\sqrt{ \sum_k \hat{n}_k (\hat{n}_k - 1)}} + 
        2  \frac{\tilde C_1\gamma n \log \frac{1}{\gamma}}
             {\sum_k \hat{n}_k (\hat{n}_k - 1)} \\
             %
 &\stackrel{(a)}\leq  2 \frac{ \sqrt{2 (P_l\vee Q_l)} \sqrt{ \tilde C_1 \gamma K \log \frac{1}{\gamma}} }
           {\sqrt{n - K}} + 
       2 \frac{\tilde C_1 \gamma K \log \frac{1}{\gamma}}{n - K} \\
       %
 &\stackrel{(b)}\leq 4 \sqrt{ \frac{P_l \vee Q_l}{n} } \sqrt{\tilde C_1 K \gamma \log \frac{1}{\gamma}} + 
       4 \frac{\tilde C_1 K \gamma \log \frac{1}{\gamma}}{n}. 
\end{align*}
For $(a)$ we used the fact that $\sum_k (\hat{n}_k - 1) \frac{\hat{n}_k}{n} = n \sum_k \left( \frac{\hat{n}_k}{n} \right)^2 - 1 \geq \frac{n}{K} - 1 $ because $\frac{\hat{n}_k}{n}$ sums to 1. For $(b)$, we used the assumption that $n-K \geq \frac{n}{2}$. To further simplify the expression, we have that $P_l \vee Q_l \geq \frac{c}{n}$ and thus, $\frac{1}{n} \leq \frac{1}{\sqrt c}\sqrt{ \frac{P_l \vee Q_l}{n} }$. Therefore, we have that, with probability at least $1 - \exp( - C_1 \gamma n \log \frac{1}{\gamma} - (3 + \delta_p) \log n)$, 
\begin{align}
| \hat{P}_l - \E \hat{P}_l | \leq \sqrt{ \frac{P_l \vee Q_l}{n} } \left( C_1' \sqrt{\gamma \log \frac{1}{\gamma}} + C_2' \gamma \log \frac{1}{\gamma} \right) \label{eqn:variance_bound}
\end{align}
for suitable constants $C_1'$ and $C_2'$. Using a similar calculation, we may also show that there exist suitable constants $C_3'$ and $C_4'$ such that
\begin{align}
| \hat{Q}_l - \E \hat{Q}_l | \leq \sqrt{ \frac{P_l \vee Q_l}{n} } \left( C_3' \sqrt{\gamma \log \frac{1}{\gamma}} + C_4' \gamma \log \frac{1}{\gamma} \right).
\end{align}
Note that for small enough $\gamma$, the term $\sqrt{\gamma \log \frac{1}{\gamma}}$ dominates, and thus we may take choose the right hand side to be 
$$\eta_2 \sqrt{ \frac{P_l \vee Q_l}{n} }$$
where $\eta_2 = C_3 \sqrt{\gamma \log \frac{1}{\gamma}}$ for a constant $C_3$. It is clear that $\eta_2$ can be made arbitrarily small by taking $\gamma$ to be arbitrarily small. Taking the union bound across all clusterings with error $\gamma$ and across all colors, we have that the probability of (\ref{eqn:variance_bound}) holding simultaneously for all colors $l$ is at least $1 - L n^{-(3+\delta_p)}$. Combining equations \eqref{eq: A1.5} and \eqref{eqn:variance_bound}, we arrive at the bound
\begin{equation}\label{eq: combine}
|P_l - \hat P_l| \leq \eta_1 \Delta_l + \eta_2 \sqrt{\frac{P_l \vee Q_l}{n}}.
\end{equation}
If $\frac{n\Delta_l^2}{P_l \vee Q_l} \geq 1$, then
\begin{equation}
|P_l - \hat P_l| \leq \eta_1 \Delta_l + \eta_2 \Delta_l = (\eta_1 + \eta_2) \Delta_l.
\end{equation}
If $\frac{n\Delta_l^2}{P_l \vee Q_l} < 1$, then
\begin{equation}
|P_l - \hat P_l| \leq \eta_1\sqrt{\frac{P_l \vee Q_l}{n}}+ \eta_2 \sqrt{\frac{P_l \vee Q_l}{n}} = (\eta_1 + \eta_2)\sqrt{\frac{P_l \vee Q_l}{n}}. 
\end{equation}
Note that $\eta_2 = C_3 \sqrt{\gamma \log \frac{1}{\gamma}}$ dominates $\eta_1 = C_1 \gamma$ for all small enough $\gamma$, and thus we may substitute $\eta_1+\eta_2$ by $C_4\sqrt{\gamma \log \frac{1}{\gamma}}$ for all small enough $\gamma$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Analysis of the spectral clustering algorithm}
\label{appendix: spectral}
Define $\bar{d} = \frac{1}{n} \sum_{u=1}^n d_u$ be the average degree.

\begin{proposition}
\label{prop:spectral_analysis}
Suppose that an unweighted $A$ is drawn from a homogeneous stochastic block model with probabilities $p, q$ and cluster imbalance factor $\beta$, with the number of communities $K$ fixed. Suppose $p, q \geq \frac{c}{n}$ for some absolute constant $c$. Then there exist a constant $C$ such that the output $\sigma$ of the spectral clustering (algorithm~\ref{alg:spectral}) with tuning parameter $\mu \geq 32 C^2 \beta$ and trim threshold $\tau = \bar{d}$ satisfies, under the assumption
$$256 \mu \beta C^2 K^3 \frac{(p \vee q)}{n (p-q)^2} \leq 1,$$ 
the inequality
\[
l(\sigma, \sigma_0) \leq 64 C^2 \beta  \frac{K^2 (p \vee q) }{n (p-q)^2},
\]
with probability at least $1 - n^{-C'}$ for a $C' > 4$.
\end{proposition}

\begin{proof}
Note that the trimming parameter $\tau$ is a random variable, since $\bar d$ is random. Since the community sizes are bounded by $n/K\beta$, it is easy to see that we can find constants $C_{d_1} < C_{d_2}= 1$ which depend only on $K$ and $\beta$, such that 
$$C_{d_1}n (p \vee q) \leq  \E \bar d \leq C_{d_2} n(p \vee q).$$ 
Using the multiplicative form of Chernoff's bound, we conclude that with probability at least $1 - \exp(-nC_{\bar d})$ for some constant $C_{\bar d}$, we must have
$$\frac{C_{d_1}}{2} n (p \vee q) \leq  \bar d \leq 2C_{d_2} n(p \vee q).$$ 
We now use state and use the following lemma:
\begin{lemma} (Lemma 5 of \cite{gao2015achieving})
\label{lem:trimmed_A_bound}
Let $P \in [0,1]^{n \times n}$ be a symmetric matrix. Let $A$ be an adjacency matrix such that $A_{uu} = 0$, $A_{uv} \sim Ber(P_{uv})$ for the lower triangular part $u < v$. For any $C' > 0$ and $0 < C_1 < C_2$ there exists some $C > 0$ such that
\[
\| T_\tau(A) - P \|_2 \leq C \sqrt{ n p_{max} + 1}
\]
with probability at least $1-n^{-C'}$, uniformly over $\tau \in [C_1 (np_{max} + 1), C_2 (np_{max} + 1)]$ where $p_{max} = \max_{u \geq v} P_{uv}$. 
\end{lemma}
\remark{Lemma 5 from \cite{gao2015achieving} is stated slightly differently ---  for any $C' >0$ there exist constants $c, C_1$ and $C_2$ such that the result holds with probability $1-n^{-C'}$. However, our restatement follows immediately by examining the proof of Lemma 5 in \cite{gao2015achieving}.}

Using the above with a fixed $C' > 4$, and $C_1 = \frac{C_{d_1}}{2}$ and $C_2 = 2C_{d_2}$ and conclude that there exists a constant $C$ such that the inequality 
\begin{equation*}
\| T_{\tau}(A) - P \|_2 \leq C \sqrt{ n (p \vee q) }
\end{equation*}
holds for any choice of $\tau \in [C_1, C_2]$ with a probability of $1 - n^{-C'}$. In particular, this inequality will also hold for $\tau = \bar d$ with probability $1-n^{-C'}$. (Note that we may replace $\sqrt{n(p \vee q) + 1}$ by $\sqrt{n(p \vee q)}$, since we assume that $(p \vee q) > c/n$.) We assume that $C \geq 1$, since the inequality in Lemma~\ref{lem:trimmed_A_bound} holds with $C$ replaced by $\max(1,C).$ 
Thus, we have that,

\begin{align*}
\| \hat{A} - P \|_2 &\leq \| T_\tau(A)  - P \|_2 + \| \hat{A} - T_\tau(A) \|_2 \\
   &\stackrel{(a)}\leq 2\| T_\tau(A) - P \|_2 \\
  &\leq 2C \sqrt{ n (p \vee q)}, 
\end{align*}
where $(a)$ follows because $\hat{A}$ is the best rank-$K$ approximation of $T_\tau (A)$ and $\rank(P) = K$, so $\|T_\tau(A) - \hat{A}\|_2 \le \|T_\tau(A) - P\|_2$ by the Eckart-Young-Mirsky Theorem. Thus, we have that
\begin{align*}
\sum_{u=1}^n \| \hat{A}_u - P_u \|_2^2 &= \| \hat{A} - P \|_F^2\\
&\leq K\| \hat A - P \|_2^2\\
&\leq 4 K C^2 n (p \vee q).
\end{align*}
For simplicity of notation, denote the $K$ possible distinct rows of $P$ by ${\cal Z}_i$, for $1 \leq i \leq K$. For a vertex $u$, denote the row $P_u$ by ${\cal Z}(u).$ Note that for $i \neq j$, we have the inequality
\[
\| {\cal Z}_i - {\cal Z}_j \|_2^2 \geq \frac{2}{\beta K} (p - q)^2 n,
\]
obtained by considering the lower bound $n/\beta K$ on the cluster sizes. If the ${\cal Z}_i$'s are known, then we can cluster $v$ by matching $\hat{A}_v$ to the closest ${\cal Z}_i$. We would make a mistake only if $\| \hat{A}_u - P_u \|_2^2 = \| \hat{A}_u - {\cal Z}(u) \|_2^2 \geq \frac{1}{\beta K} (p-q)^2 n$. Thus, the number of mistakes we make cannot be larger than

\[
\frac{\sum_{u=1}^n \| \hat{A}_u - P_u \|_2^2}{ \frac{1}{\beta K} (p-q)^2 n } \leq \frac{ 4 K C^2 n (p \vee q)}{ \frac{1}{\beta K} (p-q)^2 n} \leq \frac{4 \beta K^2 C^2 (p \vee q)}{(p-q)^2}. 
\]
But because we do not know the true ${\cal Z}_i$'s, we will construct a set $S$ as a surrogate. This set shall have $K$ points, such that every ${\cal Z}_i$ has a point in $S$ which is close to ${\cal Z}_i$ and acts as a surrogate for ${\cal Z}_i$.

Define a point $u$ as valid if $\| \hat{A}_u - {\cal Z}_i \|_2^2 \leq \frac{1}{16} \frac{1}{\beta K} (p-q)^2 n$ for some ${\cal Z}_i$, not necessarily ${\cal Z}(u)$. A point $u$ is declared invalid if the condition is not fulfilled. For a node $u$, define ${\cal Z}^*(u) = \argmin_{{\cal Z}_i} \| \hat{A}_u - {\cal Z}_i \|_2^2$, so ${\cal Z}^*(u)$ is the row of the $P$ matrix closest to $\hat{A}_u$. Notice that if $u$ is valid, then $\| \hat{A}_u - {\cal Z}^*(u) \|_2^2 \leq \frac{1}{16} \frac{1}{\beta K} (p - q)^2 n$. We break up our goal of showing that every ${\cal Z}_i$ has a surrogate element in $S$ into the following two claims:\\

\noindent \textbf{Claim 1:} $S$ contains only valid points. \\
\textbf{Claim 2:} For every pair of distinct nodes $u,v \in S$, we have ${\cal Z}^*(u) \neq {\cal Z}^*(v)$. \\

First, let us suppose that these two claims are true and see that the proposition follows. Denote the rows of $\hat A$ corresponding to the members of the set $S$ by ${\cal S}_i$, where ${\cal S}_i$ is the surrogate for ${\cal  Z}_i$. Define ${\cal S}(u)$ to be the surrogate of ${\cal Z}(u)$, and denote ${\cal S}^*(u) =\arg\min_{{\cal S}_i} \| \hat A_u - {\cal S}_i \|^2$, that is, the member of $S$ that is closest to $\hat A_u$. We say that a point $u$ is misclassified if ${\cal S}^*(u) \neq {\cal S}(u)$. The number of mistakes we make is bounded by the number of invalid points plus the number of misclassified valid points. Note that if $u$ is invalid, we have $\|\hat{A}_u - P_u\|_2^2 \ge \frac{1}{16} \frac{1}{\beta K} (p-q)^2n$. We claim that the same inequality holds for misclassified any valid point $u$. 


Since $u$ is a valid point, we know that there is a ${\cal Z}_i$ that is close to $\hat A_u$:
$$ \| \hat A_u - {\cal Z}_i \|^2 \leq \frac{1}{16}\frac{1}{\beta K} (p-q)^2 n.$$
We claim that ${\cal S}^*(u) = {\cal S}_i$, that is, the point $S$ that is closest to $\hat A_u$ is the surrogate of the row of $P$ that is close to $\hat A_u$. For any $j \neq i$, we have
\begin{align*}
\| \hat A_u - {\cal S}_j \| &\geq \|Z_i - Z_j \| - \|{\cal Z}_j - {\cal S}_j \| - \|{\cal Z}_i - \hat A_u\|\\
&\geq \sqrt{\frac{2}{\beta K} (p-q)^2 n} - 2\sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n}\\
&> 2\sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n}.
\end{align*}
Furthermore, we have
\begin{align*}
\| \hat A_u - {\cal S}_i \| &\leq \| \hat A_u - {\cal Z}_i \| + \| {\cal S}_i - {\cal Z}_i \|\\
&\leq 2 \sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n}.
\end{align*}
Thus, for any $j \neq i$
\begin{align*}
\| \hat A_u - {\cal S}_j \|  > \| \hat A_u - {\cal S}_i \|,
\end{align*}
and we must have ${\cal S}^*(u) = {\cal S}_i.$

Since $u$ has been misclassified, we have that ${\cal S}(u) \neq {\cal S}^*(u) = {\cal S}_i$. Let ${\cal S}(u) = {\cal S}_j$ and ${\cal Z}(u) = {\cal Z}_j$. We have the sequence of inequalities:
\begin{align*}
\|\hat A_u - {\cal Z}(u)\| &= \| \hat A_u - {\cal Z}_j \|\\
&\geq \| \hat A_u - {\cal S}_j \| - \| {\cal S}_j - {\cal Z}_j \|\\
&\geq 2\sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n} - \sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n}\\
&= \sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n},
\end{align*}
which is the bound we wanted to prove. Thus, the number of mistakes is bounded by
\[
\frac{\sum_{u=1}^n \| \hat{A}_u - P_u \|_2^2}{ \frac{1}{16 \beta K} (p-q)^2 n } \leq 
        \frac{ 4 K C^2 n (p \vee q)}{ \frac{1}{16 \beta K} (p-q)^2 n} \leq \frac{64 \beta K^2 C^2 (p \vee q)}{(p-q)^2},
\]
as wanted.

\paragraph{\textbf{Proof of Claim 1:}} Recall that given a point $u$, the neighbors of $u$ are $N(u) = \{ v \,:\, \| \hat{A}_u - \hat{A}_v \|_2^2 \leq \mu K^2 \frac{\bar{d}}{n} \}$. Furthermore, by Chernoff's inequality $\bar{d} \leq 2 (p \vee q) n$ with probability $1 - \exp(-C_{\bar d}n)$. 
We condition on this event so that if $v \in N(u)$, then $\| \hat{A}_u - \hat{A}_v \|^2 \leq 2 \mu K^2 (p \vee q)$. We prove the claim by showing that an invalid point $u$ cannot have $\frac{1}{\mu} \frac{n}{K}$ neighbors. We have that by definition of invalidity, $ \| \hat{A}_u - {\cal Z}_i \|_2^2 \geq \frac{1}{16 \beta K} (p-q)^2 n$ for any ${\cal Z}_i$. Let $v$ be a neighbor of $u$. By triangle inequality, we have
\begin{align*}
\| \hat A_v - {\cal Z}(v) \| &\geq  \|\hat A_u - {\cal Z}(v) \| -  \|\hat A_u - \hat A_v \| \\
&\geq \sqrt{\frac{1}{16 \beta K} (p-q)^2 n} - \sqrt{2 \mu K^2 (p \vee q)}\\
&\stackrel{(a)}\geq  \sqrt{\frac{1}{16 \beta K} (p-q)^2 n} - \sqrt{\frac{1}{64 \beta K} (p-q)^2 n} = \sqrt{\frac{1}{64 \beta K} (p-q)^2 n},
\end{align*}
where $(a)$ follows from our assumption coupled with the choice of $C \geq 1$, which essentially states that
$$2 \mu K^2 (p \vee q) \leq \frac{1}{128 \beta K} (p-q)^2 n < \frac{1}{64 \beta K} (p-q)^2 n.$$
Thus, for every neighbor $v$ of $u$, we must have $\| \hat{A}_v - P_v \|_2^2 \geq \frac{1}{64 \beta K} (p-q)^2 n$. The number of neighbors of $u$ may be bounded by 

\[
\frac{\sum_{v=1}^n \| \hat{A}_v - P_v \|_2^2}{ \frac{1}{64 \beta K} (p-q)^2 n } \leq 
        \frac{ 4 K C^2 n (p \vee q)}{ \frac{1}{64 \beta K} (p-q)^2 n} \leq \frac{256 \beta K^2 C^2 (p \vee q)}{(p-q)^2}. 
\]
This quantity is less than $\frac{1}{\mu} \frac{n}{K}$ by assumption.

\paragraph{\textbf{Proof of Claim 2:}} We first claim that in every cluster, at least half the points $u$ satisfy $\| \hat{A}_u - P_u \|_2^2 \leq \frac{1}{4} \mu K^2 (p \vee q)$. This is because the total error is bounded by $\sum_{u=1}^n \| \hat{A}_u - P_u \|_2^2 \leq 4 K C^2 n (p \vee q)$ and thus, the total number of points that violate the condition is at most $\frac{ 4 K C^2 n (p \vee q)}{ \frac{1}{4} \mu K^2 (p \vee q)} \leq \frac{n}{2 \beta K}$ by the assumption that $\mu \geq 32 C^2 \beta$. 

For two points $u$ and $v$ in the same cluster satisfying $\| \hat{A}_w - P_w \|_2^2 \leq  \frac{1}{4} \mu K^2 (p \vee q)$ for $w \in \{u,v\}$, we also have $\| \hat{A}_u - \hat{A}_v \|_2^2 \leq \mu K^2 (p \vee q)$ by triangle inequality. Thus, in every cluster, there exists a point $u$ such that $N(u) \geq \frac{n}{2\beta K} \geq \frac{1}{\mu} \frac{n}{K}$, since $\mu \geq 32C^2\beta > 2\beta$ by our choice of $C > 1$.

Suppose at iteration $r$ the set $S$ consists of points $s_1, \dots, s_r$ where $1 \leq r < K$, and suppose for contradiction that $s_{r+1}$ is such that ${\cal Z}(s_{r+1}) = {\cal Z}(s_i)$ for some $1 \leq i \leq r$. Since $s_i$ and $s_{r+1}$ are both valid points, we have by triangle inequality
$$\| \hat A_{s_{r+1}} - \hat A_{s_i} \| \leq \frac{1}{4 \beta K} (p - q)^2 n.$$

On the other hand, because $S$ does not yet have $K$ nodes, there must be some ${\cal Z}_j$ which does not have a surrogate in $S$. The cluster that corresponds to ${\cal Z}_j$ must, by our neighborhood size analysis, 
contain a node $u$ such that $N(u) \geq \frac{1}{\mu} \frac{n}{K}$ and that
\[
\| \hat{A}_u - {\cal Z}_j \|_2^2 \leq \frac{1}{4} \mu K^2 (p \vee q)  \leq \frac{1}{16} \frac{1}{\beta K} (p-q)^2 n,
\]
where the second inequality follows from the assumption of the proposition statement.

Since ${\cal Z}_j \neq {\cal Z}(s_i)$ for any $1 \leq i \leq r$, we have $\| {\cal Z}_j - {\cal Z}(s_i) \|_2^2 \geq 2 \frac{1}{\beta K} (p-q)^2 n$ for all $1 \leq i \leq r$. By claim 1, we have that all $s_i$'s are valid, and thus
$$\| \hat{A}_{s_i} - {\cal Z}(s_i) \| \leq \frac{1}{16} \frac{1}{\beta K} (p - q)^2 n.$$ 
So we have that, by triangle inequality, that $\| \hat{A}_u - \hat{A}_{s_i} \|_2^2 \geq \frac{1}{\beta K} (p - q)^2 n$ for all $s_i \in S$. This is a contradiction because $u$ is farther away from every point in $S$ than $s_{r+1}$, and thus our assumption that ${\cal Z}(s_{r+1}) =  {\cal Z}(s_i)$ must be invalid.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%

\subsection{Analysis of the Initialization Scheme}
\label{appendix: init}
\begin{proposition}
\label{prop:initial_guarantee}
Suppose that $\frac{1}{\rho_L} \leq \frac{P_l}{Q_l} \leq \rho_L$ for all colors $l$. Let $\sigma^l$ be a spectral clustering of the graph based on $\tilde{A}_{ij} = \mathbf{1}(A_{ij} = l)$ and let $\hat{P}_l, \hat{Q}_l$ be estimates of $P_l, Q_l$ constructed from $\sigma^l$. Then, there is a positive constant $C_{test}$ such that, with probability at least $1 - Ln^{-3 + \delta_p}$, for a small $\delta_p > 0$, we have

\begin{enumerate} 
\item For all colors $l$ satisfying $P_l \vee Q_l > c/n$ and $\Delta_l \geq \sqrt{C_{test}} \sqrt{ \frac{P_l \vee Q_l}{n}},$ the following holds: 
\begin{align}
C_1 \frac{ | P_l - Q_l |}{\sqrt{P_l \vee Q_l}}  \leq \frac{ | \hat{P}_l - \hat{Q}_l| }{\sqrt{ \hat{P}_l \vee \hat{Q}_l }} \leq  
C_2 \frac{ | P_l - Q_l | }{\sqrt{ P_l \vee Q_l}},
\end{align}
for some absolute constants $C_1$ and $C_2$.

\item For all colors satisfying $P_l \vee Q_l > c/n$ and $\Delta_l <  \sqrt{C_{test}} \sqrt{ \frac{P_l \vee Q_l}{n} }$, the following holds:

\begin{align}
\frac{ | \hat{P}_l - \hat{Q}_l|}{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} \leq C \sqrt{ \frac{1}{n} } 
\label{eqn:bad_l_initialization}
\end{align}
for a suitable absolute constant $C$.
\end{enumerate}
\end{proposition}

\begin{proof}
Recall from Proposition~\ref{prop:estimation_consistency} that given a clustering with error-rate $\gamma$, and under the assumptions  $P_l \vee Q_l > \frac{c}{n}$ and $\Delta_l^2 \geq \frac{P_l \vee Q_l}{n}$ we have that the estimated probabilities $\hat P_l$ and $\hat Q_l$ satisfy 
\begin{align*}
|\hat P_l - \hat P| &\leq \eta \Delta_l \quad \text{ and }\\
|\hat Q_l - \hat Q| &\leq \eta \Delta_l,
\end{align*}
with probability $1-n^{-(3+\delta_p)}$, where $\eta$ is as in Proposition \ref{prop:estimation_consistency}. We first pick a value of $\gamma$ such that $\eta < \frac{1}{4}$. Notice that we can do so because as $\eta \to 0$ as $\gamma \to 0$. We will now ensure that the error-rate $\gamma$ as obtained from Proposition~\ref{prop:spectral_analysis} matches our current choice of $\gamma$. Recall that Proposition \ref{prop:spectral_analysis} states that under the assumptions $P_l \vee Q_l > \frac{c}{n}$ and 
$$ C_1 \frac{P_l \vee Q_l}{n(P_l - Q_l)^2} \leq 1$$
for some constant $C_1$, we have 
$$l(\sigma, \sigma_0) \leq C_2 \frac{P_l \vee Q_l}{n(P_l - Q_l)^2}$$
for some constant $C_2$. More details concerning the constants $C_1$ and $C_2$ may be found in the statement of Proposition~\ref{prop:spectral_analysis}. For the purpose of this proof, it is enough to note that if $C_{test}$ is chosen large enough, then the following can be made to hold:
\begin{align*}
&C_1 \frac{P_l \vee Q_l}{n(P_l - Q_l)^2} \leq \frac{C_1}{C_{test}} < 1, \quad \text{ and}\\
l(\sigma, \sigma_0) \leq ~&C_2 \frac{P_l \vee Q_l}{n(P_l - Q_l)^2} \leq \frac{C_2}{C_{test}} < \gamma
\end{align*}
for all colors $l$ satisfying $\Delta_l \geq \sqrt{C_{test}} \sqrt{\frac{P_l \vee Q_l}{n}}$. We pick $C_{test}$ to be the larger of $1$ and this value, so that the results from Proposition \ref{prop:estimation_consistency} may also be applied for all colors satisfying the bound $\Delta_l \geq \sqrt{C_{test}} \sqrt{\frac{P_l \vee Q_l}{n}}$. In the rest of the proof, we shall use the bounds from Proposition~\ref{prop:estimation_consistency} and the fact that $|\eta| < 1/4.$ To bound $\frac{ | \hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}}$, we bound the numerator and denominator separately. First, we bound the numerator:
\begin{align*}
| \hat{P}_l - \hat{Q}_l | &\leq |\hat{P}_l - P_l| + |P_l - Q_l| + |\hat{Q}_l - Q_l|   \\
   &\leq 2 |\eta| \Delta_l + \Delta_l \\
   &\leq \frac{3}{2} \Delta.
\end{align*}
Furthermore,
\begin{align*}
| \hat{P}_l - \hat{Q}_l | &\geq  |P_l - Q_l| - |\hat{Q}_l - Q_l| - |\hat{P}_l - P_l|   \\
   &\geq  \Delta_l - 2 |\eta| \Delta_l \\
  &\geq \frac{1}{2} \Delta_l.
\end{align*}
Next, we bound the denominator:
\begin{align*}
\hat{P}_l &\leq (P_l \vee Q_l) + |\eta| \Delta_l\\
   &\leq (P_l \vee Q_l) + |\eta| (P_l \vee Q_l) \\
   &\leq \frac{5}{4} (P_l \vee Q_l).
\end{align*}
Similar reasoning applies to give an upper bound on $\hat{Q}_l$. For the lower bound on the denominator, we first observe that $\hat{P}_l \geq P_l - |\eta| \Delta_l $ and that 
$\hat{Q}_l \geq Q_l - |\eta| \Delta_l$. Let us suppose without loss of generality that $P_l \geq Q_l$. Then, we have that 
\[
\hat{P}_l \vee \hat{Q}_l \geq (P_l \vee Q_l) - |\eta| \Delta_l \geq \frac{3}{4} (P_l \vee Q_l)
\]
Therefore, we have that
\[
\frac{1}{\sqrt{5}} \frac{\Delta_l}{P_l \vee Q_l} \leq \frac{ | \hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} \leq \frac{3}{\sqrt{3}} \frac{\Delta_l}{P_l \vee Q_l}.
\]
This completes the proof for all colors satisfying $\Delta_l^2 \geq C_{test} \frac{P_l \vee Q_l}{n}$. 

Now we move on to the second claim where we suppose $\Delta_l^2 \leq C_{test} \frac{P_l \vee Q_l}{n}$. Note that this does not necessarily imply  $\Delta_l^2 \leq  \frac{P_l \vee Q_l}{n}$, since $C_{test} \geq 1$. However, we may still take the maximum of the bounds provided in Proposition~\ref{prop:estimation_consistency}, to conclude that with probability $1 - L n^{-(3+\delta_p)}$,
\begin{align*}
|\hat{P}_l - P_l| \leq \eta \left( \Delta_l \vee \sqrt{\frac{P_l \vee Q_l}{n} } \right),
\end{align*}
where $|\eta| \leq 1/4$. The condition $\Delta_l^2 \leq C_{test} \frac{P_l \vee Q_l}{n}$ implies $\sqrt{C_{test}}\sqrt{ \frac{P_l \vee Q_l}{n}} \geq \Delta_l$, which may be substituted in the bound above to obtain
\[
| \hat{P}_l - P_l | \leq \frac{\sqrt{C_{test}}}{4}\sqrt{ \frac{P_l \vee Q_l}{n} }, 
\]
where we used the fact that $C_{test} \geq 1.$ A similar bound also holds for $|\hat{Q}_l - Q_l|$ using the same reasoning. Putting these together, we have that
\begin{align*}
|\hat{P}_l - \hat{Q}_l| &= | \hat{P}_l - P_l + P_l - Q_l + Q_l - \hat{Q}_l| \\
    &\leq \Delta_l + | \hat{P}_l - P_l| + |\hat{Q}_l - Q_l| \\
    &\leq \Delta_l + \frac{\sqrt{C_{test}}}{2} \sqrt{ \frac{P_l \vee Q_l}{n} } \\
    &\leq \frac{3}{2} \sqrt{C_{test}} \sqrt{ \frac{P_l \vee Q_l}{n} }.
\end{align*}
To bound the denominator term $\sqrt{ \hat{P}_l \vee \hat{Q}_l}$, we have that $\hat{P}_l \geq P_l - \frac{\sqrt{C_{test}}}{4}\sqrt{ \frac{P_l \vee Q_l}{n} }$ and $\hat{Q}_l \geq Q_l - \frac{\sqrt{C_{test}}}{4} \sqrt{ \frac{P_l \vee Q_l}{n}}$. Suppose without loss of generality that $P_l \geq Q_l$ so that $P_l = (P_l \vee Q_l)$. Then we have
\begin{align*}
\hat{P}_l \vee \hat{Q}_l &\geq (P_l \vee Q_l) - \frac{\sqrt{C_{test}}}{4} \sqrt{ \frac{P_l \vee Q_l}{n}} \geq C P_l \vee Q_l
\end{align*} 
for some constant $C$, where we used the assumption that $P_l \vee Q_l \geq \frac{c}{n}$. Thus, we have that
\begin{align*}
\frac{| \hat{P}_l - \hat{Q}_l| }{\sqrt{ \hat{P}_l \vee \hat{Q}_l} } \leq C \sqrt{ \frac{1}{n} }
\end{align*}
for an appropriate constant $C$ that depends on $C_{test}$ and $c$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\subsection{Choosing label $l^*$}
\label{appendix: starry night}

\begin{proposition}
\label{prop:initialization_correctness}
Let $l^*$ be the color chosen by the initialization algorithm. Let $a_n = \frac{ n I_L}{L \rho^4_L} $  and assume that $a_n \rightarrow \infty$. For large enough $n$, with probability at least $1 - 2L n^{-(3+\delta_p)}$, we have that $\frac{n (P_{l^*}-Q_{l^*})^2}{(P_{l^*} \vee Q_{l^*}) \rho^4_L} \ge C \cdot a_n$ for some constant $C$. 
\end{proposition}

\begin{proof}
Let $C_{test}$ be the constant in Proposition~\ref{prop:initial_guarantee}. As shown in the proof of  Lemma~\ref{lem:friday_night}, we have that  $I_L$ is of the same order as $\sum_{l=1}^L \frac{\Delta_l^2}{P_l \vee Q_l}$, and therefore, there must exist some color $l_n$ (we leave the dependency on $n$ implicit and denote it just by $l$) such that $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq  C \frac{n I_L}{L} = C a_n \rho^4_L$ for some constant $C$. The same color $l$ must also satisfy $\Delta_l \geq C_{test} \sqrt{\frac{P_l \vee Q_l}{n}}$. Suppose that the probability event of Proposition~\ref{prop:initial_guarantee} holds, which happens with probability at least $1 - L n^{-(3+\delta)}$. 

\textbf{Step 1.} We claim that $l^*$ satisfies $\Delta_{l^*} \geq C_{test} \sqrt{ \frac{P_l \vee Q_l}{n}} $. Let $l$ be a color such that $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq C a_n \rho^4_L$ and suppose $l^*$ does not satisfy $\Delta_{l^*} \geq C_{test} \sqrt{ \frac{P_l \vee Q_l}{n} }$. Then, we have that, by Proposition~\ref{prop:initial_guarantee}, 
\begin{align*}
\frac{| \hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} 
\stackrel{(a)}  \leq \frac{| \hat{P}_{l^*} - \hat{Q}_{l^*} | }{\sqrt{ \hat{P}_{l^*} \vee \hat{Q}_{l^*}}} 
         \leq C' \sqrt{ \frac{1}{n}},
\end{align*}
where $(a)$ follows from the definition of $l^*$.
But, because $l$ satisfies $\Delta_l \geq C_{test} \sqrt{ \frac{P_l \vee Q_l}{n}}$, we also have
\begin{align*}
\frac{| \hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} 
  \geq \frac{1}{\sqrt{5}} \frac{ | P_l - Q_l|}{\sqrt{P_l \vee Q_l}}, 
   \geq C'' \sqrt{ a_n \rho^4_L \frac{1}{n}}
\end{align*}
Since $a_n \rightarrow \infty$ and $\rho_L \geq 1$, we contradict the $C'\frac{1}{\sqrt n}$ upper bound derived earlier.

\textbf{Step 2:} Again, let $l$ be a color such that $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq C a_n \rho^4_L$. By Proposition \ref{prop:initial_guarantee} and the definition of $l^*$, we obtain
\begin{align*}
\frac{ |P_{l^*} - Q_{l^*}|}{\sqrt{ P_{l^*} \vee Q_{l^*}}} &\geq 
\frac{\sqrt{3}}{3} \frac{|\hat{P}_{l^*} - \hat{Q}_{l^*} | }{\sqrt{ \hat{P}_{l^*} \vee \hat{Q}_{l^*} }} 
  \\
& \geq
\frac{\sqrt{3}}{3} \frac{|\hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}}  \\
 &\geq \frac{|P_l - Q_l|}{\sqrt{P_l \vee Q_l}} \frac{1}{\sqrt{15}}  \\
 &\geq C' \sqrt{a_n \rho^4_L \frac{1}{n} },
\end{align*}
for an appropriate constant $C'$. The conclusion thus follows.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%

\subsection{Analysis of probability of error for a single node}
\label{appendix: single node}

\begin{proposition}
\label{prop:single_node_error_bound}
Let node $u$ be arbitrarily fixed, let $\tilde \sigma_u$ be the output of Algorithm \ref{alg:initialization1}, and let $\pi_u \in S_K$ such that
$$l(\sigma_0, \tilde \sigma_u) = d(\sigma_0, \pi_u(\tilde \sigma_u)),$$
where both $l$ and $d$ are taken with respect to the set $\{1, 2, \dots, n\} \setminus \{u\}$. We assume that the conditions of Proposition \ref{prop:labeled_sbm_rate} hold. Conditioning on the event that the error rate $\gamma$ of $\tilde \sigma_u$ satisfies $\rho_L^4 \gamma \to 0$, and also on the event that the result of Proposition~\ref{prop:estimation_consistency} holds with a sequence $\eta$ that satisfies $\eta \rho_L^2 \rightarrow 0$, we have that, with probability at least $1 - (K-1)\exp \left( - (1 - o(1)) \frac{n}{\beta K} I_L \right)$, the following event holds:
\[
\pi_u^{-1}(\sigma_0(u)) = \argmax_k \sum_{v\,: \tilde \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
\]
\end{proposition}

\begin{proof}
Throughout the proof, we assume $n$ is large enough so that $\frac{1}{2} \sum_l (\sqrt{P_l} - \sqrt{Q_l})^2 \leq \frac{1}{2}$. Suppose without loss of generality that $\sigma_0(u) = 1$.  We misclassify $u$ in community $k$ if 
\begin{align}
\sum_{v\,:\, \tilde \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
&\geq 
 \sum_{v\,:\, \tilde \sigma_u(v)=1} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l).
\end{align}
This can be restated as
\begin{align}
\sum_{v \,:\, \tilde \sigma_u(v) = k} \bar{A}_{uv} - \sum_{v\,:\, \tilde \sigma_u(v) = 1} \bar{A}_{uv}
\label{eqn:bad_event1}
&\geq 0, 
\end{align}
where $\bar{A}_{uv} \equiv \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l)$. Note that the edges from $u$ are independent of the clustering $\tilde \sigma_u$, since this clustering was obtained by running the algorithm with vertex $u$ excluded. 

Define $m_1 = |\{ v \,:\, \tilde \sigma_u(v) = 1 \}|$ and $m_k = | \{ v \,:\, \tilde \sigma_u(v) = k \}|$ as the size of clusters $m_1, m_k$ under $\sigma_u$. Define  $m_1' = \{ v \,:\, \tilde \sigma_u(v) = 1 ,\, \sigma_0(v) = 1\}$ as the points correctly clustered by $\sigma_u$, and $m_k' = \{ v \,:\, \tilde \sigma_u(v) \neq  1,\, \sigma_0(v) = k \}$ as a loose definition of points correctly classified by $\tilde \sigma_u$ in community $k$. With these definitions, the probability of the bad event Equation~\ref{eqn:bad_event1} is the probability of the event
\begin{align*}
\left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m'_k} \tilde{X}_i + \right) - 
\left( \sum_{i=1}^{m_1'} \tilde{X}_i + \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) &\geq 0,
\end{align*}
where $\tilde{X}_i = \log \frac{\hat{P}_l}{\hat{Q}_l}$ with probability $P_l$ and $\tilde{Y}_i = \log \frac{\hat{P}_l}{\hat{Q}_l}$ with probability $Q_l$. (For simplicity, we abuse notation here by using the notation $\tilde Y_i$ and $\tilde X_i$ in both bracketed terms. These random variables are not the same, of course, but are independent and identical copies.) This is the same as the probability of the event
\begin{align*}
\exp \left( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i - 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) \right) &\geq 1 
\end{align*}
We bound the probability of this event as follows:
\begin{align*}
& P \left( \exp \left( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i- 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) \right) \geq 1 \right) \\ 
     %
&\leq \E \left[ 
\exp\left( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i - 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) \right)
 \right] \\ 
 %
&=  \E[\exp( t \tilde{Y}_i) ]^{m_k'} 
     \E[ \exp(t \tilde{X}_i ]) ^{m_k - m_k'}  
    \E[ \exp( - t \tilde{X}_i)] ^{m_1'} 
    \E[ \exp( -t \tilde{Y}_i )] ^{m_1 - m_1'} \\
    %
&= \left( \sum_l e^{t \log \frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k'}  
      \left( \sum_l e^{t \log \frac{\hat{P}_l}{\hat{Q}_l}} P_l \right)^{m_k - m_k'} 
      \left( \sum_l e^{- t \log \frac{\hat{P}_l}{\hat{Q_l}} } P_l \right)^{m_1'}
     \left( \sum_l e^{-t \log \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{m_1 - m_1'}.
\end{align*}
We will set $t = \frac{1}{2}$, in which case, we have:
\begin{align}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k'}
 \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l \right)^{m_k - m_k'}
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l \right)^{m_1 - m_1'}
       \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1'} \nonumber \\
=&  \left( \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right)^{m_k - m_k'}
 \left( \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right)^{m_1 - m_1'}  
   \label{eqn:excess_error_term} \\
 & \left( \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{m_k} 
    \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1}.
   \label{eqn:Ihat_error_term} 
\end{align} 
We will bound term~\ref{eqn:excess_error_term} and \ref{eqn:Ihat_error_term} separately. Loosely speaking, we will show that term~\ref{eqn:excess_error_term} is bounded in magnitude by $\exp( o(I_L) \frac{n}{K} )$ and that term~\ref{eqn:Ihat_error_term} is bounded by $\exp( - \frac{n}{\beta K} (1 + o(1) )I_L)$.\\ 

\noindent \textbf{Bound for Term~\ref{eqn:excess_error_term}.} 
Establishing this bound requires a number of lemmas establishing bounds on the intermediate terms that appear in the computation. In particular, we use the bounds from Lemma~\ref{lem:sqrt_ratio_times_ql}, Lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1}, and Lemma \ref{lem:friday_night} in the following sequence of inequalities:
\begin{align*}
\left| 1 -  \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right|
 &= \left| \frac{ \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} (P_l - Q_l) }
     { \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l } \right| \\
% 1
&\stackrel{(a)}\leq \frac{8}{\sum_l \sqrt{P_l Q_l}} 
     \left| \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} }(P_l - Q_l) \right| \\
% 2
&\stackrel{(b)}\leq 16 \left|  \sum_{l} \left( \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right) (P_l - Q_l)  \right| \\
% 3
&\leq 16 \left| 
     \sum_{l \in L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} - 1 \right)(P_l - Q_l) 
     \right| + 16 \sum_{l \notin L_1} \left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} - 1 \right| |P_l - Q_l| \\
% 4
&\stackrel{(c)}\leq 16 \sum_{l \in L_1} \frac{\Delta^2_l}{Q_l}(1+ \eta') + 
      \sum_{i \notin L_1} 32 \rho_L \frac{\Delta_l}{\sqrt{n (P_l \vee Q_l)}} \\
%
&\leq 16 \rho_L \sum_{l \in L_1} \frac{\Delta^2_l}{P_l \vee Q_l} (1+ \eta') + 
      \sum_{i \notin L_1} 32 \rho_L \frac{\Delta_l}{\sqrt{n (P_l \vee Q_l)}} \\
% 5
&\stackrel{(d)}\leq C I_L  \rho_L(1 + \eta') + C' \rho_L \frac{L}{n} \\
% 6
&\stackrel{(e)} \leq C \rho_L I_L. 
\end{align*}
In $(a)$, we used the bound Lemma~\ref{lem:sqrt_ratio_times_ql}, which states that
$$\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \geq \frac{1}{8} \sum_l P_lQ_l.$$ 
In $(b)$, we the fact that $\sum_l  \sqrt{P_l Q_l}  \to 1$, and thus for large enough $n$ it will exceed $1/2$. In $(c)$, we employ Lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1}, which provides the appropriate bounds for the term $\left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} - 1 \right)$. Here $\eta'$ is a $o(1)$ sequence. Inequality $(d)$ follows from Lemma~\ref{lem:friday_night}, which shows that the first term in the previous step is $\Theta(I_L)$, and from the definition of the set $L_1^c$. Finally, inequality $(e)$ follows from the assumption that $\frac{I_L n}{L \rho_L^4} \rightarrow \infty$ (note that $\rho_L \geq 1$) and by appropriately redefining $\eta'$. Identical analysis shows that
\[
\left| 1 - \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right| 
\leq C\rho_LI_L. 
\]
Now, we note that $ |x| \leq \exp( | 1 - x | )$. Therefore, term~\ref{eqn:excess_error_term} can be bounded as
\begin{align*}
  \left( \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right)^{m_k - m_k'}
 \left( \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right)^{m_1 - m_1'}  &\leq \exp( C \rho_L I_L (m_k - m_k' + m_1 - m_1') ) \\
&\leq \exp( C I_L \rho_L \gamma n).
\end{align*}
Since $\gamma \rho_L = o(1)$,  we conclude that term~\ref{eqn:excess_error_term} is bounded by $\exp \left( \frac{n}{K}o(I_L)\right)$, as desired.\\
 
\noindent \textbf{Bound for Term~\ref{eqn:Ihat_error_term}.} Define 
$\hat{I} = - \log \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right) \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right) $. 
With this definition, 
\begin{align*}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k} 
       \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1} 
&= \exp( - \hat{I} )^{\frac{m_k + m_1}{2}}  \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}}.
\end{align*}
We claim that the following three statements are true. 
\begin{enumerate}
\item $ m_1, m_k \geq \frac{n}{\beta K} (1 -\beta K \gamma)$
\item $\hat{I} \geq I_L(1 + o(1))$, where the notation $1+o(1)$ stands for $1+\eta'$ for an $o(1)$ sequence $\eta'$ 
\item $\left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} = \exp\left(\frac{n}{K}  o(I_L)\right) $
\end{enumerate}
Let us first suppose that these statements are true and see that term~\ref{eqn:Ihat_error_term} can be bounded. 
\begin{align*}
\exp( - \hat{I} )^{\frac{m_1 + m_k}{2}}  \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}}  &\leq  \exp \left( - I_L(1+o(1)) \frac{n}{\beta K} \cdot (1 -\beta K \gamma) + \frac{n}{K} o(I_L)\right)  \\
&\leq \exp \left( - (1+o(1)) \frac{n}{\beta K} I_L  \right), 
\end{align*}
where last inequality holds because $\gamma = o(1)$. We will prove each of the three claims in the remainder of the proof.\\

\noindent \textbf{Claim 1:} This is straightforward. The labeling $\tilde \sigma_u$ has at most $\gamma n$ errors and therefore, $m_1 \geq m_1' \geq \frac{n}{\beta K} - \gamma n$. A similar argument also works for $m_k$.\\

\noindent \textbf{Claim 2:} We show that the estimation error of $\hat{P}_l, \hat{Q}_l$ does not make $\hat{I}$ too small.
\begin{align}
\hat{I} - I_L &= - \log \frac{ 
     \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)
     \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)}{ 
          \left( \sum_l \sqrt{P_l Q_l} \right)^2 }. \label{eqn:Ihat_Istar}
\end{align}
Let us consider the numerator:
\begin{align*}
& \left( \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)
\left( \sum_l \sqrt{ \frac{\hat{Q}_l}{\hat{P}_l}} P_l \right) \\
&= \left( \sum_l \sqrt{ P_l Q_l} \sqrt{ \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l}} \right) 
     \left( \sum_l \sqrt{P_l Q_l} \sqrt{ \frac{P_l}{\hat{P}_l} \frac{\hat{Q}_l}{ Q_l}} \right) \\
&= \sum_l P_l Q_l + 2\sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} + 
   \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
&= \left( \sum_l \sqrt{P_l Q_l} \right)^2 + \sum_{l < l'} 
                  \sqrt{P_l Q_l P_{l'} Q_{l'}} \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right), 
\end{align*}
where we define 
$$T_{l,l'} = \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} \frac{P_{l'}}{\hat{P}_{l'}} \frac{\hat{Q}_{l'}}{Q_{l'}}.$$
%It will be later shown that $T_{l,l'} \rightarrow 1$ and thus, continuing equation~\ref{eqn:Ihat_Istar},
Continuing,
\begin{align}
\hat{I} - I_L &= - \log \left( 1 + \frac{ \sum_{l<l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right)}
    { \left( \sum_l \sqrt{ P_l Q_l} \right)^2 }  \right) \nonumber \\
     &  \geq  - \log \left( 1 + 4 \sum_{l<l'} \sqrt{P_l Q_l P_{l'} Q_{l'}}  
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right)  \right) 
  \quad \trm{(assuming that $\sum_l \sqrt{P_l Q_l} \geq 1/2$)} \nonumber \\
   & \geq - 4 \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right). \label{eqn:Ihat_Istar2}
\end{align}
We proceed by first bounding $|T_{l,l'} - 1|$:
\begin{align*}
|T_{l,l'} - 1| &= \left| \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} 
      \frac{P_{l'}}{\hat{P}_{l'}} \frac{\hat{Q}_{l'}}{Q_{l'}} - 1 \right| \\
 &= \left| \left( 1 - \frac{P_l - \hat{P}_l}{P_l} \right)
    \left( 1 - \frac{\hat{Q}_l - Q_l}{\hat{Q}_l} \right)
   \left( 1- \frac{\hat{P}_{l'} - P_{l'}}{\hat{P}_{l'}}\right)
   \left( 1 -  \frac{Q_{l'}- \hat{Q}_{l'}}{Q_{l'}} \right) -1 \right| \\
&\stackrel{(a)} \leq 2\left( \frac{|P_l - \hat{P}_l|}{P_l} +  \frac{|\hat{Q}_l - Q_l|}{\hat{Q}_l}
           +   \frac{| \hat{P}_{l'} - P_{l'}|}{\hat{P}_{l'}} +
               \frac{| Q_{l'} - \hat{Q}_{l'} | }{Q_{l'}} \right) \\
&\stackrel{(b)} \leq 4\left( \frac{|P_l - \hat{P}_l|}{P_l} +  \frac{|\hat{Q}_l - Q_l|}{Q_l}
           +   \frac{| \hat{P}_{l'} - P_{l'}|}{P_{l'}} +
               \frac{| Q_{l'} - \hat{Q}_{l'} | }{Q_{l'}} \right), 
\end{align*}
where $(a)$ and $(b)$ follow from Lemma~\ref{lem:bound_ratio_P_Pl}, which says that $\frac{|P_l - \hat P_{l}|}{P_l}$ and $\frac{|P_l - \hat P_{'}|}{\hat P_{l}}$ both go to $0$, and at the same rate since $\hat P_l = \Theta(P_l)$. The same also holds true for $Q_l$ and $\hat Q_l$. Since we only work with pairs $(l, l')$ such that $l' > l$ and we can choose whatever ordering we would like. Suppose that the $l$'s are in decreasing order of $\frac{|\hat{P}_l - P_l|}{P_l} + \frac{|\hat{Q}_l - Q_l|}{Q_l}$ and therefore, we have that, for all pairs $l < l'$, 
\[
| T_{l,l'} - 1 | \leq 8
    \left( \frac{|\hat{P}_l - P_l|}{P_l} + \frac{|\hat{Q}_l - Q_l|}{Q_l} \right).
\]
By Proposition~\ref{prop:estimation_consistency}, we have that, for $l \in L_1$,
\begin{align*}
\frac{|P_l - \hat{P}_l|}{P_l} + \frac{|\hat{Q}_l - Q_l|}{Q_l} &\leq \eta \Delta_l \left(\frac{1}{P_l} + \frac{1}{Q_l}\right)
\leq \frac{\eta \Delta_l}{P_l \vee Q_l} \cdot 2\rho_L
\leq \eta' \frac{\Delta_l}{P_l \vee Q_l}.
\end{align*}
Similarly, for $l \notin L_1$, 
\begin{align*}
\frac{|P_l - \hat{P}_l|}{P_l} \leq \eta \sqrt{\frac{P_l \vee Q_l}{n P_l^2}} = \eta \frac{P_l \vee Q_l}{P_l} \sqrt{\frac{1}{n(P_l \vee Q_l)}} \leq \eta \rho_L \sqrt{\frac{1}{n(P_l \vee Q_l)}} \leq \eta' \sqrt{\frac{1}{n(P_l \vee Q_l)}},
\end{align*}
and likewise for the $\frac{|\hat{Q}_l - Q_l|}{Q_l}$ term. We plug these bounds into the previous derivation and obtain
\begin{align*}
|T_{l,l'} - 1|  &\leq \eta'  \frac{\Delta_l}{P_l \vee Q_l}  \quad \trm{for $l \in L_1$}\\
|T_{l,l'} - 1 | &\leq \eta' \frac{1}{\sqrt{ n (P_l \vee Q_l)}} \quad \trm{for $l \notin L_1$}.
\end{align*}
The Taylor approximation of $\sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2$ around $T_{l,l'}=1$ is:
\begin{align*}
\sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} -2  &= 
  \frac{1}{4} (T_{l,l'} - 1)^2 + O (T_{l,l'}-1)^3. 
\end{align*}
Continuing on from equation~\ref{eqn:Ihat_Istar2}, we have that
\begin{align*}
\hat{I} - I_L &\geq - 4 \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
&\geq - 4 \sum_{l \in L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) 
     - 4 \sum_{l \notin L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
  &\geq - \sum_{l \in L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
             \eta' \left( \frac{\Delta_l}{P_l \vee Q_l}  \right)^2 
        - \sum_{l \notin L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
             \eta' \frac{1}{n (P_l \vee Q_l)} \\
&\geq - \eta' \left( \sum_{l \in L_1} \frac{\Delta_l^2 \sqrt{P_l Q_l}}{(P_l \vee Q_l)^2} \right)
 	\left( \sum_{l'}  \sqrt{P_{l'}Q_{l'}} \right) 
	- \eta' \left( \sum_{l \notin L_1} \frac{\sqrt{P_lQ_l}}{n(P_l \vee Q_l)} \right) 
          \left( \sum_{l'} \sqrt{P_{l'} Q_{l'} } \right) \\
 &\geq - \eta' \left( \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} \right)
         \left( \sum_{l'}  \sqrt{P_{l'}Q_{l'}} \right) 
       - \eta' \left( \sum_{l \notin L_1} \frac{1}{n} \right) 
          \left( \sum_{l'} \sqrt{P_{l'} Q_{l'} } \right) \\
 &\stackrel{(a)}=  -o(I_L),
\end{align*}
where $(a)$ follows from the following facts: We have $\sum_{l'} \sqrt{P_{l'} Q_{l'}} \leq 1$. Furthermore, Lemma~\ref{lem:friday_night} states that $\sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} = \Theta(I_L)$, and finally our assumptions imply that  $\sum_{l \notin L_1} \frac{1}{n} \leq \frac{L}{n} = o(I_L)$. This proves claim 2.\\

\noindent \textbf{Claim 3.} 
We rewrite the term in claim 3 as follows:
\begin{align*}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} \\
&= \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} 
   \left( \frac{\sum_l \sqrt{\hat{P}_l \hat{Q}_l}}{\sum_l \sqrt{\hat{P}_l \hat{Q}_l}} \right)^{\frac{m_1 - m_k}{2}} \\
&=  \left( 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} \hat{P_l} } \right)^{\frac{m_1 - m_k}{2}}. 
\end{align*}
Assume that $m_k \geq m_1$. The reverse case can be analyzed in the identical manner. We can rewrite the term as
\begin{align*}
\left( 1 + 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l)}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( 1+ \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} (\hat{P}_l - P_l)}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l } \right)^{\frac{m_k - m_1}{2}}. 
\end{align*}
Note that $\sum_l \sqrt{P_l Q_l} \to 1$, and thus Lemma~\ref{lem:sqrt_ratio_times_ql} implies that the denominators are $\Theta(1).$ To bound the numerator term, we apply Lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1}:
\begin{align*}
\left| \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l) \right|  &= 
  \left|  \sum_l \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) 
 \right| \\
 %
& \leq \left| \sum_{l \in L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) \right| +  %next term
  \left| \sum_{l \notin L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) \right| \\
%
& \stackrel{(a)} \leq \left| \sum_{l \in L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) \eta \Delta_l \right| +  %next term
  \left| \sum_{l \notin L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) \eta \sqrt{ \frac{P_l \vee Q_l}{n}}  \right| \\
%
&\stackrel{(b)} \leq \sum_{l \in L_1} \eta \frac{\Delta_l^2}{Q_l} + \sum_{l \notin L_1} \eta \rho_L \frac{1}{n} \\
%
&\leq \eta \rho_L \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} + \sum_{l \notin L_1} \eta \rho_L \frac{1}{n} \\
%
& \stackrel{(c)} \leq \eta' I_L + \eta' \frac{L}{n}  \\
%
&\stackrel{(d)}  \leq \eta' I_L.  
\end{align*}
In the above sequence of inequalities, step $(a)$ follows from Proposition~\ref{prop:estimation_consistency}. Step $(b)$ follows from Lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1}. Step $(c)$ follows from Lemma \ref{lem:friday_night} and the assumption that $\eta \rho_L \to 0$. Step $(d)$ follows from our assumption of $\frac{L}{n} = o(I_L)$. Thus, we obtain
\begin{align*}
& \left( 1 + 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l)}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( 1+ \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} (\hat{P}_l - P_l)}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l } \right)^{\frac{m_k - m_1}{2}} 
\\
&\leq \exp\left( (m_k - m_1) \log(1 + o(I_L) ) \right) \\
&\leq \exp \left( \frac{n}{K} o(I_L) \right) ,
\end{align*}
proving claim 3.\\

\noindent \textbf{Combining bounds for  terms~\ref{eqn:excess_error_term} and \ref{eqn:Ihat_error_term} :}  Multiplying the bounds for terms~\ref{eqn:excess_error_term} and \ref{eqn:Ihat_error_term} shows that the probability of misclassifying $u$ into some cluster $k \neq 1$ is at most $\exp\left(( - (1 + o(1)) \frac{nI_L}{\beta K} \right)$. Taking a union bound over all clusters $k \neq 1$ completes the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%

\subsection{Lemmas for Proposition~\ref{prop:labeled_sbm_rate}}
\label{appendix: lemmas for labeled_sbm_rate}

\begin{lemma}\label{lemma: first}
Let $L$, $P_l$, $Q_l$, $\rho_L$, and $I_L$ satisfy the assumptions in Proposition~\ref{prop:labeled_sbm_rate}. Define the new probabilities of edge labels as follows:
\begin{equation}
P_l' := P_l(1-\delta) + \frac{\delta}{L}, \quad \text{ and }
Q_l' := Q_l(1-\delta) + \frac{\delta}{L},
\end{equation}
for all $1 \leq l \leq L$, where $\delta = \frac{cL}{n}$ for a constant $c>0$. Let $I_L'$ be the  Renyi divergence between $P_l'$ and $Q_l'$. Then for all large enough $n$, we have that $P_l', Q_l' > \frac{c}{n}$ for all $1 \leq l \leq L$, and 
\begin{equation}
I_L' = I_L(1+o(1)).
\end{equation}
\end{lemma}
\begin{proof}
We know that 
\begin{align*}
I_L &= -2\log \sum_{i=1}^L \sqrt{P_lQ_l}\\
&= -2\log\left(1 - \frac{1}{2}\sum_{i=1}^L (\sqrt P_l - \sqrt Q_l)^2\right)\\
&=\left( \sum_{i=1}^L (\sqrt P_l - \sqrt Q_l)^2\right) (1+o(1)).
\end{align*}
Similarly, we also have $I_L' = \left( \sum_{i=1}^L (\sqrt {P_l'} - \sqrt {Q_l'})^2\right) (1+o(1))$, and it is enough to show that 
$$\left( \sum_{i=1}^L (\sqrt P_l - \sqrt Q_l)^2\right) = \left( \sum_{i=1}^L (\sqrt {P_l'} - \sqrt {Q_l'})^2\right) (1+o(1)).$$ 
Equivalently, we want to show that
$$\left | \sum_{l=1}^L \frac{\Delta_l^2}{(\sqrt P_l + \sqrt Q_l)^2} - \sum_{l=1}^L \frac{(\Delta_l')^2}{(\sqrt{P_l'} + \sqrt{Q_l'})^2} \right | = o(I_L).$$
We consider two cases: $\rho_L = \omega(1)$ and $\rho_L = \Theta(1)$. If $\rho_L = \omega(1)$, choose $a = \frac{nI_L}{\rho_L L}$. If $\rho_L = \Theta(1)$, choose $a = o\left(\frac{nI_L}{L}\right)$ such that $a \to \infty$. Note that in both cases, we have $\frac{a}{\rho_L} \to \infty$ and $\frac{aL}{n} = o(I_L)$. We now break up the set of colors into two groups: $G_1$ contains colors which satisfy $P_l \vee Q_l \leq \frac{a}{n}$, and $G_2$ = $G_1^c$. 

For colors in $G_1$, we have $\Delta_l \leq \frac{a}{n}$. Thus,
\begin{align*}
\frac{\Delta_l^2}{(\sqrt P_l + \sqrt Q_l)^2} \leq \Delta_l \leq \frac{a}{n},
\end{align*}
and
\begin{align*}
\frac{(\Delta_l')^2}{(\sqrt {P_l'} + \sqrt {Q_l'})^2} \leq \Delta_l' = (1-\delta)\Delta_l \leq (1-\delta) \frac{a}{n}.
\end{align*}
Thus, we have
\begin{align*}
\left | \sum_{l \in G_1} \frac{\Delta_l^2}{(\sqrt P_l + \sqrt Q_l)^2} - \sum_{l \in G_1} \frac{(\Delta_l')^2}{(\sqrt{P_l'} + \sqrt{Q_l'})^2} \right | &\leq \sum_{l \in G_1} \left(\frac{a}{n} + (1-\delta)\frac{a}{n} \right)\\
&\leq \frac{2aL}{n}\\
&= o(I_L).
\end{align*}
For colors in $G_2$, we may write a similar expression
\begin{align*}
&\left | \sum_{l \in G_2} \frac{\Delta_l^2}{(\sqrt P_l + \sqrt Q_l)^2} - \sum_{l \in G_2} \frac{(\Delta_l')^2}{(\sqrt{P_l'} + \sqrt{Q_l'})^2} \right |\\
&= \sum_{l \in G_2} \frac{\Delta_l^2}{(\sqrt P_l + \sqrt Q_l)^2} \left | 1 - (1-\delta)^2\frac{(\sqrt P_l + \sqrt Q_l)^2}{(\sqrt{P_l'} + \sqrt{Q_l'})^2} \right |.
\end{align*}
We analyze the term inside the absolute value as follows.
\begin{align*}
\frac{(\sqrt P_l + \sqrt Q_l)^2}{(\sqrt{P_l'} + \sqrt{Q_l'})^2} &= \left(\frac{\sqrt P_l + \sqrt Q_l}{\sqrt P_l \sqrt{\frac{P_l'}{P_l}} + \sqrt Q_l \sqrt{\frac{Q_l'}{Q_l}}}\right)^2\\
&=  \left(\frac{\sqrt P_l + \sqrt Q_l}{\sqrt P_l \sqrt{1-\delta + \frac{c}{nP_l}} + \sqrt Q_l \sqrt{1-\delta + \frac{c}{nQ_l}}}\right)^2.\\
\end{align*}
Since $P_l \vee Q_l > \frac{a}{n}$,
\begin{align*}
\frac{1}{n(P_l \vee Q_l)} < \frac{1}{a} = o(1), \quad \text{ and }\\
\frac{1}{nP_l} \leq \frac{\rho_L}{n(P_l \vee Q_l)} < \frac{\rho_L}{a} = o(1).
\end{align*}
Furthermore, since $\delta = o(1)$, 
\begin{align*}
\left(\frac{\sqrt P_l + \sqrt Q_l}{\sqrt P_l \sqrt{1-\delta + \frac{c}{nP_l}} + \sqrt Q_l \sqrt{1-\delta + \frac{c}{nQ_l}}}\right)^2 &= \left(\frac{\sqrt P_l + \sqrt Q_l}{\sqrt P_l (1+o(1)) + \sqrt Q_l (1+o(1))}\right)^2\\
&= 1+o(1).
\end{align*}
Hence, we may conclude that
\begin{align*}
 \sum_{l \in G_2} \frac{\Delta_l^2}{(\sqrt P_l + \sqrt Q_l)^2} \left | 1 - (1-\delta)^2\frac{(\sqrt P_l + \sqrt Q_l)^2}{(\sqrt{P_l'} + \sqrt{Q_l'})^2} \right | &=  \sum_{l \in G_2} \frac{\Delta_l^2}{(\sqrt P_l + \sqrt Q_l)^2} o(1)\\
 &= o(I_L).
\end{align*}
Combining the result for colors in $G_1$ and $G_2$, we conclude that $I_L' = (1+o(1)) I_L.$
\end{proof}



We often use the bound that $\frac{1}{2} P \leq \hat{P}_l \leq 2 P_l$. The following lemma justifies this.
\begin{lemma}
\label{lem:bound_ratio_P_Pl}
Let $l$ be any color and suppose that $\frac{1}{\rho_L} \leq \frac{P_l}{Q_l} \leq \rho_L$ where $\rho_L > 1$; suppose also that $P_l, Q_l \geq \frac{c}{n}$ for some absolute constant $c$. Let us condition on the event that the 
conclusion of proposition~\ref{prop:estimation_consistency} holds with a sequence $\eta$ such that $\eta \rho_L^2 \rightarrow 0$. Then we have that
\[
\max_l \frac{|\hat{P}_l - P_l|}{P_l} \rightarrow 0
\quad \trm{ and } \quad
\max_l \frac{|\hat{Q}_l - Q_l|}{Q_l} \rightarrow 0.
\]
In particular, for small enough $\eta$, we have that $\frac{1}{2} P_l \leq \hat{P}_l \leq 2P_l$ for all $l$ and likewise for $Q_l$.
\end{lemma}


\begin{proof}

We prove the statement first for $P_l$; the same argument goes for $Q_l$. By Proposition~\ref{prop:estimation_consistency}, we have that either $| \hat{P}_l - P_l | \leq \eta \Delta_l$ or $| \hat{P}_l - P_l | \leq \eta \sqrt{ \frac{P_l \vee Q_l}{n} }$ holds. Let us suppose $| \hat{P}_l - P_l | \leq \eta \Delta_l$ first. Then,
\begin{align*}
\frac{ | \hat{P}_l - P_l |}{P_l} \leq \eta \frac{\Delta_l}{P_l} \leq \eta (1+\rho_L) \rightarrow 0.
\end{align*}
Now suppose that $| \hat{P}_l - P_l | \leq \eta \sqrt{ \frac{P_l \vee Q_l}{n} }$. Then,
\begin{align*}
\frac{| \hat{P}_l - P_l |}{P_l} \leq \eta \sqrt{ \frac{\rho_L}{ P_l n}} \leq \eta \sqrt \rho_L \sqrt{ \frac{1}{c}} \rightarrow 0,
\end{align*}
where we use that $\frac{P_l \vee Q_l}{P_l}$ is at most $\rho_L$.
\end{proof}



\begin{lemma}
\label{lem:sqrt_ratio_pl_ql_minus_1}
Suppose that $\frac{1}{\rho_L} \leq \frac{P_l}{Q_l} \leq \rho_L$ for $\rho_L > 1$ and that $P_l, Q_l \geq \frac{c}{n}$ for some absolute constant $c$. Let us condition on the event that the 
conclusion of Proposition~\ref{prop:estimation_consistency} holds with a sequence $\eta$ such that $\eta \rho_L^2 \rightarrow 0$. The following are true:
\begin{enumerate}
\item 
For all $l$ satisfying  $n \frac{\Delta_l^2}{P_l \vee Q_l} \geq 1$, we have  
\[
\left| \sqrt{ \frac{\hat{P}_l }{\hat{Q}_l} } - 1 \right|
                    \leq \left| \frac{P_l - Q_l}{Q_l} \right| (1 + \eta'),
\]
where $\eta' \rightarrow 0$ and does not depend on the color $l$. 

\item
For all $l$ satisfying $n \frac{\Delta_l^2}{P_l \vee Q_l} < 1$, we have that
\[
\left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right| \leq
 2 \rho_L \frac{1}{\sqrt{n  (P_l \vee Q_l) } }.
\]

\end{enumerate}
The same bounds also hold for $\sqrt{ \frac{\hat{Q}_l}{\hat{P}_l}} - 1$ by symmetry.
\end{lemma}



\begin{proof}
First, suppose that $l$ satisfies $n \frac{\Delta_l^2}{P_l \vee Q_l} \geq 1$. We note that by Lemma~\ref{lem:bound_ratio_P_Pl}, we have that $ \frac{\hat{Q}_l - Q_l}{Q_l} = \eta'$ where $\max_l |\eta'| \rightarrow 0$. In the following derivation, we use $\eta'$ to denote a sequence such that $\max_l |\eta'| = o(1)$; the actual value of $\eta'$ may change from instance to instance. We use $\eta$ to denote a sequence where $\max_l |\eta \rho_L| = o(1)$.

\begin{align*}
\frac{\hat{P}_l}{\hat{Q}_l} - 1&= 
      \frac{ \hat{P}_l - P_l + P_l }{ \hat{Q}_l - Q_l + Q_l} -1 \\
     %
  &= \frac{  \frac{\hat{P}_l - P_l}{Q_l} + \frac{P_l}{Q_l}}
       { \frac{\hat{Q}_l - Q_l}{Q_l} + 1} - 1 \\
       %
 &= \left( \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} \right)
    \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta')  \right) -1  \\
    %
 &= \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} 
     - \frac{P_l}{Q_l} \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta') 
     - \frac{\hat{P}_l - P_l}{Q_l} \frac{\hat{Q}_l - Q_l}{Q_l}(1+ \eta') 
   - 1 \\
   %
 &\stackrel{(a)}= \frac{P_l}{Q_l} + \frac{\eta \Delta_l}{Q_l} 
     + \rho_L \frac{\eta \Delta_l}{Q_l} (1+\eta') + \frac{\eta \Delta_l}{Q_l} \eta' 
   - 1 \\
   %
 &= \frac{P_l}{Q_l} + \eta \frac{\Delta_l}{Q_l}
     + \eta \rho_L \frac{\Delta_l}{Q_l}
   - 1 \\
   %
 &= \frac{P_l - Q_l}{Q_l} ( 1 + \eta').
\end{align*}
In $(a)$,  we used the fact that $| \hat{P}_l - P_l| \leq \eta \Delta_l$ and $| \hat{Q}_l - Q_l| \leq \eta \Delta_l$ by the conclusion of Proposition~\ref{prop:estimation_consistency}. Using the inequality 
$$|\sqrt{x} - 1| \leq |x-1|$$ 
now completes the proof of the first case.

The proof of the second case is almost identical. Let us assume that $l$ is such that $n \frac{\Delta_l^2}{P_l \vee Q_l} < 1$. In this case, we have that
\begin{align*}
| \hat{P}_l - P_l | &= \eta \sqrt{ \frac{P_l \vee Q_l}{n} } \quad \text{ and } \\
| \hat{Q}_l - Q_l | &= \eta \sqrt{ \frac{P_l \vee Q_l}{n} },
\end{align*}
where  $\max_l |\eta \rho_L^2| \rightarrow 0$. By Lemma~\ref{lem:bound_ratio_P_Pl}, we have $\frac{\hat{Q}_l - Q}{Q_l} = \eta'$ where $\max_l |\eta'| = o(1)$. In the following derivation, we use $\eta'$ to denote a sequence such that $\max_l |\eta'| = o(1)$; the actual value of $\eta'$ may change from instance to instance. We may check that the following equalities hold:
\begin{align*}
\frac{\hat{P}_l}{\hat{Q}_l} - 1 &= 
     \left( \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} \right)
     \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} ( 1 + \eta') \right) - 1 \\
     %
 &= \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} 
     - \frac{P_l}{Q_l} \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta') 
     - \frac{\hat{P}_l - P_l}{Q_l} \frac{\hat{Q}_l - Q_l}{Q_l}(1+ \eta') 
   - 1  \\
   %
&= \frac{P_l }{Q_l}  + \eta \sqrt{\frac{P_l \vee Q_l}{nQ_l^2}} + \rho_L \eta \sqrt{\frac{P_l \vee Q_l}{nQ_l^2}} + \eta \sqrt{\frac{P_l \vee Q_l}{nQ_l^2}} -1\\ 
%
&= \frac{P_l }{Q_l}  + \rho_L \eta \sqrt{\frac{(P_l \vee Q_l)^2}{nQ_l^2(P_l \vee Q_l)}} -1\\ 
%
&= \frac{P_l - Q_l}{Q_l}  + \eta \rho_L^2 \sqrt{\frac{1}{n(P_l \vee Q_l)}}\\
%
 &= \frac{P_l - Q_l}{Q_l} + \eta' \rho_L \sqrt{ \frac{1}{ n (P_l \vee Q_l)} } .
\end{align*}
Using the inequality $|\sqrt{1+x}-1| \leq x$ for $x \geq 0$, we conclude that
\begin{align*}
\left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right| &= 
 \left|  \sqrt{ 1 + \frac{P_l - Q_l}{Q_l} + \eta' \rho_L \frac{1}{\sqrt{n (P_l \vee Q_l)} }}
   -1  \right| \\
  &\leq \left| \frac{P_l - Q_l}{Q_l} + \eta' \rho_L \frac{1}{\sqrt{ n (P_l \vee Q_l)}}    \right| \\
 &\stackrel{(a)} \leq 2 \rho_L \frac{1}{\sqrt{n (P_l \vee Q_l)} }, 
\end{align*}
where $(a)$ follows because we assumed $\frac{n\Delta_l^2}{P_l \vee Q_l} <1$, which means
$$ \left| \frac{P_l - Q_l}{Q_l} \right|\leq \sqrt{\frac{P_l \vee Q_l}{n Q_l^2}} =  \sqrt{\frac{(P_l \vee Q_l)^2}{n Q_l^2(P_l \vee Q_l)}} \leq \rho_L \frac{1}{\sqrt{n(P_l \vee Q_l)}}.$$
\end{proof}




The following lemma bounds $\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } Q_l$. 
\begin{lemma}
\label{lem:sqrt_ratio_times_ql}
Suppose that
\[
\frac{|\hat{Q}_l - Q_l|}{Q_l} = \eta' \quad \text{ and }
\frac{|\hat{P}_l - P_l|}{P_l} = \eta'
\]
where $\max_l |\eta'| = o(1)$. Then, we have that for all small enough $\eta$, 
\[
\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } Q_l \geq \frac{1}{2} \sqrt{\hat{P}_l \hat{Q}_l} \geq
  \frac{1}{8} \sqrt{P_l Q_l}
\]
\end{lemma}


\begin{proof}
We have the sequence of inequalities
\begin{align*}
\sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l  &= \sqrt{ \hat{P}_l \hat{Q}_l} \frac{Q_l}{\hat{Q}_l} \\
&= \sqrt{\hat{P}_l \hat{Q}_l} \frac{1}{ \frac{\hat Q_l - {Q}_l}{Q_l} + 1 } \\
&= \sqrt{\hat{P}_l \hat{Q}_l} 
  \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta') \right)  \\
&= \sqrt{\hat{P}_l \hat{Q}_l} (1 - \eta).
\end{align*}
where in the second to last equality, we used the fact that $\frac{\hat{Q}_l - Q_l}{Q_l} = \eta' \rightarrow 0$. A small enough value of $\eta$ leads to the proof of the first inequality.

Also, for small enough $\eta$, we have that $\hat{P}_l \geq \frac{1}{2} P_l$ and $\hat{Q}_l \geq \frac{1}{2} Q_l$, giving us the second inequality.
\end{proof}

\begin{lemma}\label{lem:friday_night}
Define $L_1 = \{ l \,:\, n \frac{\Delta_l^2}{P_l \vee Q_l} \geq 1 \}$. Then 
\begin{align}
 C_1\sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} \leq I_L \leq  C_2 \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l},  \label{eqn:good_color_I},
\end{align}
for some constants $C_1$ and $C_2$. 
\end{lemma}
\begin{proof}
Throughout the proof, we let $\eta'$ denote a sequence that converges to 0, and $C$ to be a $\Theta(1)$ sequence. Their value could change from line to line. First observe that
\begin{align*}
I_L  = -2 \log \sum_l \sqrt{P_l Q_l} = -2 \log \left( 1 - \frac{\sum_l (\sqrt P_l - \sqrt Q_l)^2}{2}\right).
\end{align*}
Using the fact that $I_L \to 0$ as $n \to \infty$, and thus $\sum_l (\sqrt P_l - \sqrt Q_l)^2 \to 0$, we see that the following bounds hold for all large enough $n$:
\begin{align*}
\frac{1}{2} \sum_l (\sqrt{P_l} - \sqrt{Q_l} )^2 \leq I_L \leq 2 \sum_l (\sqrt{P_l} - \sqrt{Q_l} )^2.
 \end{align*}
Expressing $\sum_l (\sqrt{P_l} - \sqrt{Q_l} )^2$ as $\sum_l \frac{\Delta_l^2}{(\sqrt{P_l} + \sqrt{Q_l})^2}$, we conclude that there exist constants $\tilde C_1$ and $ \tilde C_2$ such that
\begin{align*}
\tilde C_1\sum_{l } \frac{\Delta_l^2}{P_l \vee Q_l} \leq I_L \leq  \tilde C_2 \sum_{l } \frac{\Delta_l^2}{P_l \vee Q_l}.
\end{align*}
Observe that
\begin{align}
\sum_{l } \frac{\Delta_l^2}{P_l \vee Q_l} &= \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} + \sum_{l \in L_1^c} \frac{\Delta_l^2}{P_l \vee Q_l}.
\end{align}
The contribution of the colors in $L_1^c$ is bounded as
\begin{align}
\sum_{l \in L_1^c} \frac{\Delta_l^2}{P_l \vee Q_l} &\leq \frac{L}{n},
\end{align}
and using our assumptions $\frac{L}{n}$ is $o(I_L)$. This implies that the contribution from the colors in $L_1$ must be $\Theta(I_L)$; i.e.. we can find constants $C_1$ and $C_2$ such that
$$ C_1\sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} \leq I_L \leq  C_2 \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l}.$$
\end{proof}



We state Lemma 4 from~\cite{gao2015achieving} which analyzes the consensus step of the algorithm.
\begin{lemma} 
\label{lem:consensus_analysis}
Let $\sigma, \sigma'$ be two clusters such that, for some constant $C \geq 1$, the minimum cluster size is at least $ \frac{n}{Ck}$. Define a map $\xi \,:\, [k] \rightarrow [k]$ as $\xi(k) = \argmax_{k'} | \{ v \,:\, \sigma(v) = k \} \cap \{ v \,:\, \sigma'(v) = k' \} |$. Then, if $\min_{\pi \in S_k} l(\pi(\sigma), \sigma') < \frac{1}{Ck}$, we have that $\xi \in S_k$ and $l(\xi(\sigma), \sigma') = \min_{\pi \in S_k} l(\pi(\sigma), \sigma')$. 
\end{lemma}

We include a simple additional lemma.
\begin{lemma}
\label{lem:consensus_uniqueness}
Let $\sigma, \sigma' \,:\, [n] \rightarrow [K]$ be two clusterings where the minimum cluster size of $\sigma$ is $T$. Let $\pi, \xi \in S_K$ be such that
\[
d(\pi(\sigma), \sigma') < T/2 \qquad \text{ and }  \qquad d(\xi(\sigma), \sigma') < T/2
\]
Then it must be that $\pi = \xi$.
\end{lemma}

\begin{proof}
Suppose not, then choose any $k$ such that $\pi(k) \neq \xi(k)$. 
\begin{align*}
| \{ \sigma(u) = k \} \cap \{ \sigma'(u) \neq \pi(k) \} | < d(\pi(\sigma), \sigma') < T/2.
\end{align*}
So, then, we have that $| \{ \sigma(u) = k \} \cap \{ \sigma'(u) = \pi(k) \}| > T/2$. But then,
\begin{align*}
d(\xi(\sigma), \sigma') &\geq | \{ \sigma(u) = k \} \cap \{\sigma'(u) \neq \xi(k) \} | \\
   &\geq | \{ \sigma(u) = k\} \cap \{ \sigma'(u) = \pi(k) \}| \\ 
   &\geq T/2.
\end{align*}
\end{proof}

%*********************************************************************************************************%
%*********************************************************************************************************%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%*********************************************************************************************************%
%*********************************************************************************************************%

\section{Proof of Proposition~\ref{prop:discretization1}}

We first, for the convenience of the readers, restate the proposition:

\begin{repproposition} {prop:discretization1}
Let $p(x), q(x)$ be two densities supported on $[0,1]$. Suppose that $H \equiv \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = o(1)$ and suppose they satisfy the following assumptions:
\begin{enumerate}
\item[C1] Suppose $p(x), q(x) \leq C$ on $[0,1]$ and are absolutely continuous.
\item[C2] There exists $R$ a subinterval of $[0,1]$ such that $\frac{1}{\rho} \leq \left| \frac{p(x)}{q(x)} \right| \leq \rho$ and $\mu\{R^c\} = o(H)$ where $\mu$ is the Lebesgue measure.
\item[C3] Define $\alpha^2 = \int_R \frac{(p(x) - q(x))^2}{q(x)} dx$ and $\gamma(x) = \frac{q(x) - p(x)}{\alpha}$. Suppose $\int_R q(x) \left| \frac{\gamma(x)}{q(x)} \right|^r dx  \leq M$ for constants $M, r \geq 4$.
\item[C4] Let $h(x) \geq \sup_n \max \left\{  \left|\frac{\gamma'(x)}{q(x)} \right|, 
 \left|\frac{q'(x)}{q(x)}\right|  \right\} $. Suppose $\int_R |h(x)|^t dx \leq M'$ for some constant $M'$ and $1 \geq t \geq 2/r$. Suppose also that the level set $\{x \,:\, |h(x)| \geq \kappa\}$ is a union of at most $K_h$ intervals for all large enough $\kappa$.  
\item[C5] For all $x \leq \frac{1}{L}$, $p'(x), q'(x) \geq 0$ and for all $x \geq 1 - \frac{1}{L}$, we have that $p'(x), q'(x) \leq 0$.
\end{enumerate}
Suppose $\frac{1}{c_0} \leq \frac{1 - P_0}{1-Q_0} \leq c_0$. Let $\\bin_l = [a_l, b_l]$ for $l=1,...,L$ be a uniformly spaced binning of the interval $[0,1]$ and let
\begin{align*}
\tilde P_l &= (1- P_0) \int_{a_l}^{b_l} p(x) dx := (1-P_0) P_l\quad \text{ and }\\
\tilde Q_l &= (1-Q_0)\int_{a_l}^{b_l} q(x) dx := (1-Q_0) Q_l.
\end{align*}
Suppose $L \leq \frac{2}{H}$. Define 
\begin{align*}
I &= -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0) p(x) q(x)} dx \right) \quad \text{ and }\\
I_L &= -2 \log \left( \sqrt{P_0 Q_0} + \sum_{l=1}^L \sqrt{\tilde P_l \tilde Q_l} \right).
\end{align*}
Then, we have that
 $$\left| \frac{I - I_L}{I} \right| = o(1),$$ 
and that $\frac{1}{2\rho c_0} \leq \frac{\tilde P_l}{\tilde Q_l} \leq 2\rho c_0$ for all $l$. 
\end{repproposition}


\begin{proof}
We first show that the likelihood ratio $\frac{\tilde P_l}{\tilde Q_l} = \frac{1-P_0}{1-Q_0} \frac{P_l}{Q_l}$ satisfies the bounds claimed. Let us consider an $l$ such that $\bin_l \cap R^c = \emptyset$. Then, for all $x \in \bin_l$, we have that 
$\frac{1}{\rho} \leq \frac{p(x)}{q(x)} \leq \rho$ by assumption C2. It follows immediately that $\frac{P_l}{Q_l} = \frac{\int_{\bin l} p(x)dx}{\int_{\bin_l} q(x)dx} \leq \rho.$ The lower bound follows in the same manner. Using the fact that $\frac{1-P_0}{1-Q_0}$ is bounded between $1/c_0$ and $c_0$, we conclude that $\frac{P_l}{Q_l}$ is bounded between $\frac{1}{\rho c_0}$ and $\rho c_0$.

Now suppose $\bin_l \cap R^c \neq \emptyset$. By the fact that $R$ is an interval and that $\mu\{R^c\} = o(H)$, and since $L \leq 2/H$ we conclude that $\mu\{R^c\} < 1/2L$ for all large enough $n$. This means only bins $[0, \frac{1}{L}]$ and $[1-\frac{1}{L}, 1]$ can potentially satisfy $\bin_l \cap R^c \neq 0$. Notice that assumption C5 indicates that both $p(x)$ and $q(x)$ are increasing in the interval $[0, 1/L]$ and decreasing in the interval $[1-1/L, 1]$. Define $P_l' = \int_{\bin_l \cap R} p(x) dx$ and $Q'_l = \int_{\bin_l \cap R} q(x) dx$. Define $P_l'' = \int_{\bin_l \cap R^c} p(x) dx$ and $Q''_l = \int_{\bin_l \cap R^c} q(x) dx$. Thus, we have $P_l = P_l' + P_l''$ and $Q_l = Q_l' + Q_l''$ for all $l$. Note  that $\frac{1}{\rho} \leq \frac{P'_l}{Q'_l} \leq \rho$ by same argument as before.  Furthermore, using the monotonic properties of $p(x)$ and $q(x)$ in the relevant intervals, we have
\[
P'_l \geq \min_{x \in \bin_l \cap R} p(x) \frac{1}{2L} \geq \max_{x \in \bin_l \cap R^c} p(x) \frac{1}{2L} \geq P''_l,
\]
where the first inequality follows because $\mu(R^c) \leq \frac{1}{2L}$ and the second inequality follows from assumption C5. Similarly, we can derive that $Q'_l \geq Q''_l$. Thus,
\begin{align*}
\frac{P_l}{Q_l} &\leq \frac{2 P'_l}{ Q'_l} \leq 2\rho, \quad \text{ and } \\
\frac{P_l}{Q_l} &\geq \frac{P'_l}{2 Q'_l} \geq \frac{1}{2\rho}.
\end{align*}
Using the bound on $\frac{1-P_0}{1-Q_0}$ completes the proof.\\

We now proceed with the bounding $|I-I_L|$. Using the simple relation between Renyi divergence by Hellinger distance detailed in Lemma~\ref{lem:renyi_hellinger}, we have that
\begin{align*}
I &= (1+o(1))\left\{ (\sqrt{P_0} - \sqrt{Q_0} )^2 + 
          \int \left( \sqrt{(1-P_0)p(x)} - \sqrt{(1-Q_0)q(x)} \right)^2 dx \right\} \\
  &= (1+o(1)) \left\{ 
       (\sqrt{P_0} - \sqrt{Q_0} )^2 + (\sqrt{1- P_0} - \sqrt{1-Q_0} )^2
     + \sqrt{(1-P_0)(1-Q_0)} \int \left( \sqrt{p(x)} - \sqrt{q(x)} \right)^2 dx \right\}.
\end{align*}
Likewise, we have that
\begin{align*}
I_L &= (1+o(1))\left\{ (\sqrt{P_0} - \sqrt{Q_0} )^2 + 
          \sum_{l=1}^L \left( \sqrt{\tilde P_l} - \sqrt{\tilde Q_l} \right)^2 dx \right\} \\
  &= (1+o(1)) \left\{ 
       (\sqrt{P_0} - \sqrt{Q_0} )^2 + (\sqrt{1- P_0} - \sqrt{1-Q_0} )^2
     + \sqrt{(1-P_0)(1-Q_0)} \sum_{l=1}^L (\sqrt{{P}_l} - \sqrt{{Q}_l})^2 \right\}.
\end{align*}
The key step in completing our proof is Proposition~\ref{prop:H_HL_convergence1},  proved in Appendix \ref{appendix: d1}.
\begin{repproposition}{prop:H_HL_convergence1}
Under assumptions C1-C5,  we have that
\[
\left| \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx - \sum_{l=1}^L (\sqrt{{P}_l} - \sqrt{{Q}_l})^2 \right| = o\left( \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right).
\]
\end{repproposition}
The claimed result follows from Proposition~\ref{prop:H_HL_convergence1} by noticing that
\begin{align*}
I_L &= (1 + o(1))  \left\{ 
       (\sqrt{P_0} - \sqrt{Q_0} )^2 + (\sqrt{1- P_0} - \sqrt{1-Q_0} )^2
     + \sqrt{(1-P_0)(1-Q_0)} (1+o(1)) \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx  \right\} \\
  &= (1 + o(1)) I.
\end{align*}

The proof Proposition~\ref{prop:H_HL_convergence1} contains a number of subparts, which we briefly describe below. Since $p(x)$ and $q(x)$ are easier to handle on the interval $R$, we initially only concern ourselves with comparing 
$$H_R := \int_R (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \quad \text{and} \quad H^R_L := \quad \sum_{l=1}^L (\sqrt{{P}_l'} - \sqrt{{Q}_l'})^2.$$
We first notice that $\{ \bin_l \} \cap R$ constitute an approximately uniform binning of $R$; i.e., we can find constants $c_\bin$ and $C_\bin$ such that $\frac{c_\bin}{L} \leq |\bin_l \cap R| \leq \frac{C_\bin}{L}.$ This is reasoned as follows. Since $R$ is an interval, we know that $\bin_l \cap R$ is an interval as well. Secondly, we have the inequality $| \bin_l \cap R^c | \leq \mu\{ R^c \} \leq \frac{1}{2L}$. So we have the inequality 
$$\frac{1}{2L} \leq |\bin_l \cap R| \leq \frac{1}{L}.$$
Then, in a series of lemmas, we show that  such an approximately uniform binning of $R$ leads to several useful bounds on $H^R$ and $H^R_L$. In particular, we show in Lemma~\ref{prop:d_dL_convergence} that as long as $L$ grows, we have $d_L \to 1/4$, where
$$d_L := \sum_l Q_l' \left(\frac{1}{2} \frac{\gamma_l'}{Q_l'}\right) \quad \text{ and } \int_R q(x) \left(\frac{1}{2}\frac{\gamma(x)}{q(x)}\right)^2 dx \stackrel{(a)}= \frac{1}{4}.$$
Here $\gamma_l' = Q_l'-P_l'$ and equality $(a)$ follows from the definition of $\alpha$ in Assumption C.3. In Lemma~\ref{prop:continuous_hellinger_chi_square}, we show that 
$$H^R = \frac{\alpha^2}{4} (1+\eta),$$
where $\eta = \Theta(\alpha)$. Similarly, in Lemma~\ref{prop:discrete_hellinger_chi_square} we show that 
$$H^R_L = d_L\alpha^2(1+\eta_L),$$
where $\eta_L = \Theta(\alpha)$. We combine the results of Lemmas~\ref{prop:d_dL_convergence}, \ref{prop:continuous_hellinger_chi_square} and \ref{prop:discrete_hellinger_chi_square}  in Lemma~\ref{prop:H_HL_convergence_R} and show that 
$$|H^R-H^R_L| = o(H^R).$$
Finally, we use Lemma~\ref{prop:H_HL_convergence_R} to complete the proof of Proposition~\ref{prop:H_HL_convergence1}.
\end{proof}





\section{Appendix for Proposition~\ref{prop:discretization1}}

\subsection{Proof of Proposition~\ref{prop:H_HL_convergence1}}
\label{appendix: d1}
\begin{proposition}
\label{prop:H_HL_convergence1}
Suppose assumptions C1-C5 hold. Then we have that
\[
\left| \frac{
         \sum_l (\sqrt{P_l} - \sqrt{Q_l})^2 }{ \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx} - 1 \right| \rightarrow 0.
\] 
\end{proposition}


\begin{proof}
Let $H = \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx$. Let $a_L$ be an $o(1)$ sequence such that $\mu(R^c) \leq a_L H $. We divide the set of bins into three sets $L_1, L_2, L_3$:
\begin{align*}
L_1 &= \{ l\,:\, \bin_l \cap R^c = \emptyset \} \\
L_2 &= \{ l \,:\, \bin_l \cap R^c \neq \emptyset, \, P_l \vee Q_l \geq 2C a_L H \} \\
L_3 &= \{ l \,:\, \bin_l \cap R^c \neq \emptyset, \, P_l \vee Q_l \leq 2C a_L H \}.
\end{align*}
For each bin $l$, define $P'_l = \int_{\bin_l \cap R} p(x) dx$ and $P''_l = \int_{\bin_l \cap R^c} p(x) dx$. Likewise for $Q'_l$ and $Q''_l$. We now proceed in two steps:

\textbf{Step 1:} We first claim that for all $l \in L_2$, 
\[
\left| (\sqrt{P_l} - \sqrt{Q_l})^2 - (\sqrt{P'_l} - \sqrt{Q'_l})^2 \right| \leq a_L H.
\]
Since $\mu(R^c) \leq a_L H$, we have that $P''_l = \int_{\bin_l \cap R^c} p(x) dx \leq C a_L H $ and likewise for $Q''_l$. 
\begin{align*}
(\sqrt{P_l} - \sqrt{Q_l})^2 - (\sqrt{P'_l} - \sqrt{Q'_l})^2 &=
   P_l + Q_l - P'_l - Q'_l - 2 \sqrt{P_l Q_l} + 2 \sqrt{P'_l Q'_l} \\
  &\stackrel{(a)}\leq P''_l + Q''_l - 2 \sqrt{P''_l Q''_l} \\
  &\leq P''_l + Q''_l \\
  &\leq 2 C a_L H,
\end{align*}
Here, $(a)$ follows from the following reasoning. First, by AM-GM inequality, we have that $2 \sqrt{P'_l Q'_l P''_l Q''_l} \leq P'_l Q''_l + P''_l Q'_l$. Thus:
\begin{align*}
P'_l Q'_l + P''_l Q''_l + 2 \sqrt{P'_l Q'_l P''_l Q''_l } \leq (P'_l + P''_l)(Q'_l + Q''_l).
\end{align*}
Taking square roots and replacing $P_l' + P_l'' = P_l$ and likewise for $Q_l$, we conclude 
$$\sqrt{P'_l Q'_l} + \sqrt{P''_l Q''_l}   \leq \sqrt{ P_lQ_l},$$ 
which yields inequality $(a)$.

On the other hand, we have that
\begin{align*}
\sqrt{P_l Q_l} - \sqrt{P'_l Q'_l} 
  &= \frac{ P_l Q_l  - P'_l Q'_l }
          {  \sqrt{P_l Q_l} + \sqrt{P'_l Q'_l} }\\
  &= \frac{ P'_l Q''_l + P''_l Q'_l + P''_l Q''_l} 
          {  \sqrt{ P_l Q_l } + \sqrt{P'_l Q'_l} } \\
  &\leq  \frac{ P'_l Q''_l + P''_l Q'_l + P''_l Q''_l} 
          {  2 \sqrt{P'_l Q'_l} } \\
  &\leq Q''_l \frac{P'_l}{2 \sqrt{P'_l Q'_l}} + P''_l \frac{Q'_l}{2\sqrt{P'_l Q'_l}}
        + Q''_l \frac{P''_l}{2 \sqrt{P'_l Q'_l}}. 
\end{align*}
Note that because $P'_l$ and $Q'_l$ are defined on $R$, we have that
\begin{align*}
\left| \frac{P'_l}{Q'_l} \right| =
\left| \int_{\bin_l \cap R} \frac{ p(x)}{Q'_l} dx \right| 
 \leq \int_{\bin_l \cap R} \left| \frac{p(x)}{q(x)} \right| \frac{q(x)}{Q'_l} dx 
 \leq \rho. 
\end{align*}
Thus, $\sqrt{ \frac{P'_l}{Q'_l} } \vee \sqrt{ \frac{Q'_l}{P'_l}} \leq \sqrt{\rho}$. This bounds the terms $ Q''_l \frac{P'_l}{2 \sqrt{P'_l Q'_l}} + P''_l \frac{Q'_l}{2\sqrt{P'_l Q'_l}} \leq \sqrt{\rho} (Q''_l + P''_l)$. 

We still need to bound the last term $\frac{Q''_l P''_l}{2 \sqrt{P'_l Q'_l}}$. Since $l \in L_2$, we have that either $P_l \geq 2 C a_L H$ or that $Q_l \geq 2 C a_L H$. Let us suppose the former; the latter case can be handled in an identical manner. Since $P''_l \leq C a_L H$ and $P_l \geq 2C a_L H$ , we have that $P''_l \leq P'_l$ and thus, 
$$\frac{Q''_l P''_l}{2 \sqrt{P'_l Q'_l}} \leq Q''_l \frac{P'_l}{2 \sqrt{P'_l Q'_l}} \leq \sqrt{\rho} Q''_l.$$
Putting all this together, we have that
\[
\sqrt{P_l Q_l} - \sqrt{P'_l Q'_l} \leq 2\sqrt{\rho} (Q''_l + P''_l).
\]
Thus, 
\begin{align*}
(\sqrt{P_l} - \sqrt{Q_l})^2 - (\sqrt{P'_l} - \sqrt{Q'_l})^2 &=
   P_l + Q_l - P'_l - Q'_l - 2 \sqrt{P_l Q_l} + 2 \sqrt{P'_l Q'_l} \\
  &\geq P''_l + Q''_l - 4\sqrt{\rho}( Q''_l + P''_l) \\
  &\geq -(4\sqrt{\rho}-1) (P''_l + Q''_l) \\
  &\geq - (4\sqrt{\rho}-1)\cdot 2 C a_L H
\end{align*}
Combining these two bounds, we have
\[
\left| (\sqrt{P_l} - \sqrt{Q_l})^2 - (\sqrt{P'_l} - \sqrt{Q'_l})^2 \right| \leq C_{C, \rho} a_L H
\]
for an appropriate constant $C_{C, \rho}$. This completes step 1.\\


\textbf{Step 2:} In step 2, we verify that $\{ \bin_l \}_{l \in L_1} \cup \{ \bin_l \cap R \}_{l \in L_2} \cup \{ \bin_l \cap R \}_{l \in L_3}$ constitute a valid approximately uniform binning of $R$.  First, because $R$ is an interval, it is easy to see that $\bin_l \cap R$ is an interval as well. Secondly, $| \bin_l \cap R^c | \leq \mu\{ R^c \} \leq a_L H$. Since $\frac{1}{H} \leq L$ by assumption, we have that $\mu\{ R^c \} \leq a_L \frac{1}{L}$ and so, there exists constants $C_\bin$ such that $ \frac{C_\bin}{L} \leq | \bin_l \cap R| \leq \frac{1}{L}$.\\ 


\textbf{Step 3:}
We now turn to main step of the proof. We can bound the difference between $H$ and $H_L$ as
\begin{align*}
& \left| \sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2 
        - \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| \\
&= \left| 
   \sum_{l \in L_1}  (\sqrt{P_l} - \sqrt{Q_l})^2 
    + \sum_{l \in L_2}  (\sqrt{P_l} - \sqrt{Q_l})^2  
      + \sum_{l \in L_3}  (\sqrt{P_l} - \sqrt{Q_l})^2 
     - \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| \\
&\stackrel{(a)}\leq \left| 
   \sum_{l \in L_1}  (\sqrt{P_l} - \sqrt{Q_l})^2 
    + \sum_{l \in L_2}  (\sqrt{P_l} - \sqrt{Q_l})^2  
     - \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| + 8 C a_L H\\
&\stackrel{(b)}\leq  \left| 
   \sum_{l \in L_1}  (\sqrt{P_l} - \sqrt{Q_l})^2 
    + \sum_{l \in L_2}  (\sqrt{P'_l} - \sqrt{Q'_l})^2  
     - \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| + C_{C, \rho} a_L H\\
& \stackrel{(c)}\leq  \left| 
   \sum_{l \in L_1}  (\sqrt{P_l} - \sqrt{Q_l})^2 
    + \sum_{l \in L_2}  (\sqrt{P'_l} - \sqrt{Q'_l})^2  
    + \sum_{l \in L_3} (\sqrt{P'_l} - \sqrt{Q'_l})^2 
     - \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| + C_{C, \rho} a_L H \\
&\stackrel{(d)} \leq  \left| 
   \sum_{l \in L_1}  (\sqrt{P_l} - \sqrt{Q_l})^2 
    + \sum_{l \in L_2}  (\sqrt{P'_l} - \sqrt{Q'_l})^2
    + \sum_{l \in L_3} (\sqrt{P'_l} - \sqrt{Q'_l})^2 
     - \int_{R} (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| 
    + C_{C, \rho} a_L H \\
&\stackrel{(e)} \leq C_{C, \rho} a_L H.
\end{align*}
where $(a)$ follows because $P_l \vee Q_l \leq 2 C a_L H$ for all $l \in L_3$ and because $|L_3| \leq 2$, $(b)$ follows from step 1 and the fact that $|L_2| \leq 2$, $(c)$ follows because 
$P'_l \leq P_l$ and thus $\sum_{l \in L_3} (\sqrt{P'_l} - \sqrt{Q'_l})^2 \leq 2 C a_L H$ as well. Inequality $(d)$ follows because 
$\int_{R^c} (\sqrt{p(x)} - \sqrt{q(x)})^2 \leq C \mu\{ R^c \} = C a_L H$, and $(e)$ follows by Lemma~\ref{prop:H_HL_convergence_R}. Since $a_L \rightarrow 0$, the conclusion follows. 

\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Lemmas for Proposition~\ref{prop:discretization1}}
In this subsection, define an approximately uniform binning of an interval $R$ to be the division of $R$ into $L$ bins $[a_l, b_l]$ such that the length of each bin is bounded $\frac{C_\bin}{L} \leq b_l - a_l \leq \frac{C_\bin}{L}$ for some constants $C_\bin$ and $C_\bin$. We also use the notation $\bin_l$ to denote the bin $[a_l, b_l]$. Let $B_l = b_l - a_l$. 

\begin{lemma}
\label{prop:d_dL_convergence}
Let $d_L = \sum_l Q_l' \left( \frac{\gamma_l'}{Q_l} \right)^2$. Suppose assumptions C1-C5 hold. Then we have that
\[
\lim_{L \to \infty} d_L = 1/4.
\]
\end{lemma}


\begin{proof}
Let $h(x)$ be as defined in Assumption C3; in particular, $|h(x)| \geq 
\left| \frac{\gamma'(x)}{q(x)} \right| \vee \left| \frac{q'(x)}{q(x)} \right|$.  Let $0 < \tau < 1$. We say that a bin $l$ is good if
$$
\sup_{x \in \bin_l} |h(x)| \leq L^\tau
$$
the exponent $\tau$ will be chosen later to balance two error terms. We will now argue that the proportion of bad bins goes to 0 as $L \rightarrow \infty$. By assumption C4, for all large enough $L$, $\left \{x \,:\, |h(x)|  \geq L^\tau \right \}$ is a union of at most $K_h$ intervals. Thus, we have that
\begin{align*}
\sum_{l \in \left \{ l \,:\, |h(x)| 
           \geq L^\tau \right \}}  B_l &\leq 
   \mu \left( \left\{x \,:\, |h(x)|
         \geq L^\tau \right\} \right) + 2 K_h C_\bin L^{-1} \\
  &\stackrel{(a)}\leq M' L^{-\tau t}  + 2K_h C_\bin L^{-1} \\
  & \stackrel{(b)}\leq C_{M', K} L^{ - \tau t}.
\end{align*}
Here $(a)$ follows by the finiteness of the integral $\int_R |h(x)|^t dx$ given by assumption C4, and $(c)$ follows from because $t \leq 1$ by assumption C4, and $\tau t < 1$ by our selection, and so the first term dominates. 
We can now bound the number of bad bins: 
\begin{align*}
\# \{ l \,:\, |h(x)| \geq L^\tau \} \leq \frac{C_{M', K} L^{- \tau t} L}{C_\bin}  \leq C_{M',K} L^{1 - \tau t},
\end{align*}
where we redefine the constant $C_{M',K}$ suitably. For a bad bin $l$, we can bound $Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2$ as follows:
\begin{align*}
Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 &= Q_l' \left( \frac{1}{Q_l'} \int_{\bin_l} \gamma(x) dx \right)^2 \\
   &= Q_l' \left(  \int_{\bin_l} \frac{\gamma(x)}{q(x)} \frac{q(x)}{Q_l'} dx \right)^2 \\
   &\stackrel{(a)} \leq Q_l' \int_{\bin_l} \frac{q(x)}{Q_l'} \left( \frac{\gamma(x)}{q(x)} \right)^2 dx\\
   &\leq \int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx \\
   &\stackrel{(b)}\leq \left(\int_{\bin_l} q(x) \left| \frac{\gamma(x)}{q(x)} \right|^r dx \right)^{2/r}
         \left(\int_{\bin_l} q(x) dx \right)^{(r-2)/r} \\
   &\stackrel{(c)} \leq M^{2/r} (C C_\bin)^{(r-2)/r} L^{-(r-2)/r} = C_{M,C} L^{-(r-2)/r}.\\
\end{align*}
Here $(a)$ follows from Jensen's inequality, $(b)$ follows from Holder's inequality, and $(c)$ follows from the finiteness of the integral as per assumption C3. Now, we have
\begin{align*}
d_L &= \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 \\
  &= \sum_{l \, good} Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 + 
      \sum_{l \, bad} Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 \\
   &\leq   \sum_{l \, good} Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 + 
       C_{M, C} L^{-(r-2)/r} | \{ l \,:\, l \trm{ bad} \}| \\
   &\leq  \sum_{l \, good} Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 + 
          C_{M, M', C, K} L^{1 - \tau t - \frac{(r-2)}{r}} \\
  &=  \sum_{l \, good} Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 + 
        C_{M, M', C, K} L^{\frac{2}{r} - \tau t}. 
\end{align*}
For each good bin $l$, define $x_l = \argmax_{x \in \bin_l} | q(x) |$. The maximum is attainable since $q$ is continuous and $q(x_l) < \infty$ since $q$ is bounded. Now, for a good bin, we have that 
\begin{align*}
Q_l' &= \int_{\trm{Bin}_l} q(x) dx \\
  &= \int_{a_l}^{b_l} q(x) dx \\
 &= \int_{a_l}^{b_l} q(x_l) + q'(c_x) (x - x_l) dx \quad \trm{for some $c_x \in [a_l, b_l]$}\\
 &= B_l q(x_l) + \int_{a_l}^{b_l} q'(c_x)(x-x_l) dx  \\
 &= B_l q(x_l) + B_l^2 \xi_l,
\end{align*}
where we define $\xi_l = \frac{1}{B_l^2} \int_{a_l}^{b_l} q'(c_x) (x-x_l) dx$. We can bound $B_l \left| \frac{\xi_l}{q(x_l)} \right|$:
$$
B_l \left| \frac{\xi_l}{q(x_l)} \right| 
    \leq \frac{1}{B_l} \int_{a_l}^{b_l} \left|\frac{q'(c_x)}{q(x_l)} \right| |x - x_l| dx 
   \leq \frac{1}{B_l} \int_{a_l}^{b_l} \left|\frac{q'(c_x)}{q(c_x)} \right| |x - x_l| dx 
                \leq \frac{1}{B_l} \int_{a_l}^{b_l} L^{\tau} |x - x_l| dx \leq 
 \frac{1}{2} C_\bin L^{\tau - 1}. 
$$ 
The second inequality follows because $q(c_x) \leq q(x_l)$. The third inequality follows because $l$ is a good bin and thus
$\left| \frac{q'(c_x)}{q(c_x)} \right| \leq L^\tau$. The last inequality follows because $B_l \leq C_\bin 1/L$. We perform similar analysis on $\gamma$:
\begin{align*}
\gamma_l' &= \int_{\bin_l} \gamma(x) dx \\
     &=  \int_{a_l}^{b_l} \gamma(x_l) + \gamma'(c_x) (x - x_l) dx  \\
     &= B_l \gamma(x_l) + B_l^2 \xi'_l,
\end{align*}  
where $\xi'_l = \frac{1}{B_l^2} \int_{a_l}^{b_l} \gamma'(c_x)(x - x_l) dx$. It is straightforward to verify that $ B_l \left| \frac{\xi'_l}{q(x_l)}\right| \leq \frac{1}{2} C_\bin L^{\tau - 1}$. For any bin $l$, we also have that
\begin{align*}
Q_l' = \int_{\bin_l} q(x) dx \leq C B_l,
\end{align*}
where $C$ is the bound on $p(x) \vee q(x)$. Now we look at a single $Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2$ term for a single good bin $l$:
\begin{align*}
Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 &= \frac{\gamma_l'^2}{Q_l'} \\
   &= \frac{ (B_l \gamma(x_l) + B_l^2 \xi'_l)^2}{B_l q(x_l) + B_l^2 \xi_l} \\
   &= B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} + B_l \frac{\xi'_l}{q(x_l)} \right)^2 
         \left( \frac{1}{1 + B_l \frac{\xi_l}{q(x_l)} } \right) \\
   &=  B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} + B_l \frac{\xi'_l}{q(x_l)} \right)^2 
         \left( 1 - B_l \frac{\xi_l}{q(x_l)} + \eta_l ( B_l \frac{\xi_l}{q(x_l)} )^2 \right).
\end{align*}
To arrive at the last equality, we assume that $L \geq  C_\bin^{1/(1-\tau)}$. Under this assumption, $\left| B_l \frac{\xi'_l}{q(x_l)} \right| \leq \frac{1}{2}$ and thus it is valid to take the Taylor approximation. Here, $\eta_l$ is some scalar that satisfies $|\eta_l| \leq 16$.
Expanding the right hand side,
\begin{align*}
& Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 =  \\
 &  \left( B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 + 
          2B_l q(x_l) \frac{\gamma(x_l)}{q(x_l)} B_l \frac{\xi'_l}{q(x_l)} +
          B_l q(x_l) \left( B_l \frac{\xi'_l}{q(x_l)} \right)^2 \right) 
       \left( 1 - B_l \frac{\xi_l}{q(x_l)} + \eta_l ( B_l \frac{\xi_l}{q(x_l)} )^2 \right).
\end{align*}
We again note that $\left| B_l \frac{\xi'_l}{q(x_l)} \right| \leq \frac{C_\bin}{2} L^{\tau-1}$ and 
$\left| B_l \frac{\xi_l}{q(x_l)} \right| \leq \frac{C_\bin}{2} L^{\tau-1}$. Suppose $L \geq (2C_\bin)^{1/(1-\tau)}$ so that $\frac{C_\bin}{2} L^{\tau - 1} \leq \frac{1}{4}$, then
\begin{align*}
 \left| B_l \frac{\xi_l}{q(x_l)} \right|  + \left| \eta_l ( B_l \frac{\xi_l}{q(x_l)} )^2 \right| &\leq C_\bin L^{\tau - 1}, \text{ and } \\
 \left| 1 - B_l \frac{\xi_l}{q(x_l)} + \eta_l ( B_l \frac{\xi_l}{q(x_l)} )^2 \right| &\leq 2.
\end{align*}
Now, we can bound
\begin{align*}
& \left| Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 
         - B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| \\
& \leq B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 C_\bin L^{\tau - 1} +
       2 B_l q(x_l) \frac{\gamma(x_l)}{q(x_l)}C_\bin  L^{\tau - 1} +
        B_l q(x_l) C_\bin^2 L^{2(\tau-1)}. 
\end{align*}
The third term is bounded by $C_1 L^{2\tau - 3}$ for a suitable constant $C_1$. To bound the second term, we perform case analysis. 

Case 1: $\left|\frac{\gamma(x_l)}{q(x_l)}\right| \geq 1$. In this case, 
$q(x) \left| \frac{\gamma(x_l)}{q(x_l)} \right| 
  \leq q(x) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 $.

Case 2: $\left| \frac{\gamma(x_l)}{q(x_l)} \right| \leq 1$. Then, the second term is bounded by $2 B_l CC_\bin L^{\tau - 1 } \leq C_2 L^{\tau - 2}$ for some constant $C_2$

In any case, we have that
\begin{align*}
& \left| Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 
         - B q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| 
  \leq C_3 B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2  L^{\tau - 1} +
        C_4 L^{\tau - 2}.
\end{align*}
Define $d_R = \sum_{l \, good} B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2$. Then,
\begin{align*}
  |d_L - d_R| &= \left| \sum_l Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 - \sum_{l \, good} B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| \\
  &\leq \sum_{l \, good} \left| Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 - B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| + C_{M, M', K, C} L^{\frac{2}{r} - \tau t} \\
  &\leq C_3 d_R L^{\tau - 1}
       + L \cdot C_4 L^{\tau - 2}  + C_5 L^{\frac{2}{r} - \tau t} \\
  &\leq  C_3 d_R L^{\tau - 1} 
       +   C_4 L^{\tau - 1}  + C_{5} L^{\frac{2}{r} - \tau t} \\
 &\leq C_3 d_R L^{\frac{2 - rt}{r(1+t)}} 
       +   C_{6} L^{\frac{2 - rt}{r(1+t)} }
                     \quad \trm{setting $\tau= \frac{2 + r}{r(1+t)}. $} 
\end{align*}

The $\tau$ is chosen to balance $L^{\tau - 1}$ and $L^{2/r - \tau t}$. Notice that $0 < \tau < 1$ by assumption C4, which says $rt > 2$. Furthermore, since $2 > rt$, we have that $$|d_L - d_R| = o(d_R) + o(1).$$

In like fashion, we bound $| d_R - d|$. We use the same definition of good and bad bins as before, and obtain 
\begin{align*}
d &= \int_R q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx \\
  &= \sum_{l=1}^L \int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx \\
  &= \sum_{l \, good} \int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx +
     \sum_{l \, bad} \int_{\bin_l}  q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx \\
  &\leq \sum_{l\, good} \int_{\bin_l}  q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx +
        |\{ l \,:\, l \, \trm{bad}\}| C_{M,C} L^{-\frac{2}{r}} \\
 & \leq \sum_{l\, good} \int_{\bin_l}  q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx +
        C_{M,M',C,K} L^{\frac{2}{r} - \tau t}. 
\end{align*}
The bound on the second term--the inequality--follows from the previous analysis. We now focus on the first term. Note that, for all $x \in \bin_l$

\begin{align*}
q(x) &= q(x_l) + q'(c_l)(x-x_l)  \\
\gamma(x) &= \gamma(x_l) + \gamma'(c'_l)(x - x_l) 
\end{align*}
The points $c_l, c'_l$ are in $\bin_l$ and they are dependent on $x$; we leave that dependency implicit to make the notations simpler. For $\bin_l$, we have
\begin{align*}
\int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 &=
  \int_{\bin_l} 
      \frac{ (\gamma(x_l) + \gamma'(c'_l)(x - x_l))^2  }{ q(x_l) + q'(c_l)(x-x_l)} dx
  \\
&= \int_{\bin_l} q(x_l)
   \left( \frac{\gamma(x_l)}{q(x_l)} + \frac{\gamma'(c'_l)}{q(x_l)} (x - x_l) \right)^2
  \left( \frac{1}{1 + \frac{q'(c_l)}{q(x_l)} (x - x_l)} \right) dx.
\end{align*}
To make the algebraic manipulation more clear, let us use the following shorthand:
\[
T_1 = \frac{q'(c_l)}{q(x_l)} ( x - x_l) \quad
T_2 = \frac{\gamma'(c'_l)}{q(x_l)} ( x - x_l) . 
\]
We observe that $|x -x_l| \leq B_l$ and that
\[
\left| \frac{\gamma'(c'_l)}{q(x_l)} \right| 
    \leq \left| \frac{\gamma'(c'_l)}{q(c'_l)} \right| \leq L^\tau,
\]
and likewise,  $\left| \frac{q'(c_l)}{q(x_l)} \right| \leq L^\tau$. Thus, we have that $|T_1|, |T_2| \leq C_\bin L^{\tau -1}$. Now, suppose $C_\bin L^{\tau -1} \leq \frac{1}{2} $, which is satisfied if $L \geq (2 C_\bin)^{\frac{1}{1-\tau}}$. We obtain
\begin{align*}
\int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx &= \int_{\bin_l} q(x_l) 
       \left( \frac{\gamma(x_l)}{q(x_l)} + T_2 \right)^2 
        \left( \frac{1}{ 1 + T_1 } \right) dx  \\
 =& \int_{\bin_l} \left( q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 + 
                q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right) T_2 + q(x_l) T_2^2 \right) 
    (1 - T_1 + \eta T_1^2) dx,
\end{align*}
where $\eta$ is some function of $x$ that satisfies $|\eta| \leq 16$. Thus, we have that
\begin{align*}
& \left| \int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx - 
      B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| \\
& \leq   B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 C_\bin L^{\tau-1} +
      B_l q(x_l) \frac{\gamma(x_l)}{q(x_l)} C_\bin L^{\tau-1} + B_l q(x_l) C_\bin^2 L^{2(\tau-1)}. 
\end{align*}
Analyzing exactly as in the case of $|d_L-d_R|$, we conclude that
$$|d - d_R| = o(d_R) + o(1).$$ 
Since $d = 1/4$, we have that $d_R \to 1/4$, which in turn implies $d_L \to 1/4$.  This completes the proof.
\end{proof}
%Since $q(x_l) \leq C$, the $q(x_l) L^{2(\tau-1)}$ term is bounded by $C L^{2(\tau - 1)}$. To bound the $q(x_l) \frac{\gamma(x_l)}{q(x_l)} L^{\tau-1} $ term, we perform case analysis, exactly as before:

%Case 1: if $\frac{\gamma(x_l)}{q(x_l)} \geq 1$. Then $q(x_l) \frac{\gamma(x_l)}{q(x_l)} L^{\tau-1} \leq q(x_l) \left(\frac{\gamma(x_l)}{q(x_l)} \right)^2 L^{\tau-1}$. 

%Case 2: if $\frac{\gamma(x_l)}{q(x_l)} \leq 1$. Then $q(x_l) \frac{\gamma(x_l)}{q(x_l)} L^{\tau-1} \leq C L^{\tau-1}$.\\ 

%Thus, in any case, we have that, for a single good bin:
%\begin{align*}
%& \left| \int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx - 
%      B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| \\
%& \leq   B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 L^{\tau-1} +
%      B_l C L^{\tau-1} 
%\end{align*}

%\[
%|d - d_R | \leq d_R L^{\tau-1} + C L^{\tau-1} + C_{M, M', C, K} L^{\frac{2}{r} - \frac{\tau}{t}}
%\]
%%%%%%%%%%%%%%%%%%

\begin{lemma}
\label{prop:continuous_hellinger_chi_square}
Let $H^R = \int_R (\sqrt{p(x)} - \sqrt{q(x)})^2 dx$, $\delta(x) = q(x) - p(x)$, $ \alpha^2 =  \int_R q(x) \left( \frac{\delta(x)}{q(x)} \right)^2 dx$, and $\gamma(x) = \frac{\delta(x)}{\alpha}$. Suppose that assumptions C1-C5 hold. Then, we have that, 
\[
H^R = \frac{\alpha^2}{4}(1 + \eta ),
\]
where $|\eta| \leq C(\alpha + \alpha^2)$ for some constant $C$. In particular, we have that if $H^R \rightarrow 0$, then $\alpha \rightarrow 0$ and $\eta \rightarrow 0$. 
\end{lemma}

\begin{proof}
We write $H^R$ as
\begin{align*}
H^R = &\int_R (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \\
=& \int_R ( \sqrt{q(x)} - \sqrt{q(x) - \delta(x)} )^2 dx \\
=& \int_R q(x) \left( 1 - \sqrt{ 1 - \frac{\delta(x)}{q(x)}} \right)^2 dx. 
\end{align*}
By convention, we let $\frac{\delta(x)}{q(x)} = 0$ whenever $q(x), p(x) = 0$. Thus, we can define $\xi(x) = 1- \frac{1}{2} \frac{\delta(x)}{q(x)} - \sqrt{ 1 - \frac{\delta(x)}{q(x)}}$ for $x \in [0,1]$ and rewrite
\begin{align*}
H^R =& \int_R q(x) \left( 1 - (1 - \frac{1}{2} \frac{\delta(x)}{q(x)} + \xi(x) ) \right)^2 dx \\
=& \int_R q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} + \xi(x) \right)^2 dx\\ 
=& \int_R q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} \right)^2 \left( 1 + \xi_2(x) \right)^2 dx, 
\end{align*}
qhere $\xi_2(x) = \frac{2\xi(x)}{\delta(x)/q(x)}$ if $\delta(x) \neq 0$ and $\xi_2(x) =0$ if $\delta(x) = 0$. Thus,
\begin{align*}
\int_R \left( \sqrt{p(x)} - \sqrt{q(x)} \right)^2 dx &= (1 + \eta)\frac{\alpha^2}{4},
\end{align*}
where 
\[
\eta = \frac{\int_R q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} \right)^2 (\xi_2(x)^2 + 2\xi_2(x)) dx }
           { \alpha^2/4} =  \int_R q(x) \left(  \frac{\gamma(x)}{q(x)} \right)^2 (\xi_2(x)^2 + 2\xi_2(x)) dx.
\]
By Lemma~\ref{lem:sqrt_linearize}, we have that $\xi_2(x) \leq 2 \left| \frac{ \delta(x)}{q(x)} \right|$. This gives
\begin{align*}
| \eta | &\leq  \int_R q(x) \left(  \frac{\gamma(x)}{q(x)} \right)^2   \left( 4\left|\frac{\delta(x)}{q(x)} \right|^2 + 4\left| \frac{\delta(x)}{q(x)} \right| \right) dx\\
%
&= 4\alpha^2 \int_R q(x) \left|  \frac{\gamma(x)}{q(x)} \right |^4 dx + 4\alpha \int_R q(x) \left |  \frac{\gamma(x)}{q(x)} \right|^3 dx\\
%
    &\leq C (\alpha^2 + \alpha),
\end{align*}
using the finiteness of integrals in assumption C3.
\end{proof}


%%%%%%%%%%%%%%%%%


\begin{lemma}
\label{prop:discrete_hellinger_chi_square}
Let $H^R_L = \sum_{l=1}^L \left( \sqrt{P_l'} - \sqrt{Q_l'} \right)^2$. Let $\delta(x) = q(x) - p(x)$, $
\alpha^2 =  \int_R q(x) \left( \frac{\delta(x)}{q(x)} \right)^2 dx$, and $\gamma(x) = \frac{\delta(x)}{\alpha} dx$. Suppose that assumptions C1-C5 hold. Then, we have that, 
\[
H^R_L = d_L ( 1 + \eta_L )
\]
where $d_L = \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 dx$, $\gamma_l' = \frac{Q_l'-P_l'}{\alpha}$ and $\sup_L |\eta_L | \leq C(\alpha + \alpha^2)$ for some constant $C$.%In particular, we have that if $\alpha \rightarrow 0$, then $\eta_L \rightarrow 0$. 
\end{lemma}

\begin{proof}

Let us define $\delta_l = Q_l'-P_l'$. We write $H^R_L$ as
\begin{align*}
H^R_L &= \sum_{l=1}^L (\sqrt{P_l'} - \sqrt{Q_l'})^2 \\
   &= \sum_{l=1}^L Q_l' \left( 1 - \sqrt{\frac{P_l'}{Q_l'}} \right)^2 \\
 &= \sum_{l=1}^L Q_l' \left( 1 - \sqrt{1 - \frac{\delta_l}{Q_l'}} \right)^2 \\
 &= \sum_{l=1}^L Q_l' \left( 1 - \left( 1 - \frac{1}{2} \frac{\delta_l}{Q_l'} - \xi_l \right) \right)^2, 
\end{align*}
where by convention, we define $\frac{\delta_l}{Q_l'} = 0$ when $Q_l', P_l' = 0$ and where $\xi_l = 1 - \frac{1}{2} \frac{\delta_l}{Q_l'} - \sqrt{ 1 - \frac{\delta_l}{Q_l'} }$. Continuing,
\begin{align*}
H^R_L &=  \sum_{l=1}^L Q_l' \left(  \frac{1}{2} \frac{\delta_l}{Q_l'} + \xi_l \right)^2 \\
  &=  \sum_{l=1}^L Q_l' \left(  \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2 \left( 1 + \xi_{2l} \right)^2, 
\end{align*}
where $\xi_{2l} = 0$ if $\frac{\delta_l}{Q_l'} = 0$ and $\xi_{2l} = 2 \xi_l \frac{Q_l'}{\delta_l}$ otherwise. This may also be written as
\begin{align*}
H^R_L &= (1 + \eta_L) \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2,
\end{align*}
where $\eta_L = \frac{ \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2 (2 \xi_{2l} + \xi_{2l}^2) }{ \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2 }$. By Lemma~\ref{lem:sqrt_linearize}, we have the inequality $|\xi_{2l}| \leq 2 \left| \frac{\delta_l}{Q_l'} \right|$. Therefore,
\begin{align*}
|\eta_L| &= \left| \frac{ \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2 (2 \xi_{2l} - \xi_{2l}^2) }
          { \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2 } \right| \\
   &\leq  \frac{ \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 (2 |\xi_{2l}| + \xi_{2l}^2) }
          { \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 } \\
  &\leq   4 \frac{ \alpha \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^3 +  
                   \alpha^2 \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^4 }
          { \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 }.
\end{align*}
The denominator tends to $1/4$ by Lemma~\ref{prop:d_dL_convergence} and can be bounded by $1/(2C')$ for large enough $L$ . To bound the numerator, we note that for a single $l$:
\[
\int_{a_l}^{b_l} \frac{q(x)}{Q_l'} \left| \frac{\gamma(x)}{q(x)} \right|^3 dx \geq
 \left| \int_{\bin_l} \frac{q(x)}{Q_l'} \frac{\gamma(x)}{q(x)} dx \right|^3 =
 \left| \frac{\gamma_l'}{Q_l'} \right|^3.
\]
Therefore,
\begin{align*}
\sum_{l=1}^L Q_l' \left| \frac{\gamma_l'}{Q_l'} \right|^3 &\leq \int_R q(x) \left| \frac{\gamma(x)}{q(x)} \right|^3 \leq M  \text{ and }\\
\sum_{l=1}^L Q_l' \left| \frac{\gamma_l'}{Q_l'} \right|^4 &\leq \int_R q(x) \left| \frac{\gamma(x)}{q(x)} \right|^4 \leq M.
\end{align*}
Thus, 
\begin{align*}
|\eta_L| &\leq (2 \alpha + \alpha^2) 2 C' M.
\end{align*}
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%



\begin{lemma}
\label{prop:H_HL_convergence_R}
Suppose assumptions A1-4 hold. Let $n \rightarrow \infty$, then for any sequence $L_n \rightarrow \infty, \alpha_n \rightarrow 0$ we have $H^R_L = H^R(1+o(1))$; i.e.,
\[
\lim_{n \rightarrow \infty} \left| \frac{\sum_l (\sqrt{P_l'} - \sqrt{Q_l})^2}{\int_R (\sqrt{p(x)} - \sqrt{q(x)})^2 dx} 
        - 1 \right| \rightarrow 0.
\]

\end{lemma}

\begin{proof}
By Lemma~\ref{prop:continuous_hellinger_chi_square} and Lemma~\ref{prop:discrete_hellinger_chi_square}, we have that, for all $\alpha$,
\begin{align*}
| H^R_L - H^R | & = \left| d_L \alpha^2 ( 1 + \eta_L) -  \frac{\alpha^2}{4} ( 1 + \eta) \right|,
 \end{align*}
which implies
\begin{align*}
\left| \frac{H^R_L}{H^R} - 1 \right| &= \left |4d_L \frac{(1+\eta_L)}{1 + \eta} - 1 \right|,
\end{align*}
where $|\eta|, |\eta_L| \leq C_1(\alpha + \alpha^2)$ for all $L$. Thus, it is clear that
\[
\lim_{\alpha_n \rightarrow 0} \sup_L \left| \frac{1+\eta_L}{1+\eta} - 1 \right| = 0.
\]
Furthermore, by Lemma~\ref{prop:d_dL_convergence},
\[
\lim_{L_n \rightarrow \infty} \left| 4d_L - 1 \right| = 0,
\]
uniformly for all $\alpha$. From these two observations, it is clear that $\left| \frac{H^R_L}{H^R} - 1 \right| \to 0$, which completes the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%










\section{Proof of Proposition~\ref{prop:discretization2}}

\begin{repproposition}{prop:discretization2}
Suppose $p(x), q(x)$ are supported on $[0,1]$.

\begin{enumerate}
\item[C1'] Suppose $p(x), q(x) \leq C$ on $[0,1]$ and are absolutely continuous.
\item[C2'] For some $r > 2$, $\sup_n \int \left| \log \frac{p(x)}{q(x)} \right|^r dx < \infty$.
\item[C3'] Let $h(x) \geq \sup_n \max \left\{  \left|\frac{p'(x)}{p(x)} \right|, 
 \left|\frac{q'(x)}{q(x)}\right|  \right\} $. Suppose $\int |h(x)|^t dx \leq M'$ for some constant $M'$ and $1 \geq t \geq 2/r$. Suppose also that the level set $\{x \,:\, |h(x)| \geq \kappa\}$ is a union of at most $K_h$ intervals for all large enough $\kappa$.  
\item[C4']  $p'(x), q'(x) \geq 0$ for all $x < c'$ and $ p'(x), q'(x) \leq 0$ for all $x > 1-c'$ for some $c' > 0$. 
\end{enumerate}
Let $L$ be a sequence such that $L \rightarrow \infty$. Suppose $\frac{1}{c_0} \leq \frac{1 - P_0}{1-Q_0} \leq c_0$. Let $\bin_l = [a_l, b_l]$ for $l=1,...,L$ be a uniformly spaced binning of the interval $[0,1]$ and let $\tilde P_l = (1- P_0) \int_{a_l}^{b_l} p(x) dx$ and $\tilde Q_l = (1-Q_0)\int_{a_l}^{b_l} q(x) dx$. Define $I = -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0) p(x) q(x)} dx \right)$ and $I_L = -2 \log \left( \sqrt{P_0 Q_0} + \sum_{l=1}^L \sqrt{\tilde P_l \tilde Q_l} \right)$. Then, we have that
 $$\left| \frac{I - I_L}{I} \right| = o(1),$$ 
and that $ \exp(-C L^{1/r}) \leq \frac{\tilde P_l}{\tilde Q_l} \leq \exp(C L^{1/r})$ for all $l$. 
\end{repproposition}

\begin{proof}


First we prove the bounds on $\frac{\tilde P_l}{\tilde Q_l}$. Define 

$$R = \left \{ x \in [0,1] \,:\, \left| \log \frac{p(x)}{q(x)} \right| \leq C (2L)^{1/r} \right \}$$. 

where $C = \left( \int \left| \log \frac{p(x)}{q(x)} \right|^r dx \right)^{1/r}$ is a constant. 

Since $\int \left| \log \frac{p(x)}{q(x)} \right|^r dx < \infty$, we have, by Markov Inequality, that
\[
\mu\{ R^c \} \leq \frac{1}{2L} 
\]

The rest of the proof proceeds exactly as in Proposition~\ref{prop:discretization2}, and we omit it here. 


The proof  also follows that of Proposition~\ref{prop:discretization1}, except the final step where we need to show that
$$\left| \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx - \sum_l (\sqrt{P_l} - \sqrt{Q_l})^2 \right| = o(\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx) = o(1).$$
We establish this fact in Proposition~\ref{prop:H_HL_convergence2} in Appendix~\ref{appendix: another saturday}.
\end{proof}


\subsection{Appendix for Proposition~\ref{prop:discretization2}} \label{appendix: another saturday}
\begin{proposition}
\label{prop:H_HL_convergence2}
Let assumptions C1', C3' be satisfied. Let $\bin_l = [a_l, b_l]$ be a uniform binning of $[0,1]$ for $l=1,..,L$ and let $P_l = \int_{\bin_l} p(x) dx$ and $Q_l = \int_{\bin_l} q(x) dx$. Then, we have that
\[
\left| \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx - \sum_l (\sqrt{P_l} - \sqrt{Q_l})^2 \right| \rightarrow 0.
\]
\end{proposition}



\begin{proof}
The proof will be similar to that of Proposition~\ref{prop:d_dL_convergence}.  First, we observe that $\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = \int p(x) dx + \int q(x) dx - 2 \int \sqrt{p(x)q(x)} dx = 2 - 2 \int \sqrt{p(x)q(x)}$. And, that $\sum_l (\sqrt{P_l} - \sqrt{Q_l})^2 = \sum_l P_l + \sum_l Q_l - 2 \sum_l \sqrt{P_l Q_l}$. Thus, we need only show that 
\[
\left| \int \sqrt{p(x)q(x)} dx - \sum_l \sqrt{P_l Q_l} \right| \rightarrow 0.
\]
We have that $|h(x)| \geq \left| \frac{p'(x)}{p(x)} \right| \vee \left| \frac{q'(x)}{q(x)} \right|$. Let $0 < \tau < 1$. We say that a bin $l$ is good if
$$
\sup_{x \in \bin_l} |h(x)| \leq L^\tau,
$$
the exponent $\tau$ will be chosen later to balance two error terms. We will now argue that the proportion of bad bins goes to 0 as $L \rightarrow \infty$. For all large enough $L$, $\left \{x \,:\, |h(x)|  \geq L^\tau \right \}$ is a union of at most $K_h$ intervals, thus, we have that
\begin{align*}
\sum_{l \in \left \{ l \,:\, \sup_{x \in \bin_l} |h(x)| 
           \geq L^\tau \right \}}  B_l &\leq 
   \mu \left( \left\{x \,:\, \sup_{x \in \bin_l} |h(x)|
         \geq L^\tau \right\} \right) + 2 K_h C_\bin L^{-1} \\
  &\leq M' L^{-\tau t}  + 2K_h C_\bin L^{-1} \\
  & \leq C_{M', K_h} L^{ - \tau t}.  
\end{align*}
The last inequality follows because $t \leq 1$ and thus $\tau t < 1$ and so the first term dominates. The second inequality follows from the assumption C3'. We can now bound the number of bad bins: 
\begin{align*}
\# \{ l \,:\, |h(x)| \geq L^\tau \} \leq \frac{C_{M', K} L^{- \tau t} L}{C_\bin}  \leq C_{M',K} L^{1 - \tau t}.
\end{align*}
For a bad bin, we can bound $P_l, Q_l \leq C C_\bin L^{-1}$ and $\int_{\bin_l} (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \leq 2 L^{-1} C C_\bin  $.

Now we consider a good bin $l$. Let $x_l$ be $\argmax_{x \in \bin_l} p(x)$. The argmax is attainable since $p$ is continuous and $p(x_l) < \infty$ since $p$ is bounded.
\begin{align*}
P_l &= \int_{a_l}^{b_l} p(x) dx \\
   &= \int_{a_l}^{b_l} p(x_l) + p'(c_x)(x-x_l) dx \\
   &= B_l p(x_l) + B_l^2 \xi_l,
\end{align*}
where $\xi_l = \frac{1}{B_l^2} \int_{a_l}^{b_l} p'(c_x)(x - x_l)dx$. We can bound $B_l \left| \frac{\xi_l}{p(x_l)} \right|$:
$$
B_l \left| \frac{\xi_l}{p(x_l)} \right| 
    \leq \frac{1}{B_l} \int_{a_l}^{b_l} \left|\frac{p'(c_x)}{p(x_l)} \right| |x - x_l| dx 
   \leq \frac{1}{B_l} \int_{a_l}^{b_l} \left|\frac{p'(c_x)}{p(c_x)} \right| |x - x_l| dx 
                \leq \frac{1}{B_l} \int_{a_l}^{b_l} L^{\tau} |x - x_l| dx \leq 
 \frac{1}{2} C_\bin L^{\tau - 1}. 
$$ 
Likewise, define $x'_l = \argmax_{x \in \bin_l} q(x)$. We have that
\[
Q_l = B_l q(x'_l) + B_l^2 \xi'_l,
\]
where $\xi'_l = \frac{1}{B_l} \int_{a_l}^{b_l} q'(c_x) (x - x'_l) dx$. We can also bound 
\[
B_l \left| \frac{\xi'_l}{q(x'_l)} \right| \leq \frac{1}{2}  L^{\tau - 1}.
\]
Thus, we have that
\begin{align*}
 \sqrt{P_l Q_l} &= \sqrt{ (B_l p(x_l) + B_l^2 \xi_l) 
                           (B_l q(x'_l) + B_l^2 \xi'_l) } \\
    &= \sqrt{p(x_l) q(x'_l)} \sqrt{ (B_l + B_l^2 \frac{\xi_l}{p(x_l)} ) 
                                           (B_l + B_l^2 \frac{\xi'_l}{q(x'_l)} ) } \\
   &=  \sqrt{p(x_l) q(x'_l)} B_l \sqrt{ (1 + B_l \frac{\xi_l}{p(x_l)} ) 
                                           (1 + B_l \frac{\xi'_l}{q(x'_l)} ) }.
\end{align*}
By our bounds on $ B_l \frac{\xi_l}{p(x_l)} $ and $B_l  \frac{\xi'_l}{q(x'_l)}$, we can bound the nuisance term 
\begin{align*}
 \sqrt{ (1 + B_l \frac{\xi_l}{p(x_l)} ) 
         (1 + B_l \frac{\xi'_l}{q(x'_l)} ) } &\leq \sqrt{ 1 + C_\bin L^{\tau - 1} (1+o(1))} \\
   &\leq 1 + \frac{1}{2}  L^{\tau - 1} (1 + o(1)).
\end{align*}
It is clear that $B_l \sqrt{p(x_l)q(x'_l)} \leq B_l C$. Therefore, we have that
\begin{align}
\label{eqn:discrete_riemann_bound2}
\left| \sqrt{ P_l Q_l} -  \sqrt{ p(x_l) q(x'_l)} B_l \right| \leq 
     B_l C  L^{\tau - 1}(1 + o(1))
\end{align}
Likewise, we have that 
\begin{align*}
\int_{a_l}^{b_l} \sqrt{p(x) q(x)}dx &= \int_{a_l}^{b_l} \sqrt{p(x) q(x)} dx \\
                      &= \int_{a_l}^{b_l} \sqrt{ (p(x_l) + p'(c_x)(x - x_l))
                                      (q(x'_l) + q'(c'_x)(x - x'_l)) } dx \\
    &=\int_{a_l}^{b_l} \sqrt{ p(x_l) q(x'_l)} \left(
              \sqrt{ 1+ (x - x_l) \frac{p'(c_x)}{p(x_l)} + (x - x'_l) \frac{q'(c'_x)}{q(x'_l)} 
                     + (x - x_l)(x - x'_l) \frac{p'(c_x)}{p(x_l)} \frac{q'(c'_x)}{q(x'_l)} } \right) dx. 
\end{align*}
We have that 
\begin{align*}
\left| (x - x_l) \frac{p'(c_x)}{p(x_l)} \right| \leq B_l \left| \frac{ p'(c_x)}{p(c_x)} \right| &\leq L^{\tau - 1}, 
\end{align*}
and
\begin{align*}
\left| (x - x_l) \frac{q'(c'_x)}{q(x_l)} \right| \leq B_l \left| \frac{ q'(c'_x)}{q(c'_x)} \right| \leq L^{\tau - 1} .
\end{align*}
Therefore, we can bound the nuisance term:
\begin{align*}
 \sqrt{ 1+ (x - x_l) \frac{p'(c_x)}{p(x_l)} + (x - x'_l) \frac{q'(c'_x)}{q(x'_l)} 
                     + (x - x_l)(x - x'_l) \frac{p'(c_x)}{p(x_l)} \frac{q'(c'_x)}{q(x'_l)} } 
    &\leq \sqrt{ 1 + C_\bin L^{\tau - 1} (1 + o(1)) } \\
    &\leq 1 + \frac{1}{2} C_\bin L^{\tau - 1} (1 + o(1)). 
\end{align*}
The term $B_l \sqrt{p(x_l)q(x'_l)}$ is bounded by $B_l C$. Hence, we have
\begin{align}
\label{eqn:continuous_riemann_bound2}
\left| \int_{a_l}^{b_l} \sqrt{p(x) q(x)}dx - B_l \sqrt{p(x_l) q(x'_l)} \right| &\leq B_l C C_\bin L^{\tau - 1}
\end{align}
By combining inequalities~\eqref{eqn:discrete_riemann_bound2} and \eqref{eqn:continuous_riemann_bound2}, we have that
\[
\left| \sqrt{P_l Q_l} - \int_{a_l}^{b_l} \sqrt{p(x) q(x)} dx \right| \leq B_l C C_\bin L^{\tau - 1}.
\]
We can then complete the proof:
\begin{align*}
\left| \sum_l \sqrt{P_l Q_l} - \int \sqrt{p(x) q(x)} dx \right| &\leq
   \sum_{l \,:\, \trm{ $l$ bad}} B_l C + 
   \sum_{l \,:\, \trm{ $l$ good}} \left| \sqrt{P_l Q_l} - \int_{a_l}^{b_l} \sqrt{ p(x) q(x)} dx \right| \\
  & \leq C_{M',K} L^{ - \tau t} + \sum_{l \,:\, \trm{ $l$ good}} B_l C C_\bin L^{\tau - 1} \\
  & \leq  C_{M',K} L^{ - \tau t} + C C_\bin L^{\tau - 1}.
\end{align*}
By setting $\tau = \frac{1}{1+t}$, we get that
\[
\left| \sum_l \sqrt{P_l Q_l} - \int \sqrt{p(x) q(x)} dx \right| \rightarrow 0. 
\]
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proofs of Theorems~\ref{thm:weighted_sbm_rate1} and Theorem~\ref{thm:weighted_sbm_rate2}}
\label{sec:transformation_proof}

We now outline the proofs of Theorems~\ref{thm:weighted_sbm_rate1} and~\ref{thm:weighted_sbm_rate2}, with proofs of supporting propositions in the succeeding subsections.

\subsection{Main argument: Proof of Theorem~\ref{thm:weighted_sbm_rate1}}
\label{AppThmRate1}

First, we claim that the divergence $I$ and $H$ between $p(x), q(x)$ does not change after we apply the transformation $\Phi$. To see this, note that the transformed density $p_{\Phi}(z)$ and $q_{\Phi}(z)$, now supported over $[0,1]$, have the following form:
\[
p_{\Phi}(z) = \frac{p(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))} \quad \text{and} \quad
q_{\Phi}(z) = \frac{q(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}.
\]
Therefore, we have, by a change of variable $z = \Phi^{-1}(x)$, that
\begin{align*}
\int_\R \sqrt{p(x) q(x)} dx &= \int_0^1 \sqrt{p_{\Phi}(z) q_{\Phi}(z)} dz, \quad \text{ and} \\
\int_\R (\sqrt{p(x)} - \sqrt{q(x)})^2 dx &= \int_0^1 (\sqrt{p_{\Phi}(z)} - \sqrt{ q_{\Phi}(z)})^2 dz. 
\end{align*}
Under A1-A5, Proposition~\ref{prop:transformation1} shows that conditions C1-C5 are also satisfied. Also, under our assumption that $L = o(\frac{1}{H})$, it must be that $L \leq \frac{2}{H}$ for large enough $L$. Therefore, Proposition~\ref{prop:discretization1} applies and we can conclude that after transformation and discretization, the label probabilities $P_l, Q_l$'s satisfy 
\[
\frac{1}{2c_0 \rho} \leq \frac{P_l}{Q_l} \leq 2c_0 \rho,
\]
for all $l$.  Under our assumption that $L = o(nI)$ and the conclusion from Proposition~\ref{prop:discretization1} that $I_L = I (1 + o(1))$, we know that $L = o(n I_L)$ as well and thus, we can use Proposition~\ref{prop:labeled_sbm_rate} (with $\rho_L = 2 c_0 \rho$) to get that
\[
\lim_{n \rightarrow \infty} P \left( l(\hat{\sigma}, \sigma_0) \leq \exp \left( - \frac{ n I_L}{ \beta K} (1 + o(1)) \right) \right) \rightarrow 1.
\]
The theorem then follows from the fact that $I_L = I(1+o(1))$. 

\subsection{Main argument: Proof of Theorem~\ref{thm:weighted_sbm_rate2}}
\label{AppThmRate2}

The proof mirrors that of Theorem~\ref{thm:weighted_sbm_rate2}. First we note again that the divergence $I$ and $H$ does not change after we transform the densities $p(x), q(x)$ into $p_{\Phi}(z)$ and $q_{\Phi}(z)$ with $\Phi$. Proposition~\ref{prop:transformation2} then shows that assumptions A1'-A4' implies C1'-C4'. Therefore, Proposition~\ref{prop:discretization2} applies and we can conclude that after transformation and discretization, the label probabilities $P_l, Q_l$'s satisfy 
\[
\frac{1}{2c_0 \exp(L^{1/r})} \leq \frac{P_l}{Q_l} \leq 2c_0 \exp(L^{1/r}),
\]
for all $l$ and that $I_L = I(1+o(1))$. Therefore, we can again use Proposition~\ref{prop:labeled_sbm_rate} (with $\rho_L = 2 c_0 \exp(L^{1/r})$) to conclude that
\[
\lim_{n \rightarrow \infty} P \left( l(\hat{\sigma}, \sigma_0) \leq \exp \left( - \frac{ n I_L}{ \beta K} (1 + o(1)) \right) \right) \rightarrow 1.
\]
The theorem follows from the fact that $I_L = I(1+o(1))$. 

\subsection{Transformation Analysis}

\begin{proposition}
\label{prop:transformation1}
Let $p(x), q(x)$ be densities over $\R$ and let $\Phi \,:\, \R \rightarrow [0,1]$ be a CDF. Suppose $p(x), q(x), \Phi$ satisfy the following conditions:

\begin{enumerate}
\item[A1] Suppose $p(x), q(x) \leq C$ are absolutely continuous.  $\lim_{|x| \rightarrow \infty} \sup_n \frac{p(x) \vee q(x)}{\phi(x)} < \infty$
\item[A2] There exists $R$ a subinterval of $\R$ such that $\frac{1}{\rho} \leq \frac{p(x)}{q(x)} \leq \rho $ and $\Phi\{R^c\} = o(H)$.

\item[A3] Define $\alpha^2 = \int_R q(x) \left( \frac{p(x) - q(x)}{q(x)} \right)^2 dx$ and $\gamma(x) = \frac{q(x) - p(x)}{\alpha}$. Suppose $\int_R q(x) \left| \frac{\gamma(x)}{q(x)} \right|^r dx  \leq M$ for constants $M, r \geq 4$.

\item[A4] {Let $h(x) \geq \sup_n \max \left\{  \left|\frac{\gamma'(x)}{q(x)} \right|, 
 \left|\frac{q'(x)}{q(x)}\right|, \left| \frac{\phi'(x)}{\phi(x)}\right|, \left | \frac{\gamma(x)}{q(x)} \right|  \right\} $. Let $\int_R |h(x)|^{4t/(1-t)} \phi(x) dx \leq M'$ for some constant $M'$ and $1 \geq t \geq 2/r$. Suppose also that the level set $\{x \,:\, |h(x)| \geq \kappa\}$ is a union of at most $K_h$ intervals for all large enough $\kappa$. Suppose $\int \phi(x)^{\frac{1-t}{1+t}} dx < \infty$.}
 
\item[A5]  $(\log p)'(x), (\log q)'(x) \geq (\log \phi)'(x) \geq 0$ for all $x < -c'$ and $ (\log p)'(x), (\log q)'(x) \leq (\log \phi)'(x) \leq 0$ for all $x > c'$ for a constant $c' > 0$.
\end{enumerate}

Now we let $p_\Phi(z), q_\Phi(z)$ be the $\Phi$-transformed densities over $[0,1]$.
Then, we have that the following conditions are satisfied for $p_\Phi(z) = \frac{p(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$ and $q_\Phi(z) = \frac{q(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$:

\begin{enumerate}
\item[C1] We have $p_\Phi(z), q_\Phi(z) \leq C$ on $[0,1]$ and are absolutely continuous.
\item[C2] There exists $R_\Phi$ a subinterval of $[0,1]$ such that $\frac{1}{\rho} \leq \frac{p_\Phi(z)}{q_\Phi(z)} \leq \rho$ for all $z \in \R_\Phi$ and $\mu\{R_\Phi^c\} = o(H)$ where $\mu$ is the Lebesgue measure.
\item[C3] Define $\alpha^2 = \int_R \frac{(p_\Phi(z) - q_\Phi(z))^2}{q_\Phi(z)} dz$ and $\gamma_\Phi(z) = \frac{q_\Phi(z) - p_\Phi(z)}{\alpha}$. Then $\int_R q_\Phi(z) \left| \frac{\gamma_\Phi(z)}{q_\Phi(z)} \right|^r dz  \leq M$ for constants $M, r \geq 4$.
\item[C4] Let $h(z) = \sup_n \max \left\{  \left|\frac{\gamma'_\Phi(z)}{q_\Phi(z)} \right|, 
 \left|\frac{q'_\Phi(z)}{q_\Phi(z)}\right|  \right\} $. Then $\int_R |h(z)|^t dz \leq M'$ for some constant $M'$ and $1 \geq t \geq 2/r$. Additionally, the level set $\{z \,:\, |h(z)| \geq \kappa\}$ is a union of at most $K_h$ intervals for all large enough $\kappa$.  
\item[C5] For all large enough $L$, we have that for all $z \leq \frac{1}{L}$, $p'_\Phi(z), q'_\Phi(z) \geq 0$ and for all $z \geq (1 - \frac{1}{L})$, we have that $p'_\Phi(z), q'_\Phi(z) \leq 0$.
\end{enumerate}

\end{proposition}


\begin{proof}

\textbf{C1} follows from A1 and the condition that $\phi$ is positive and continuous. \\

To prove \textbf{C2}, assume A2 is true and let $R$ be a subinterval of $\R$ such that $\frac{1}{\rho} \leq \frac{p(x)}{q(x)} \leq \rho$. Define $R_{\Phi} = \left\{ z \in [0,1] \,:\, \Phi^{-1}(z) \in R \right\}$, that is, $\mathbf{1}_{R_\Phi}(z) = \mathbf{1}_R(\Phi^{-1}(z))$. It is then clear that $R_{\Phi}$ is an interval and that $\Phi\{ R^c \} = \mu\{ R_{\Phi}^c \}$ where $\mu$ is the Lebesgue measure. 

 
\textbf{C3} follows from A3 with a change of variable. 


Therefore, we need only prove C4 and C5. We prove that \textbf{C5} holds first. Note that 
\begin{align*}
p'_\Phi(z) = \frac{ p'(\Phi^{-1}(z))  - 
                     p(\Phi^{-1}(z)) \frac{\phi'(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))} }
           { \phi(\Phi^{-1}(z))}.
\end{align*}
Therefore, $p'_\Phi(z) \geq 0$ if and only if $p'(x) \geq p(x) \frac{\phi'(x)}{\phi(x)}$. Likewise for $q'_\Phi(z)$. Claim C5 follows. 


For \textbf{C4}, note that 
\begin{align*}
\frac{q'_\Phi(z)}{q_\Phi(z)} = \frac{ q'(\Phi^{-1}(z)) }{q(\Phi^{-1}(z))} \frac{1}{ \phi(\Phi^{-1}(z))} - 
                     \frac{ \phi'(\Phi^{-1}(z)) }{\phi(\Phi^{-1}(z))} \frac{1}{ \phi(\Phi^{-1}(z))}.
\end{align*}
By a change of variables, we have that
\begin{align*}
\int_R \left| \frac{q'_\Phi(z)}{q_\Phi(z)} \right|^t dz 
          &= \int_R \left| \frac{ q'(x) }{q(x)} \frac{1}{ \phi(x)} - 
                     \frac{ \phi'(x) }{\phi(x)} \frac{1}{ \phi(x)} \right|^t \phi(x) dx \\
   &\stackrel{(a)}\leq  \int_R \left| \frac{ q'(x) }{q(x)} \frac{1}{ \phi(x)} \right|^t \phi(x) dx + 
         \int_R  \left| \frac{ \phi'(x) }{\phi(x)} \frac{1}{ \phi(x)} \right|^t \phi(x) dx, 
\end{align*}
where $(a)$ follows because $t \leq 1$. This is finite since 
\begin{align*}
 \int_R \left| \frac{ q'(x) }{q(x)} \frac{1}{ \phi(x)} \right|^t \phi(x) dx &\leq 
       \left\{ \int_R \left| \frac{ q'(x) }{q(x)} \right|^{\frac{2t}{1-t}} \phi(x) dx \right \}^{(1-t)/2}
       \left\{ \int_R \phi(x)^{\frac{1-t}{1+t}} dx \right\}^{(1+t)/2} dx,
\end{align*}
and each of the two terms in the product are finite by A4. Finally, we also have
\begin{align*}
 \int_R \left| \frac{\gamma'_\Phi(z)}{q_\Phi(z)} \right|^t dz &= 
    \int_R \left| \frac{1}{\alpha} \frac{p'(x) - p(x) \frac{\phi'(x)}{\phi(x)} - q'(x) + q(x) \frac{\phi'(x)}{\phi(x)} }
                                       {q(x) \phi(x) } \right|^t \phi(x) dx \\
  &= \int_R \left| \frac{1}{\alpha} \frac{p'(x) - q'(x)}{q(x)} 
                       - \frac{1}{\alpha} \frac{p(x) - q(x)}{q(x)} \frac{\phi'(x)}{\phi(x)}\right|^t \left| \frac{1}{\phi(x)} \right|^t \phi(x) dx\\ 
&\stackrel{(a)} \leq \left\{  \int_R \left| \frac{1}{\alpha} \frac{p'(x) - q'(x)}{q(x)} 
                       - \frac{1}{\alpha} \frac{p(x) - q(x)}{q(x)} \frac{\phi'(x)}{\phi(x)}\right|^{2t/(1-t)} \phi(x) dx \right\}^{(1-t)/2} 
 \left\{ \int_R \phi(x)^{\frac{1-t}{1+t}} dx \right\}^{(1+t)/2}, 
\end{align*}
where we use Holder's inequality in step $(a)$. The second term is finite by A4. To show that the first term is finite, we need only show the finiteness of
\[
 \int_R \left| \frac{1}{\alpha} \frac{p'(x) - q'(x)}{q(x)} \right|^{2t/(1-t)} \phi(x) dx, 
\] 
and
\[
\int_R  \left| \frac{1}{\alpha} \frac{p(x) - q(x)}{q(x)}  \frac{\phi'(x)}{\phi(x)}\right|^{2t/(1-t)} \phi(x) dx.
\]

The former follows from A4. To bound the latter term, we use Cauchy-Schwartz inequality:
\begin{align*}
& \int_R  \left| \frac{1}{\alpha} \frac{p(x) - q(x)}{q(x)}  \frac{\phi'(x)}{\phi(x)}\right|^{2t/(1-t)} \phi(x) dx \\
& =  \left\{ \int_R  \left| \frac{1}{\alpha} \frac{p(x) - q(x)}{q(x)}  \right|^{4t/(1-t)} \phi(x) dx 
     \right\}
   \left\{    \int_R \left| \frac{\phi'(x)}{\phi(x)}\right|^{4t/(1-t)} \phi(x) dx \right\}
\end{align*}

Both of the decomposed terms are finite by A4.

\end{proof}



\begin{proposition}
\label{prop:transformation2}
Suppose that the following assumptions hold:
\begin{enumerate}
\item[A1'] Suppose $p(x), q(x) \leq C$ are absolutely continuous.  $\lim_{|x| \rightarrow \infty} \sup_n \frac{p(x) \vee q(x)}{\phi(x)} < \infty$
\item[A2'] For some $r > 2$, $\sup_n \int \left| \log \frac{p(x)}{q(x)} \right|^r \phi(x) dx < \infty$.
\item[A3'] {Let $h(x) \geq \sup_n \max \left\{  \left|\frac{p'(x)}{p(x)} \right|, 
 \left|\frac{q'(x)}{q(x)}\right|, \left| \frac{\phi'(x)}{\phi(x)}\right| \right\} $. Suppose $\int |h(x)|^{2t/(1-t)} \phi(x) dx \leq M'$ for some constant $M'$ and $1 \geq t \geq 2/r$. Suppose also that the level set $\{x \,:\, |h(x)| \geq \kappa\}$ is a union of at most $K_h$ intervals for all large enough $\kappa$. Suppose $\int \phi(x)^{\frac{1-t}{1+t}} dx < \infty$.}
\item[A4']  $(\log p)'(x), (\log q)'(x) \geq (\log \phi)'(x) \geq 0$ for all $x < -c'$ and $ (\log p)'(x), (\log q)'(x) \leq (\log \phi)'(x) \leq 0$ for all $x > c'$ for a constant $c' > 0$.
\end{enumerate}

Now we let $p_\Phi(z), q_\Phi(z)$ be the $\Phi$-transformed densities over $[0,1]$.
Then, we have that the following conditions are satisfied for $p_\Phi(z) = \frac{p(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$ and $q_\Phi(z) = \frac{q(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$:
\begin{enumerate}
\item[C1'] We have $p_\Phi(z), q_\Phi(z) \leq C$ on $[0,1]$ and are absolutely continuous.
\item[C2'] For some $r > 2$, $\sup_n \int \left| \log \frac{p_\Phi(z)}{q_\Phi(z)} \right|^r dz < \infty$.
\item[C3'] Let $h(z) = \sup_n \max \left\{  \left|\frac{p'_\Phi(z)}{p_\Phi(z)} \right|, 
 \left|\frac{q'_\Phi(z)}{q_\Phi(z)}\right|  \right\} $. Then $\int |h(z)|^t dz \leq M'$ for some constant $M'$ and $1 \geq t \geq 2/r$. Additionally, the level set $\{z \,:\, |h(z)| \geq \kappa\}$ is a union of at most $K_h$ intervals for all large enough $\kappa$.  
\item[C4']  We have that $p'_\Phi(z), q'_\Phi(z) \geq 0$ for all $z < c'$ and $p'_\Phi(z), q'_\Phi(z) \leq 0$ for all $z > 1-c'$ for a constant $1/2 > c' > 0$. 
\end{enumerate}

\end{proposition}

\begin{proof}
The proof is identical to that of Proposition~\ref{prop:transformation1}.

\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Proof of Proposition~\ref{prop:theta_rate}}
\label{sec:theta_rate_proof}


First suppose $\| \theta_1 - \theta_0 \| \rightarrow 0$. In Lemma~\ref{lem:hellinger_theta_equivalence}, we show that in this case $\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \rightarrow 0$. Condition \textbf{A1} follows directly from condition B1 and condition \textbf{A5} follows directly from condition B5. 

We now prove \textbf{A2}. Let $\rho$ be an constant and define 
\begin{align}
R = \left\{ x \,:\, \sup_\theta \| \nabla_\theta f_\theta(x) \| 
      \leq \frac{\log \rho}{\| \theta_1 - \theta_0 \|} \right\}  \label{eqn:parametric_R_defn}
\end{align}

From B3, we conclude that, for large enough $\rho$, $R$ is an interval. 

\begin{align*}
\log \frac{p(x)}{q(x)} &= f_{\theta_1}(x) - f_{\theta_0}(x)\\
           &= (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}} (x).
\end{align*}
This implies 
\begin{align*}
\left | \log \frac{p(x)}{q(x)} \right | &\leq \| \theta_1 - \theta_0 \| \| \nabla_{\theta} f_{\bar{\theta}} (x) \| \\
                &\leq  \| \theta_1 - \theta_0 \| \sup_\theta \| \nabla_{\theta} f_{\theta} (x) \|,
\end{align*}
where $\bar{\theta}$ is some convex combination of $\theta_0, \theta_1$. Therefore, for all $ x \in
  \left\{ x \,:\, \sup_\theta \| \nabla_{\theta} f_{\theta} (x) \| \leq \frac{\log \rho}{\| \theta_1 - \theta_0 \|} \right\}$, we have that $\frac{1}{\rho} \leq \frac{p(x)}{q(x)} \leq \rho$.

Since we have, from assumption B4, that
\[
 \int_{-\infty}^{\infty} \sup_\theta \| \nabla_{\theta} f_{\theta}(x) \|^{r} \phi(x) dx < \infty,
\]
by Markov's inequality, we have
\begin{align*}
\Phi(R^c) &= \Phi \left\{ x \,:\, \sup_\theta \| \nabla_{\theta} f_{\theta}(x) \|  
    > \frac{\log \rho}{\| \theta_1 - \theta_0 \|} \right\} 
    \leq C \frac{\| \theta_1 - \theta_0 \|^{r}}{(\log \rho)^{r}} 
    \stackrel{(a)}= \Theta( H^{r/2} ) = o(H)
\end{align*}
where $(a)$ follows from Lemma~\ref{lem:hellinger_theta_equivalence}. The last equality follows from the assumption that $r > 2$. This proves A2.

Proposition~\ref{prop:theta_A3_bound} proves \textbf{A3}. 

Proposition~\ref{prop:theta_A4_bound} proves \textbf{A4}.\\

Now suppose $\| \theta_1 - \theta_0 \| = \Theta(1)$. Lemma~\ref{lem:hellinger_theta_equivalence} implies that $\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = \Theta(1)$. Condition \textbf{A1'} follows directdly from B1, while condition \textbf{A4'} follows directly from condition B5. 

To prove \textbf{A2'}, we note from previous derivation that
\[
\left | \log \frac{p(x)}{q(x)} \right | 
                \leq  \| \theta_1 - \theta_0 \| \sup_\theta \| \nabla_{\theta} f_{\theta} (x) \|,
\]
A2' directly follows from B4.

\textbf{A3'} follows from proposition~\ref{prop:theta_A4_bound2}

\subsection{Supporting propositions}


\begin{proposition}
\label{prop:theta_A3_bound}
Suppose assumptions B1-B5 are satisfied. Let $R \subset \R$ be defined in equation~\ref{eqn:parametric_R_defn}. Then, we have that,
\[
\int_R \frac{1}{\alpha^r} \left| \frac{p(x)}{q(x)} - 1 \right|^r q(x) dx \leq \infty.
\]
\end{proposition}

\begin{proof}
By Lemma~\ref{lem:chi_square_theta_equivalence}, we have that $\alpha \asymp \| \theta_1 - \theta_0 \|$. Thus, we need only show that
\[
\int_R \frac{1}{\| \theta_0 - \theta_1 \|^r} \left| \frac{p(x)}{q(x)} - 1 \right|^r q(x) dx \leq \infty
\]




\begin{align*}
\frac{1}{\| \theta_1 - \theta_0\|}  \left| \frac{p(x)}{q(x)} - 1 \right| &=
    \frac{1}{\| \theta_1 - \theta_0\|}  
        \left| \exp( f_{\theta_1}(x) - f_{\theta_0}(x) ) - 1 \right| \\
   &=  \frac{1}{\| \theta_1 - \theta_0\|} \Big|(\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x)  \Big|
      \exp( f_{\bar{\theta}}(x) - f_{\theta_0}(x)) \\
  &\leq \| \nabla_{\theta} f_{\bar{\theta}}(x) \|   \exp( f_{\bar{\theta}}(x) - f_{\theta_0}(x)) \\
  &= \|  \nabla_{\theta} f_{\bar{\theta}}(x) \| 
       \exp\Big( (\bar{\theta} - \theta_1)^\tran \nabla_{\theta} f_{\tilde{\theta}}(x) \Big) \\
  &\leq  \|  \nabla_{\theta} f_{\bar{\theta}}(x) \| 
        \exp\Big( \| \theta_1 - \theta_0\| \| \nabla_{\theta} f_{\tilde{\theta}}(x) \| \Big),
\end{align*}
Where $\bar{\theta}, \tilde{\theta}$ are some convex combinations of $\theta_0, \theta_1$. Therefore, 
\begin{align*}
\int_R \left( \frac{1}{\| \theta_1 - \theta_0\|} \left| \frac{p(x)}{q(x)} - 1 \right| \right)^r q(x) dx &\leq
    \int_R  \|  \nabla_{\theta} f_{\bar{\theta}}(x) \|^r
        \exp\Big( r \| \theta_1 - \theta_0\| \| \nabla_{\theta} f_{\tilde{\theta}}(x) \| \Big)  q(x) dx \\
  &\stackrel{(a)}\leq   \int_R  \|  \nabla_{\theta} f_{\bar{\theta}}(x) \|^r
        e^{r \log \rho}  q(x) dx \\
  &= \int_{R}  \|  \nabla_{\theta} f_{\bar{\theta}}(x) \|^r
       \rho^r  q(x) d x \\
  &\leq \rho^r  \int_{-\infty}^{\infty} \|  \nabla_{\theta} f_{\bar{\theta}}(x) \|^r
        q(x) d x \\ 
  &\lesssim \int_{-\infty}^{\infty} \|  \nabla_{\theta} f_{\bar{\theta}}(x) \|^r
        \frac{q(x)}{\phi(x)} \phi(x) d x \\
  &\stackrel{(b)} \lesssim  \int_{-\infty}^{\infty} \|  \nabla_{\theta} f_{\bar{\theta}}(x) \|^r
          \phi(x) d x
\end{align*}
where $(a)$ follows from the definition of $R$. $(b)$ follows because of B1, which implies that $\frac{q(x)}{\phi(x)}$ is bounded. 

\end{proof}



\begin{proposition}
\label{prop:theta_A4_bound}
Suppose assumptions B1-B4 are satisfied. Let $R \subset \R$ be defined in equation~\ref{eqn:parametric_R_defn}. Then, we have that,
\begin{align*}
\int_R \left| \frac{1}{\alpha} \frac{p(x) - q(x)}{q(x)} \right|^{8t/(1-t)} \phi(x) dx &< \infty \\
\int_R \left| \frac{1}{\alpha} \frac{p'(x) - q'(x)}{q(x)} \right|^{4t/(1-t)} \phi(x) dx &< \infty.
\end{align*}

\end{proposition}

\begin{proof}
The proof of the first inequality follows that of proposition~\ref{prop:theta_A3_bound}. By lemma~\ref{lem:chi_square_theta_equivalence}, we have that $\alpha \asymp \| \theta_0 - \theta_1 \|$. Thus, we need only bound
\[
\int_R \left( \frac{1}{\| \theta_1 - \theta_0\|} 
        \left| \frac{p(x)}{q(x)} - 1 \right| \right)^{8t/(1-t)} \phi(x) dx
\]

Following the proof of proposition~\ref{prop:theta_A3_bound}, we have that

\begin{align*}
\int_R \left( \frac{1}{\| \theta_1 - \theta_0\|} 
        \left| \frac{p(x)}{q(x)} - 1 \right| \right)^{8t/(1-t)} \phi(x) dx
  &\leq \rho^{8t/(1-t)} \int_{-\infty}^\infty \| \nabla_\theta f_{\bar{\theta}}(x) \|^{8t/(1-t)} \phi(x) dx\\
  & \stackrel{(a)} \leq \infty.
\end{align*}

where $(a)$ follows from B4. 


We now consider the second inequality. Using the fact that $\alpha \asymp \| \theta_1 - \theta_0 \|$, we need only prove that
\[
\int_R \left| \frac{1}{\| \theta_1 - \theta_0\| } \frac{p'(x) - q'(x)}{q(x)} \right|^{4t/(1-t)} \phi(x) dx < \infty.
\]
Note that
\begin{align*}
\frac{1}{\| \theta_0 - \theta_1 \|} \frac{p'(x) - q'(x)}{q(x)}& = 
   \frac{1}{\| \theta_0 - \theta_1 \|} \left[
           f'_{\theta_1} \frac{p(x)}{q(x)} - f_{\theta_0}'(x) \right] \\
  &= \frac{1}{\|\theta_0 - \theta_1\|} \left\{ 
          ( f'_{\theta_1}(x) - f'_{\theta_0}(x) ) \frac{p(x)}{q(x)} 
       + f'_{\theta_0} \left( \frac{p(x)}{q(x)} - 1 \right) \right\} \\
  &= \| \nabla_{\theta} f'_{\bar{\theta}}(x) \| \frac{p(x)}{q(x)} 
         + \frac{1}{\| \theta_1 - \theta_0\|} f'_{\theta_0}(x) \left( \frac{p(x)}{q(x)} - 1 \right).
\end{align*}
where $\bar{\theta}$ is some convex combination of $\theta_1, \theta_0$. Therefore, we have:
\begin{align*}
& \int_R \left| \frac{1}{\| \theta_1 - \theta_0\| } \frac{p'(x) - q'(x)}{q(x)} \right|^{4t/(1-t)} \phi(x) dx \\
& =  \int_R \left|  \nabla_{\theta} f'_{\bar{\theta}}(x) \frac{p(x)}{q(x)} 
         + \frac{1}{\| \theta_1 - \theta_0\|} f'_{\theta_0}(x) \left( \frac{p(x)}{q(x)} - 1 \right)  \right|^{4t/(1-t)} \phi(x)dx.
\end{align*}
To show that this integral is finite, we need only show, regardless of the value of $2t/(1-t)$, that the two components have finite integrals:
\[
 \int_R \left|  \nabla_{\theta} f'_{\bar{\theta}}(x) \frac{p(x)}{q(x)} \right|^{4t/(1-t)} \phi(x) dx < \infty \quad \trm{ and } \quad
         \int_R \left| \frac{1}{\| \theta_1 - \theta_0\|} f'_{\theta_0}(x) \left( \frac{p(x)}{q(x)} - 1 \right)  \right|^{4t/(1-t)} \phi(x)dx < \infty.
\]
We bound the first integral as follows:
\begin{align*}
 \int_R \left|  \nabla_{\theta} f'_{\bar{\theta}}(x) \frac{p(x)}{q(x)} \right|^{4t/(1-t)} \phi(x) dx &\leq 
            \int_R \left|  \nabla_{\theta} f'_{\bar{\theta}}(x) \right|^{4t/(1-t)} \rho \phi(x) dx  \stackrel{(a)}< \infty,
\end{align*}
where $(a)$ follows from assumption B4.  For the second term, we simplify as follows:
\begin{align*}
&  \int_R \left| \frac{1}{\| \theta_1 - \theta_0\|} f'_{\theta_0}(x) \left( \frac{p(x)}{q(x)} - 1 \right)  \right|^{4t/(1-t)} \phi(x)dx \\
    &\leq \int_R | f'_{\theta_0}(x) |^{4t/(1-t)}
             \left|  \frac{1}{\| \theta_1 - \theta_0\|}  \left( \frac{p(x)}{q(x)} - 1 \right)  \right|^{4t/(1-t)} \phi(x) dx \\
    &\leq \left\{ \int_R  | f'_{\theta_0}(x) |^{8t/(1-t)} \phi(x) dx \right\}^{1/2} 
         \left\{
            \int_R
             \left| \frac{1}{\| \theta_1 - \theta_0\|}  \left( \frac{p(x)}{q(x)} - 1 \right)  \right|^{8t/(1-t)} \phi(x) dx 
         \right\}^{1/2} 
\end{align*}

The first quantity is finite by assumption. A bound for the second quantity follows from proposition~\ref{prop:theta_A3_bound}.
\end{proof}


\begin{proposition}
\label{prop:theta_A4_bound2}
Suppose assumption B4 is satisfied. Then, we have that,

\begin{align*}
\int \left| \frac{q'(x)}{q(x)} \right|^{4t/(1-t)} \phi(x) dx &< \infty \\
\int \left| \frac{\phi'(x)}{\phi(x)} \right|^{4t/(1-t)} \phi(x) dx &< \infty 
\end{align*}

\end{proposition}

\begin{proof}

The second bound follows directly from assumption B4.

\begin{align*}
\frac{q'(x)}{q(x)} = f'_{\theta}(x)
\end{align*}

Therefore, the first bound also follows from assumption B4. 

\end{proof}




\subsection{Supporting lemmas}

\begin{lemma}
\label{lem:chi_square_theta_equivalence}
Suppose assumptions B1-B5 hold. Let $R$ be the interval in Proposition~\ref{prop:interval_existence}. Define $\alpha = \int_R q(x) \left( \frac{p(x)}{q(x)} - 1 \right)^2 dx$. Then we have that
\[
\alpha \asymp \| \theta_1 - \theta_0 \|.
\]
\end{lemma}

\begin{proof}
We write $\alpha^2$ as
\begin{align*}
\alpha^2 &= \int_R \left( \frac{p(x)}{q(x)} - 1 \right)^2 q(x) dx \\
 &= \int_{R} \left| \exp\Big( f_{\theta_1}(x) - f_{\theta_0}(x) \Big) - 1 \right| ^2
            q(x )dx \\
 &= \int_{R} \left( (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x) 
                            \exp\Big( f_{\bar{\theta}}(x) - f_{\theta_0}(x) \Big) \right)^2 q(x)dx. 
\end{align*}
First we show an upper bound:
\begin{align*}
\alpha^2 & \leq \int_{R} \| \theta_1 - \theta_0 \|^2 \| \nabla_{\theta} f_{\bar{\theta}}(x)\|^2
                            \exp\Big( f_{\bar{\theta}}(x) - f_{\theta_0}(x) \Big)
              \exp( f_{\bar{\theta}}(x) ) dx\\
& \leq  \int_{R} \| \theta_1 - \theta_0 \|^2 \| \nabla_{\theta} f_{\bar{\theta}}(x)\|^2
                            \exp\Big( \| \theta_1 - \theta_0\| \|\nabla_{\theta} f_{\tilde{\theta}}(x) \| \Big) 
              \exp( f_{\bar{\theta}}(x) ) dx.
\end{align*}
Since we are in $R$, we have that $\| \theta_1 - \theta_0\| \sup_\theta \| \nabla_\theta f_{\theta}(x) \| \leq \log \rho$. We can thus continue the bound:
\begin{align*}
\alpha^2 & \leq   \| \theta_1 - \theta_0 \|^2 \int_{R} \| \nabla_{\theta} f_{\bar{\theta}}(x)\|^2
                           e^{\log \rho}
              \exp( f_{\bar{\theta}}(x) ) dx\\
& \leq  \| \theta_1 - \theta_0 \|^2 \rho \int_{-\infty}^\infty \| \nabla_{\theta} f_{\bar{\theta}}(x)\|^2
              \exp( f_{\bar{\theta}}(x) ) dx \\
& \stackrel{(a)}\lesssim \| \theta_1 - \theta_0 \|^2.
\end{align*}
where $(a)$ follows from assumptions B1 and B4. We now establish a lower bound:
\begin{align*}
\alpha^2 &\geq  \int_{R} \left( (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x) \right)^2
                            \exp\Big( - |f_{\bar{\theta}}(x) - f_{\theta_0}(x)| \Big)  \exp(f_{\bar{\theta}}(x))
             dx \\
  &\geq \int_{R} \left( (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x) \right)^2
                            \exp\Big( - \|\theta_1 - \theta_0\| \| \nabla_{\theta} f_{\bar{\theta}}(x) \| \Big)  
          \exp(f_{\bar{\theta}}(x)) dx \\
  &\stackrel{(a)}\geq e^{- C_R}  (\theta_1 - \theta_0)^\tran 
                \left( \int_{R} ( \nabla_{\theta} f_{\bar{\theta}}(x) ) 
                                      ( \nabla_{\theta} f_{\bar{\theta}}(x) )^\tran
                          \exp(f_{\bar{\theta}}(x)) dx \right) (\theta_1 - \theta_0),
\end{align*}
where $(a)$ follows from assumption B3. Define 
\[
\tilde{G}_{\bar \theta} =  \int_{R} ( \nabla_{\theta} f_{\bar{\theta}}(x) ) 
                                      ( \nabla_{\theta} f_{\bar{\theta}}(x) )^\tran
                          \exp(f_{\bar{\theta}}(x)) dx.
\]
For increasing $\rho$ or as $\| \theta_1 - \theta_0 \| \rightarrow 0$, we have that $R$ increases unboundedly, therefore,
$\lambda_{min}(\tilde{G}_{\bar \theta}) \rightarrow \lambda_{min}(G_{\bar \theta}) > 0$. Hence, $\alpha^2 \gtrsim \| \theta_1 - \theta_0 \|^2$.
\end{proof}




\begin{lemma}
\label{lem:hellinger_theta_equivalence}
When $\| \theta_1 - \theta_0 \| = \Theta(1)$, we have
\[
\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = c \| \theta_0 - \theta_1 \|_2^2,
\]
where $ c_{\min} \leq c \leq \frac{1}{4} c_{\max} d_{\Theta} $.
\end{lemma}

\begin{proof}
Expanding the left hand side, wehave
\begin{align*}
\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx & = \int q(x) \left(
        \sqrt{ \frac{p(x)}{q(x)} } - 1 \right)^2 dx \\
&= \int q(x)
       \Big( \exp\Big( f_{\theta_1}(x)/2 - f_{\theta_0}(x)/2 \Big) - 1 \Big)^2 dx.
\end{align*}
Now, let's look at the exponential term $\exp( f_{\theta_1}(x)/2 - f_{\theta_0}(x)/2 ) - 1$. Define $h(\theta) = \exp(f_{\theta}(x)/2 - f_{\theta_0}(x)/2 )$. It is clear that $h(\theta_0) = 1$ and that we wish to bound $h(\theta_1) - h(\theta_0)$. We bound this as follows:
\begin{align*}
|h(\theta_1) - h(\theta_0)| &= |(\theta_1 - \theta_0)^\tran \nabla_{\theta} h(\bar{\theta})| \\
   &= \left |\frac{1}{2} (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x) 
             \exp( f_{\bar{\theta}}(x)/2 - f_{\theta_0}(x)/2 ) \right|\\
&\leq \frac{1}{2} \| \theta_1 - \theta_0 \| 
                  \| \nabla_{\theta} f_{\bar{\theta}}(x) \| \exp( f_{\bar{\theta}}(x)/2 - f_{\theta_0}(x)/2 ), 
\end{align*}
where $\bar{\theta} \in \Theta$ is some convex combination of $\theta_1, \theta_0$. Thus, we have that
\begin{align*}
\int q(x)
       \Big( \exp\Big( f_{\theta_1}(x)/2 - f_{\theta_0}(x)/2 \Big) - 1 \Big)^2 dx 
  &\leq 
  \int q(x) \frac{1}{4} \| \theta_1 - \theta_0 \|^2 \| \nabla_{\theta} f_{\bar{\theta}}(x) \|^2 
               \exp( f_{\bar{\theta}}(x) - f_{\theta_0}(x)) dx \\
  &= \frac{1}{4} \| \theta_1 - \theta_0 \|^2 \int  \| \nabla_{\theta} f_{\bar{\theta}}(x) \|^2  
            \exp( f_{\bar{\theta}} (x)) d x \\
  &\leq \frac{1}{4}  \| \theta_1 - \theta_0 \|^2 \tr( G_{\bar{\theta}} ) \\
  &\leq \frac{1}{4}  \| \theta_1 - \theta_0 \|^2 c_{\max} d_{\Theta},
\end{align*}
where $\Theta \subset \mathbb R^{d_\Theta}$. To get an upper bound,
\begin{align*}
\int q(x)
       \Big( \exp\Big( f_{\theta_1}(x)/2 - f_{\theta_0}(x)/2 \Big) - 1 \Big)^2 dx 
   &= \int \Big( (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x) \Big)^2 
               \exp(f_{\bar{\theta}}(x))  dx \\
   &= (\theta_1 - \theta_0)^\tran G_{\bar{\theta}} (\theta_1 - \theta_0)  \\
   &\geq c_{\min} \| \theta_1 - \theta_0 \|^2.
\end{align*}
\end{proof}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Appendix for Theorem~\ref{thm:lower_bound}}
\label{sec:lower_bound_proof}


\subsection*{Properties of Permutation Invariant Estimators}

Permutation invariant estimators possess symmetry properties. The next lemma formalizes one symmetry property useful for the proof of theorem~\ref{thm:lower_bound}. 

We will need to define new notation first. Let $\hat{\sigma}$ be a clustering algorithm and $A$ be a weighted network so that $\hat{\sigma}(A) \,:\, [n] \rightarrow [K]$ is the clustering outputted by $\hat{\sigma}$ when given $A$ as input. 

Define 
\[
S_K[ \hat{\sigma}(A), \sigma_0] = \argmin_{\rho \in S_K} d_H(\rho \circ \hat{\sigma}(A), \sigma_0) 
\] 
as the set of permutations over the $K$ cluster labels that minimize the Hamming distance between $\hat{\sigma}(A)$ and $\sigma_0$ and define

\begin{align}
\mathcal{E}[ \hat{\sigma}(A), \sigma_0] = \Big\{ v \,:\, (\rho \circ \hat{\sigma}(A))(v) \neq  \sigma_0(v),\, 
          \exists \rho \in S_K[\hat{\sigma}(A), \sigma_0]   \Big\} \label{eqn:error_set_defn}
\end{align}
When $S_K[\hat{\sigma}(A), \sigma_0]$ is a singleton, $\mathcal{E}[\hat{\sigma}(A), \sigma_0]$ represent the set of nodes mis-clustered by $\hat{\sigma}(A)$ where we use $\sigma_0$ as the true clustering. However, when $S_K[\hat{\sigma}(A), \sigma_0]$ contains multiple elements, the notion of whether a node is mis-clustered or not becomes ambiguous. We choose, for simplicity, to call node $v$ mis-clustered if for any $\rho \in S_K[\hat{\sigma}(A), \sigma_0$, $(\rho \circ \hat{\sigma}(A))(v) \neq \sigma_0(v)$.





\begin{lemma}
\label{lem:permutation_invariance_symmetry}
Let the true clustering $\sigma_0$ be arbitrary, the weighted matrix $A$ be drawn from an arbitrary probability measure, $\hat{\sigma}$ be any permutation invariant estimator. Let $u,v$ be two nodes such that there exists a permutation $\pi \in S_n$ where (1) $\pi(u) = v$ , (2) $\pi$ is measure preserving, i.e., $A$ and $\pi(A)$ have the same measure, and (3) $\pi$ preserves true clustering, i.e., there exists $\tau \in S_K$ such that $\tau \circ \sigma_0 \circ \pi^{-1} = \sigma_0$. Then, we have that

\[
P( u \in \mathcal{E}[\hat{\sigma}(A), \sigma_0]) = P( v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0] )
\]
\end{lemma}

\begin{proof}

Since $A$ and $\pi A$ follow the same distribution, we have that
\[
P \Big(v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0 ] \Big) = 
       P \Big(v \in \mathcal{E}[\hat{\sigma}(\pi A), \sigma_0] \Big) 
\]

Now, let $A$ be arbitrarily fixed. Let $\tau$ be the permutation in $S_K$ such that $\tau \circ \sigma_0 \circ \pi^{-1} = \sigma_0$. Let $\xi \in S_K$ be the permutation such that $\hat{\sigma}(\pi A) = \xi \circ \hat{\sigma}(A) \circ \pi^{-1}$. Let $\rho \in S_K$ be an arbitrary permutation. Then, 
\begin{align*}
d_H(\rho \circ \hat{\sigma}(A), \sigma_0) &= 
      d_H( \tau \circ \rho \circ \xi^{-1} \circ \xi \circ \hat{\sigma}(A) \circ \pi^{-1}, \tau \circ \sigma_0 \circ \pi^{-1} ) \\
  &= d_H( \tau \circ \rho \circ \xi^{-1} \circ \hat{\sigma}(\pi A), \sigma_0)
\end{align*}
where the second equality follows from the assumption that $\hat{\sigma}$ is permutation invariant.

Therefore, we have that if $\rho^* \in \argmin_{\rho \in S_K} d_H(\rho \circ \hat{\sigma}(A), \sigma_0)$, 
then $\tau \circ \rho^* \circ \xi^{-1} \in \argmin_{\rho \in S_K} d_H( \rho \circ \hat{\sigma}(\pi A), \sigma_0)$.

Without loss of generality, let $\argmin_{\rho \in S_K} d(\rho \circ \hat{\sigma}(A), \sigma_0)$ include the identity map so that $\tau \circ \xi^{-1} \in \argmin_{\rho \in S_K} d( \rho \circ \hat{\sigma}( \pi A), \sigma_0)$.

Suppose $v \in \mathcal{E}[\hat{\sigma}(\pi A), \sigma_0]$, which, by definition, means that $\tau \circ \xi^{-1} \circ \hat{\sigma}(\pi A)(v) \neq \sigma_0(v)$. 

\begin{align*}
\hat{\sigma}(A)(u) &= \hat{\sigma}(A)(\pi^{-1}(v)) \\
   &= \tau^{-1} \circ \tau \circ \xi^{-1} \circ \xi \circ \hat{\sigma}(A) \circ \pi^{-1} (v) \\
   &\stackrel{(a)} = \tau^{-1} \circ \tau \circ \xi^{-1} \circ \hat{\sigma}( \pi A) (v) \\
   &\neq \tau^{-1} \circ \sigma_0(v) \\
   &= \tau^{-1} \circ \sigma_0( \pi( u )) \\
   &\stackrel{(b)} = \sigma_0(u)
\end{align*}
where $(a)$ follows from the assumption that $\hat{\sigma}$ is permutation-invariant, and $(b)$ follows because $\tau \circ \sigma_0 \circ \pi^{-1} = \sigma_0$ implies that $\sigma_0 = \tau^{-1} \circ \sigma_0 \circ \pi$.

Thus, $v \in \mathcal{E}[ \hat{\sigma}(\pi A), \sigma_0]$ implies that $\hat{\sigma}(A)(u) \neq \sigma_0(u)$ and $u \in \mathcal{E}[ \hat{\sigma}(A), \sigma_0]$. 
Same reasoning shows that if $u \in \mathcal{E}[ \hat{\sigma}(A), \sigma_0]$ implies that $v \in \mathcal{E}[\hat{\sigma}(\pi A), \sigma_0]$.

Therefore, 
\begin{align*}
P( u \in \mathcal{E}[\hat{\sigma}(A), \sigma_0] ) &= P( v \in \mathcal{E}[ \hat{\sigma}(\pi A), \sigma_0] ) \\
     &= P( v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0] 
\end{align*}

\end{proof}


\begin{corollary}
\label{cor:permutation_invariance_symmetry_sbm}

Let the true cluster assignment $\sigma_0$ be arbitrary, the weighted matrix $A$ be drawn from the weighted SBM, and $\hat{\sigma}$ be any permutation invariant estimator. Let $u,v$ be two nodes such that $u$ and $v$ are in equally sized clusters. Then,
\[
P( u \in \mathcal{E}[\hat{\sigma}(A), \sigma_0]) = P( v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0] )
\]
\end{corollary}

\begin{proof}

By Lemma~\ref{lem:permutation_invariance_symmetry}, we just have to produce a permutation $\pi \in S_n$ such that $\pi(u) = v$, $A$ and $\pi A$ are identically distributed, and that there exists $\tau \in S_K$ such that $\tau \circ \sigma_0 \circ \pi^{-1} = \sigma_0$.

In the first case, let us suppose that $u, v$ are in the same cluster. Then we let $\pi$ be the permutation that swaps only $u$ and $v$, i.e., $\pi(u) = v$, $\pi(v) = u$, $\pi(v') = v'$ for all $v' \neq u,v$. The 3 necessary properties are easily verified with $\tau$ as the identity.

In the second case, let us suppose that, without loss of generality, $u$ is in cluster 1 and $v$ is in cluster 2 where cluster 1 and 2 are equally sized. Then we let $\pi$ be the permutation that swaps all the nodes in cluster 1 with all the nodes in cluster 2 leaving every other nodes unchanged. The 3 necessary properties are also easily verified with $\tau$ as the permutation that swaps only label 1 and 2. 

\end{proof}


\subsection*{Properties of the Renyi Divergence}




We first state a lemma that gives an alternative characterization of the Renyi divergence. 

\begin{lemma}
\label{lem:information_equivalence}

Let $P, Q$ be two probability measures on $\R$ absolutely continuous with respect to each other. Suppose that part of $P,Q$ singular to the Lebesgue measure is a point mass at zero, denoted $P_0, Q_0$. Define
\[
I = - 2 \log \int \left( \frac{dP}{dQ} \right)^{1/2} dQ \qquad  \text{and} \quad
D = \inf_{Y \in \mathcal{P}} \max \left\{ \int \log \frac{dY}{dP} dY, \, \int \log \frac{dY}{dQ} dY \right\},
\]
where we use $\mathcal{P}$ to denote probability measures absolutely continuous to $P$ (and thus $Q$).Then, we have that
\[ 
I = 2 D. 
\]
\end{lemma}


\begin{proof} 
First, we note that $D$ must be finite since we can substitute $Y = P$ or $Y= Q$. We claim that $D$ is equivalent to the following:
\begin{align*}
\inf_{Y \in \mathcal{P}} & \int \log \frac{dY}{dP} dY \quad \text{ such that } \quad \int \log \frac{dP}{dQ} dY = 0.
\end{align*}
This is because for any $Y \in \mathcal{P}$ such that $\int \log \frac{dP}{dQ} dY \neq 0$, 
we have that $\int \log \frac{dY}{dP} dY \neq \int \log \frac{dY}{dQ} dY$. 
Suppose without loss of generality that the former quantity is larger.  Therefore, it is possible to take $\tilde{Y} = (1 - \epsilon) Y + \epsilon P$ for $\epsilon$ small enough such that $ \max \left\{ \int \log \frac{d\tilde{Y}}{dP} d\tilde{Y}, \, \int \log \frac{d\tilde{Y}}{dQ} d\tilde{Y} \right\}$ strictly decreases. Since the new optimization is convex in $Y$, we can solve and get $Y_0 = \frac{P_0^{1/2} Q_0^{1/2}}{Z}$ and $(1-Y_0) y(x) = \frac{((1-P_0) \cdot p(x))^{1/2} ((1-Q_0) \cdot q(x))^{1/2}}{Z}$. Here, we denote by $(1-Y_0) y(x), (1-P_0)p(x), (1-Q_0)q(x)$ the Radon-Nikodym derivative of the continuous part of $Y, P, Q$ with respect to the Lebesgue measure. The quantity $Z$ is the normalization term: $Z = P_0^{1/2} Q_0^{1/2} + \int \sqrt{ (1-P_0) p(x) (1-Q_0) q(x)} dx$. 

\begin{align*}
\int \log \frac{dY}{dP} dY  &= \log \frac{1}{Z} \left\{ 
           \left( \frac{Q_0}{P_0} \right)^{1/2}  Y_0 + \int \left(\frac{(1-P_0) p(x)}{(1-Q_0)q(x)} \right)^{1/2} (1-Y_0) y(x) dx \right\} \\
  &= \log \frac{dP}{dQ} dY - \log Z \\
  &= - \log Z. 
\end{align*}
It is straightforward to verify that $-2 \log Z = I$. 
\end{proof}




\subsection*{Proof of Theorem~\ref{thm:lower_bound}}
\label{sec:actual_lower_bound_proof}
Throughout this proof, we let $C$ denote a $\Theta(1)$ sequence whose value may change from instance to instance. 


Define
\[
\tilde{l}(\hat{\sigma}(A), \sigma_0) = \frac{1}{n} 
     \sum_{v=1}^n \mathbbm{1}\{ v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0 \},
\]
where $\mathcal{E}[\hat{\sigma}(A), \sigma_0]$ is defined in equation~\ref{eqn:error_set_defn}. Suppose the following are true:

\begin{align}
\trm{if $\frac{nI}{K} \rightarrow \infty$}, & \quad 
       \E \tilde{l}(\hat{\sigma}(A), \sigma_0) \geq \exp \left( - (1+o(1)) \frac{nI}{\beta K} \right) 
    \label{eqn:tilde_l_bound1} \\
\trm{if $\frac{nI}{K} \leq c < \infty$}, &\quad 
       \E \tilde{l}(\hat{\sigma}(A), \sigma_0) \geq c' > 0 \quad \trm{ for some constants $c, c'$}
    \label{eqn:tilde_l_bound2}
\end{align}

then, we claim that the theorem holds. To see this, suppose first that equation~\ref{eqn:tilde_l_bound1} is true and suppose that $\frac{nI}{K} \rightarrow \infty$. We perform case analysis on the probability that $l(\hat{\sigma}(A), \sigma_0)$ is large.

\textbf{Case 1:} $P\left( l(\hat{\sigma}(A), \sigma_0) \geq \frac{1}{2\beta K} \right) \geq \frac{1}{2} \E \tilde{l}(\hat{\sigma}(A), \sigma_0)$. In this case, we have that
\begin{align*}
\E l(\hat{\sigma}(A), \sigma_0) &\geq \frac{1}{2 \beta K} P\left( l(\hat{\sigma}(A), \sigma_0) \geq \frac{1}{2\beta K} \right) \\
     & \geq \frac{1}{2 \beta K}  \E \tilde{l}(\hat{\sigma}(A), \sigma_0) \\
     &\geq \frac{1}{2 \beta K} \exp \left( - (1+o(1)) \frac{nI}{\beta K} \right) \\
     &\geq \exp \left( - (1+o(1)) \frac{nI}{\beta K} \right) 
\end{align*}
\textbf{Case 2:}  $P\left( l(\hat{\sigma}(A), \sigma_0) \geq \frac{1}{2\beta K} \right) < \frac{1}{2} \E \tilde{l}(\hat{\sigma}(A), \sigma_0)$. In this case, we have that
\begin{align*}
\E l(\hat{\sigma}(A), \sigma_0) &\geq \E\left[ l(\hat{\sigma}(A), \sigma_0) 
       \given l(\hat{\sigma}(A), \sigma_0) < \frac{1}{2 \beta K} \right] 
         P\left(  l(\hat{\sigma}(A), \sigma_0) < \frac{1}{2 \beta K} \right) \\
  &= \E\left[ \tilde{l}(\hat{\sigma}(A), \sigma_0) 
       \given l(\hat{\sigma}(A), \sigma_0) < \frac{1}{2 \beta K}\right]
         P\left(  l(\hat{\sigma}(A), \sigma_0) < \frac{1}{2 \beta K} \right) \\
  &= \E \tilde{l}(\hat{\sigma}(A), \sigma_0) -  
      \E\left[ \tilde{l}(\hat{\sigma}(A), \sigma_0) 
       \given l(\hat{\sigma}(A), \sigma_0) \geq \frac{1}{2 \beta K} \right] 
         P\left(  l(\hat{\sigma}(A), \sigma_0) \geq \frac{1}{2 \beta K} \right) \\
  &\geq \E \tilde{l}(\hat{\sigma}(A), \sigma_0) - \frac{1}{2}  \E \tilde{l}(\hat{\sigma}(A), \sigma_0) \\
  &= \frac{1}{2}  \E \tilde{l}(\hat{\sigma}(A), \sigma_0) \\
  &\geq \exp\left( - (1 + o(1)) \frac{nI}{\beta K} \right)
\end{align*}




Without loss of generality, let us suppose that cluster 1 and 2 are the two smallest clusters, where cluster 1 is of size $\frac{n}{\beta K} + 1$ and cluster 2 is of size $\frac{n}{\beta K}$. Let us suppose that node $1$ is assigned to cluster $1$, i.e., $\sigma_0(1)= 1$ and $\sigma_0(2) = 2$. Let $C_i = \{u : \sigma_0(u) = i\}$ denote the $i$-th community. For convenience, let us use $v_1$ to denote node 1.\\


Let $\sigma_0^1  = \sigma_0$ and $\sigma_0^2 \,:\, [n] \rightarrow [K]$ be a cluster assignment that differ with $\sigma_0$ only on node $v_1$. That is, $\sigma_0^2(v) = \sigma_0(v)$ for all $v \neq v_1$ and $\sigma_0^2(v_1) = 2$. 


Let $\sigma^*$ be a random cluster assignment where 
\[
\sigma^* = \left \{
     \begin{array}{cc}
     \sigma^1_0  &  \trm{ with $1/2$ probability} \\
     \sigma^2_0 & \trm{with $1/2$ probability}
    \end{array} \right.
\]


Let $\Phi$ denote the probability measure on $(\sigma^*, A, \hat{\sigma}(A))$ defined as
\[
P_{\Phi}(\sigma^*, A, \hat{\sigma}(A)) = P(\sigma^*) P_{SBM}(A \given \sigma^*) P_{alg}( \hat{\sigma}(A) \given A) 
\]
where $P_{SBM}( A \given \sigma^*)$ is the measure on the weighted graph defined by the weighted SBM treating $\sigma^*$ as the true cluster assignment, and $P_{alg}$ represent any randomness in the clustering algorithm.


Let $\Psi$ denote an alternative probability measure as such:
\[
P_{\Psi} (A, \hat{\sigma}(A)) = P(\sigma^*) P_{\Psi}(A  \given \sigma^*) P_{alg}( \hat{\sigma}(A) \given A) 
\]
where $P_{\Psi}( A \,;\, \sigma_0)$ is defined as such:
\begin{enumerate}
\item If $u,v \neq v_1$, then $A_{uv}$ is distributed just as in $P_{SBM}( A \given \sigma^*)$. 
\item If $v = v_1$ and $u \notin C_1 \cup C_2$, then $A_{uv}$ is distributed just as in $P_{SBM}( A \given \sigma^*)$.
\item If $v = v_1$ and $u \in C_1 \cup C_2$, then $A_{uv}$ is distributed as $Y$, where $Y$ is the distribution that minimizes $D$ in Lemma~\ref{lem:information_equivalence}; i.e., $Y_0 \propto (P_0 Q_0)^{1/2}$ and $(1-Y_0) y(x) \propto \sqrt{(1-P_0)p(x)(1-Q_0) q(x)}$.
\end{enumerate}

Note that $P_{\Psi}(A \given \sigma^*) = P_{\Psi}(A)$ actually does not depend on the outcome of $\sigma^*$. 



The log-likelihood ratio $\log \frac{d P_{\Psi} }{d P_{\Phi}}$ is 
\begin{align*}
\mathcal{Q} =& \log \frac{d P_{\Psi}}{d P_{\Phi}} \\
      = & \log \frac{ d P_{SBM}(A \given \sigma^*)}{ d P_{\Psi} (A \given \sigma^*)} \\
      =& \sum_{u \in C_{\sigma^*(v_1)}} \log \frac{Y(A_{u v_1})}{P(A_{u v_1})} + \sum_{u \in C_1 \cup C_1 - C_{\sigma^*(v_1)}  } \log \frac{Y(A_{u v_1})}{Q(A_{u v_1})}  
\end{align*}

where we use the notation $P(A_{uv_1}) = P_0$ if $A_{uv_1} = 0$ and $P(A_{uv_1}) = (1-P_0)p(A_{uv_1})$ if $A_{uv_1} \neq 0$. 

Let $f(n)$ be an arbitrary function. Also, define $E$ as the event
 
$$E = \Big\{ v_1 \notin\mathcal{E}[\hat{\sigma}(A), \sigma^*] \trm{ and } \tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K} \Big\}$$ 
where $\mathcal{E}[\hat{\sigma}(A), \sigma^*]$ is the set of vertices which are incorrectly clustered by $\hat \sigma(A)$. We have the equality
\begin{align}
P_{\Psi}( \mathcal{Q} \leq f(n) ) =& P_{\Psi}( \mathcal{Q} \leq f(n),\, \neg E) + P_{\Psi}( \mathcal{Q} \leq f(n),\, E)  \nonumber\\
  =&
  \underbrace{P_{\Psi}( \mathcal{Q} \leq f(n), v_1 \in \mathcal{E}[\hat{\sigma}(A), \sigma^*] 
       \trm{ or } \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K}    )}_{\textrm{First term}} \nonumber \\
& +  
 \underbrace{P_{\Psi}( \mathcal{Q} \leq f(n), v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma^*] 
        \trm{ and } \tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K} )}_{\textrm{Second term}} \label{eqn:psi_Q_bound}
\end{align}
We first bound the first term as follows:
\begin{align*}
P_{\Psi}( \mathcal{Q} \leq f(n), \neg E) &= \int_{\mathcal{Q} \leq f(n), \neg E} d P_{\Psi} \\
    &= \int_{\mathcal{Q} \leq f(n), \neg E} \exp(\mathcal{Q}) d P_{\Phi} \\
    &\leq \exp(f(n)) P_{\Phi}( \mathcal{Q} \leq f(n), \neg E) \\
    &\leq \exp(f(n)) P_{\Phi}(\neg E) \\
    &= \exp(f(n)) P_{\Phi}\Big(v_1 \in \mathcal{E}[\hat{\sigma}(A), \sigma^*] 
       \trm{ or } \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K} \Big)  \\
    &\leq \exp(f(n)) \Big( P_{\Phi}(v_1 \in \mathcal{E}[\hat{\sigma}(A), \sigma^*]) + 
               P_{\Phi} \left( \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K}  \right) \Big)    \qquad \mathbf{(*)}
\end{align*}

By lemma~\ref{lem:permutation_invariance_symmetry}, we have
\begin{align*}
 \E_{\Phi}  \tilde{l}(\hat{\sigma}(A), \sigma^*) &= 
    \frac{1}{n} \sum_{v=1}^n P_{\Phi}( v \in \mathcal{E}[\hat{\sigma}(A), \sigma^*]) \\
  &\geq \frac{1}{n} \sum_{v \in C_{\sigma^*(v_1)}} P_{\Phi}( v \in \mathcal{E}[\hat{\sigma}(A), \sigma^*]) \\
  &\stackrel{(a)}= \frac{|C_{\sigma^*(v_1)}|}{n} P_{\Phi}( v_1 \in \mathcal{E}[\hat{\sigma}(A), \sigma^*]) \\
  &\geq \frac{1}{ \beta K} P_{\Phi}( v_1 \in \mathcal{E}[\hat{\sigma}(A), \sigma^*])
\end{align*}

where $(a)$ follows from corollary~\ref{cor:permutation_invariance_symmetry_sbm}. Also,

\begin{align*}
\E_{\Phi} \tilde{l}(\hat{\sigma}(A), \sigma^*) &\geq  
         \E_{\Phi}\Big[ \tilde{l}(\hat{\sigma}(A), \sigma^*) \,\Big|\, 
        \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K}\Big] 
              P\left( \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K} \right) \\
    &\geq \frac{1}{4\beta K} P\left( \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K} \right)
\end{align*}

Therefore, continuing on from $\mathbf{(*)}$ where we left off, we have:

\begin{align*}
P_{\Psi}( \mathcal{Q} \leq f(n), \neg E) &\leq 
     \exp( f(n)) \cdot 5 \beta K \cdot \E_{\Phi} \tilde{l}(\hat{\sigma}(A), \sigma^*)
\end{align*}


Now we turn to the second term in equation~\ref{eqn:psi_Q_bound}. We observe that under $\Psi$, the distribution over $A$ does not depend on $\sigma^*$. Therefore, we have that

\begin{align*}
&P_{\Psi}\left(   v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma^*]   
    \trm{ and }  \tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K} \right) \\
&=   P_{\Psi}\left( v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma^*] 
     \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K} 
        \given \sigma^* = \sigma_0^1 \right) \frac{1}{2}  \\
 &\quad +    P_{\Psi}\left( v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma^*]
       \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K} 
       \given \sigma^* = \sigma_0^2 \right) \frac{1}{2} \\
      &=  P_{\Psi}\left( v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^1] 
                 \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^1)  \leq \frac{1}{4\beta K} \right) \frac{1}{2} \\
  &\quad +  
          P_{\Psi}\left( v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^2]  
          \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^2)  \leq \frac{1}{4\beta K}  \right) \frac{1}{2} 
\end{align*}


Because $l(\hat{\sigma}(A), \sigma_0^1) \leq \tilde{l}(\hat{\sigma}(A), \sigma_0^1) \leq \frac{1}{4 \beta K}$, we know, by lemma \ref{lem:consensus_uniqueness}, that $S_K[\hat{\sigma}(A), \sigma_0^1]$ has only one element--let's call it $\rho$. Because $d_H(\sigma_0^1, \sigma_0^2) = 1$, it must be that $\frac{1}{n} d_H(\rho \circ \hat{\sigma}(A), \sigma_0^2) \leq \frac{1}{4 \beta K} + \frac{1}{n} \leq \frac{1}{2 \beta K}$ and therefore, $\rho \in S_K[\hat{\sigma}(A), \sigma_0^2]$ as well. Because $(\rho \circ \hat{\sigma}(A) )(v_1) $ cannot both be $\sigma_0^1(v_1) = 1$ and $\sigma_0^2(v_1) = 2$ at the same time, it must be that $v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^1]$ and $v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^2]$ cannot both be true.

Therefore, we have that
\[
\mathbbm{1}\left\{  v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^1]  
   \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^1)  \leq \frac{1}{4\beta K}  \right\} +  
\mathbbm{1}\left\{ v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^2] 
   \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^2)  \leq \frac{1}{4\beta K}  \right\} + 
 \leq 1.
\]

This implies that
\[
P_{\Psi}\left\{  v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^1]  
   \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^1)  \leq \frac{1}{4\beta K}  \right\} +  
P_{\Psi}\left\{ v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^2] 
   \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^2)  \leq \frac{1}{4\beta K}  \right\}
 \leq 1.
\]


And therefore, 
\[
P_{\Psi}(E) = P_{\Psi}\left(  v_1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma^*]   
    \trm{ and }  \tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K} \right) \leq \frac{1}{2}
\]

Thus
\[
P_{\Psi}(\mathcal{Q} \leq f(n), E) 
             \leq P_{\Psi}(E) \leq \frac{1}{2}.
\]




Combining these two bounds, we have that
\[
P_{\Psi}(\mathcal{Q} \leq f(n)) \leq \exp(f(n)) \frac{\beta K}{2} \E_{\Phi} l(\hat{\sigma}, \sigma_0) + \frac{1}{2}.
\]
Let $f(n) = \log \frac{1}{2 \beta K  \E_{\Phi} \tilde{l}(\hat{\sigma}, \sigma_0)}$, then 

\[
P_{\Psi}\left(\mathcal{Q} \leq \log \frac{1}{2 \beta K \E_{\Phi} l(\hat{\sigma}, \sigma_0)} \right) \leq \frac{3}{4}.
\]
From Chebyshev's inequality, we also have that
\[
P_{\Psi}\left( \mathcal{Q} \leq \E_{\Psi} \mathcal{Q} + \sqrt{ 5 V_{\Psi}(\mathcal{Q})} \right) \geq 4/5,
\]
where $V_\Psi(Q)$ is the variance of $Q$ under measure $\Psi$. Hence, we get that $\log \frac{1}{2 \beta K \E_{\Phi} l(\hat{\sigma},\sigma_0)} \leq \E_{\Psi} \mathcal{Q} + \sqrt{ 5 V_{\Psi}(\mathcal{Q})}$, stated equivalently as
\[
\E_{\Phi} l(\hat{\sigma}, \sigma_0) \geq \frac{1}{2 \beta K} \exp\Big( - (\E_{\Psi} \mathcal{Q} + \sqrt{ 5 V_{\Psi}(\mathcal{Q})} ) \Big).
\]
We now just need to compute $\E_{\Psi} \mathcal{Q}$ and $V_{\Psi}(\mathcal{Q})$. We first compute $\E_{\Psi} \mathcal{Q}$.

\[
\E_{\Psi} \mathcal{Q} = \E_{\Psi} [ \mathcal{Q} \given \sigma^* = \sigma_0^1 ] \frac{1}{2} + 
              \E_{\Psi} [ \mathcal{Q} \given \sigma^* = \sigma_0^2 ] \frac{1}{2} 
\]

\begin{align*}
\E_{\Psi} [\mathcal{Q} \given \sigma^* = \sigma_0^1] &= \E_{\Psi}  \sum_{u: \, u \neq v_1,\, \sigma_0^1(u) = 1} \log \frac{Y(A_{uv_1})}{P(A_{uv_1})} + \sum_{u :\, \sigma_0^1(u) = 2} \log \frac{Y(A_{uv_1})}{Q(A_{uv_1})}  \\
     &= \frac{n}{\beta K}  \int \log \frac{dY}{dP} dY + \frac{n}{\beta K} \int \log \frac{dY}{dQ} dY \\
     &= \frac{n}{\beta K} 2 D\\
     &= \frac{n}{\beta K} I \\
\end{align*}

In like fashion, we can show that
\[
\E_{\Psi}[ \mathcal{Q} \given \sigma^* = \sigma_0^2] = \frac{n}{\beta K} I
\]

Therefore, we have that
\begin{align}
\E_{\Psi} \mathcal{Q} = \frac{n I}{\beta K} \label{eqn:expectation_psi_Q_equality}
\end{align}


The bound for the variance term is involved. We start with the decomposition:

\begin{align*}
V_{\Psi}(\mathcal{Q}) &= V( \E_{\Psi}[ \mathcal{Q} \given \sigma^* ] ) + 
              E[ V_{\Psi}( \mathcal{Q} \given \sigma^* ) ] \\
      &=   E[ V_{\Psi}( \mathcal{Q} \given \sigma^* ) ] \\
      &=  V_{\Psi}( \mathcal{Q} \given \sigma^* = \sigma_0^1) \frac{1}{2} + 
          V_{\Psi}( \mathcal{Q} \given \sigma^* = \sigma_0^2) \frac{1}{2}
\end{align*}


\begin{align*}
V_{\Psi}(\mathcal{Q} \given \sigma^* = \sigma_0^1) &= \sum_{u:\, u \neq 1, \sigma_0^1(u) = 1 } 
         V_{\Psi} \left( \log \frac{Y(A_{v_1 u})}{P(A_{v_1 u})} \right) +
                        \sum_{u :\, \sigma_0^1(u) = 2} V_{\Psi} \left( \log \frac{Y(A_{v_1 u})}{Q(A_{v_1 u})} \right)  \\
   &\leq \frac{n}{\beta K}  \E_{\Psi} \left( \log \frac{Y(A_{v_1 u})}{P(A_{v_1 u})} \right)^2 + 
         \frac{n}{\beta K} \E_{\Psi} \left( \log \frac{Y(A_{v_1 u})}{Q(A_{v_1 u})} \right)^2. 
\end{align*}


We will show that $\E_{\Psi} \left( \log \frac{Y(A_{v_1 u})}{P(A_{v_1 u})} \right)^2$ can be bounded by $C I$ so that $\sqrt{ 5 V_{\Psi}(\mathcal{Q})} \leq C \sqrt{\frac{nI}{\beta K}} $. We have that
\begin{align}
\E_{\Psi} \left( \log \frac{Y(A_{uv^*})}{P(A_{uv^*})} \right)^2 &= 
    \int \left( \log \frac{dY}{dP} \right)^2 dY \nonumber \\
  &= Y_0 \log^2 \frac{Y_0}{P_0} + (1-Y_0) \int y(x) \log^2 \frac{(1-Y_0) y(x)}{(1-P_0)p(x)} dx. \label{eqn:lower_bound_var_terms}
\end{align}

We bound the first term $Y_0 \log^2 \frac{Y_0}{P_0}$. We may bound $\left| \log \frac{Y_0}{P_0} \right|$ as 
\begin{align*}
\left| \log \frac{Y_0}{P_0} \right|&= \left| \frac{1}{2} \log \frac{Q_0}{P_0} - \log Z \right| \\
    &\leq \frac{1}{2} \left| \log \left( 1 - \frac{P_0 - Q_0}{P_0} \right) \right| + I/2 \\ 
   & \stackrel{(a)} \leq \frac{1}{2} \left| \frac{P_0 - Q_0 }{P_0} \right| + \left| \frac{P_0 - Q_0}{P_0} \right|^2 C + I/2 \\   
   & \stackrel{(b)} \leq C \left| \frac{P_0 - Q_0}{P_0} \right| + C I 
\end{align*}

Inequality $(a)$ follows from Lemma~\ref{lem:log_linearize} and from the fact that $\frac{Q_0}{P_0}$ is bounded. Inequality $(b)$ follows from the fact that $\left| \frac{ P_0 - Q_0}{P_0} \right| = \left| 1 - \frac{Q_0}{P_0} \right| \leq 1 + \left| \frac{Q_0}{P_0} \right|$ is bounded by a constant. 

Therefore, we have that:
\begin{align*}
Y_0 \log^2 \frac{Y_0}{P_0} &\leq Y_0 \left( C \frac{ |P_0 - Q_0| }{P_0} + C I \right)^2 \\
     &\stackrel{(a)} \leq Y_0 \frac{|P_0 - Q_0|^2}{P_0^2 } C + Y_0 I^2 C \\
     &\stackrel{(b)} \leq \frac{|P_0 - Q_0|^2}{P_0} C + I^2 C 
\end{align*}

where $(a)$ follows because $(x + y)^2 \leq 2x^2 + 2y^2$ and $(b)$ follows because $Y_0 = \frac{\sqrt{P_0 Q_0}}{Z} = (1+o(1)) C P_0$. 

Now, $I = o(1)$ so $I^2 \leq I$. Also, $I \geq (1+o(1)) ( \sqrt{P_0} - \sqrt{Q_0})^2 = (1+o(1)) \frac{ (P_0 - Q_0)^2}{(\sqrt{P_0} + \sqrt{Q_0})^2} = C (1 + o(1)) \frac{ (P_0 - Q_0)^2 }{P_0}$. Therefore, we have that:

\begin{align}
Y_0 \log^2 \frac{Y_0}{P_0} & \leq C I \label{eqn:lower_bound_var_term_1}
\end{align}      

Now we turn our attention to the second term in equation~\ref{eqn:lower_bound_var_terms}:  $(1-Y_0) \int y(x) \log^2 \frac{(1-Y_0) y(x)}{(1-P_0)p(x)} dx $.

First, we observe that
\begin{align*}
\left| \log \frac{(1-Y_0) y(x)}{(1-P_0)p(x)} \right| &\leq 
              \frac{1}{2} \left| \log \frac{ (1-Q_0) q(x)}{(1-P_0)p(x)} - \log Z \right| \\
%
    &\leq \frac{1}{2} \left| \log \frac{1 - Q_0}{1-P_0} \right| + 
               \frac{1}{2} \left| \log \frac{q(x)}{p(x)} \right| + I/2 
\end{align*}

We bound $\log \frac{1- Q_0}{1-P_0}$ and $\log \frac{p(x)}{q(x)}$ separately. 

\begin{align*}
\left| \log \frac{1 - Q_0}{1- P_0} \right| &= \left| \log \left( 1 - \frac{Q_0 - P_0}{1 - P_0} \right) \right|\\ 
  &\stackrel{(a)} \leq \left| \frac{Q_0 - P_0}{1 - P_0} \right| + C \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 \\
  &\stackrel{(b)} \leq C \left| \frac{Q_0 - P_0}{1 - P_0} \right|
\end{align*}

where $(a)$ follows from Lemma~\ref{lem:log_linearize} and the fact that $\frac{1 - Q_0}{1 - P_0}$ is bounded. $(b)$ follows from the fact that $\left| \frac{Q_0 - P_0}{1 - P_0} \right| = \left| 1 - \frac{1 - Q_0}{1 - P_0} \right| \leq 1 + \left| \frac{1 - Q_0}{1 - P_0} \right| \leq C$.

Now,
\begin{align*}
\left| \log \frac{q(x)}{p(x)} \right| &= \left| \log \left( 1 - \frac{p(x) - q(x)}{p(x)} \right) \right| \\
    &\stackrel{(a)} \leq \left| \frac{p(x) - q(x)}{p(x)} \right| + \left( \frac{p(x) - q(x)}{p(x)} \right)^2 C \\
  &\stackrel{(b)} \leq C \left| \frac{p(x) - q(x)}{p(x)} \right|
\end{align*}

where $(a)$ follows from Lemma~\ref{lem:log_linearize} and the fact that $\frac{q(x)}{p(x)}$ is bounded and $(b)$ follows from the fact that $\left| \frac{p(x) - q(x)}{p(x)} \right| = \left| 1 - \frac{q(x)}{p(x)} \right| \leq 1 + \left| \frac{q(x)}{p(x)} \right| \leq C$. 

Therefore,

\begin{align*}
(1 - Y_0) \int y(x) \left( \log \frac{1 - Y_0}{1 - P_0} \frac{y(x)}{p(x)} \right)^2 dx &\leq 
   (1 - Y_0) \int y(x) \left\{ C \frac{| Q_0 - P_0|}{1 - P_0} + C \frac{ | p(x) - q(x)|}{p(x)} + I/2 \right\}^2 dx \\
  &\leq (1 - Y_0) \int y(x) \left\{ C\left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 + 
                     C \left( \frac{p(x) - q(x)}{p(x)} \right)^2 + C I^2 \right\} dx 
\end{align*}

In the last inequality, we used the fact that $(x + y + z)^2 \leq 9x^2 + 9y^2 + 9z^2$. 

Again, we bound this by bounding the following three terms: \textbf{term a:} $(1 - Y_0) \int y(x) C \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 dx$, \textbf{term b:} $(1 - Y_0) \int y(x) C  \left( \frac{ p(x) - q(x)}{p(x)} \right)^2 dx$, and \textbf{term c:} $(1 - Y_0) \int y(x) C I^2  dx$.

\textbf{term a}:
\begin{align*}
(1 - Y_0) \int y(x) C \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 dx &= 
              C \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 \int \frac{\sqrt{(1-P_0)p(x) (1-Q_0)q(x)}}{Z} dx \\
   &\stackrel{(a)} \leq C (1+o(1)) \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 \int \sqrt{\frac{ (1-Q_0 )q(x)}{(1-P_0) p(x)} } (1 - P_0) p(x) dx \\ 
   &\stackrel{(b)} \leq C (1+o(1)) \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 (1 - P_0) \\
   &\leq C (1 + o(1)) \frac{ (Q_0 - P_0)^2}{1 - P_0} 
\end{align*}
In $(a)$, we use the fact that $\frac{1}{Z} = (1 + o(1))$ since $Z \rightarrow 1$. In $(b)$, we use the fact that $\frac{1-Q_0}{1-P_0}$ and $\frac{q(x)}{p(x)}$ are bounded by some absolute constant. 

Now, note that $I \geq (1+o(1)) (\sqrt{1 - P_0} - \sqrt{1- Q_0})^2 =  (1+o(1)) \frac{ (P_0 - Q_0)^2}{(\sqrt{1 - P_0} + \sqrt{1 - Q_0})^2} = C (1 + o(1)) \frac{ (P_0 - Q_0)^2}{1 - P_0}$. Therefore, we have that 
\[
(1 - Y_0) \int y(x) C \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 dx \leq C ( 1 + o(1)) I \leq C I 
\]


Moving on to \textbf{term b}:
\begin{align*}
C (1 - Y_0) \int y(x) \left( \frac{ p(x) - q(x)}{p(x)} \right)^2 dx &= 
         \frac{C}{Z} \int \sqrt{ \frac{1 - Q_0}{1 - P_0} \frac{q(x)}{p(x)} } (1 - P_0) p(x) 
                 \left( \frac{p(x) - q(x)}{p(x)} \right)^2 dx  \\
   &\stackrel{(a)} \leq \frac{C}{Z} (1 - P_0) \int p(x) \left( \frac{p(x) - q(x)}{p(x)} \right)^2 dx 
\end{align*}
where in $(a)$, we used the fact that $\frac{1}{Z} = (1 + o(1))$ and that $\frac{1- Q_0}{1-P_0}$ and $\frac{p(x)}{q(x)}$ are both bounded by an absolute constant by assumption. 
Now, note that $H = \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx 
= \int \frac{ (p(x) - q(x))^2 }{(\sqrt{p(x)} + \sqrt{q(x)})^2 } dx = C \int \frac{(p(x) - q(x))^2}{p(x)} dx $. 


Therefore, we have that
\begin{align*}
C (1 - Y_0) \int y(x) \left( \frac{ p(x) - q(x)}{p(x)} \right)^2 dx &\leq 
        C ( 1 - P_0) H \\
   &\leq C \sqrt{ (1 - P_0)(1 - Q_0)} H \leq C I
\end{align*}


Finally, we have, for \textbf{term c}, that
\begin{align*}
(1 - Y_0) \int y(x) C I^2 dx &= (1 - Y_0) C I^2 \leq C I 
\end{align*}

Therefore, we have that 

\begin{align*}
(1 - Y_0) \int y(x) \left( \log \frac{1 - Y_0}{1 - P_0} \frac{y(x)}{p(x)} \right)^2 dx \leq C I 
\end{align*}

Combining the above bound with \ref{eqn:lower_bound_var_term_1}, we can now 
go back to Equation~\ref{eqn:lower_bound_var_terms} and get that
\begin{align*}
\E_{\Psi} \left( \log \frac{Y(A_{uv^*})}{P(A_{uv^*})} \right)^2 \leq C I 
\end{align*}

Hence, $\sqrt{5 V_{\Psi} (\mathcal{Q})} \leq C \sqrt{ \frac{n I}{\beta K}}$. 



Now, suppose $\frac{nI}{K} \rightarrow \infty$. Then, we have that $\sqrt{\frac{nI}{K}} = o \left( \frac{nI}{K} \right)$ and thus, $\sqrt{ 5 V_{\Psi}(\mathcal{Q})}$ is $o( n I/ K)$. Therefore, we have that $\E_{\Phi} l(\hat{\sigma}, \sigma_0) \geq \frac{1}{4} \exp \left( - (1 + o(1)) \frac{nI}{\beta K} \right)$. 


Suppose $nI/K \rightarrow c < \infty$, then $\E_{\Psi} \mathcal{Q} = c (1 + o(1))$ and $\sqrt{ 5 V_{\Psi}(\mathcal{Q})} \leq C ( 1 + o(1))$. Therefore, 

 $\E_{\Phi} l(\hat{\sigma}, \sigma_0) > c' > 0$ for some constant $c'$. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional useful lemmas}



\begin{lemma}
\label{lem:renyi_hellinger}
Let $I = -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0) p(x) q(x)} dx \right)$ and let $I^{h} = (\sqrt{P_0} - \sqrt{Q_0})^2 + \int \left( \sqrt{ (1-P_0) p(x)} - \sqrt{ (1-Q_0) q(x)} \right)^2 dx$. If $I^h < 2 - 2\epsilon$, then we have that

\[
I = I^h(1+\eta)
\]

where $|\eta| \leq \frac{I^h}{2\epsilon}$. Therefore, we have that $I \rightarrow 0$ iff $I^h \rightarrow 0$ and that if $I \rightarrow 0$, $ I = I^h(1+o(1))$.

\end{lemma}

\begin{proof}

\begin{align*}
I &= -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0) p(x) q(x)} dx \right) \\
  &= -2 \log \left( 1 - \frac{1}{2} \left( 
                (\sqrt{P_0} - \sqrt{Q_0})^2 + 
               \int (\sqrt{(1-P_0)p(x)} - \sqrt{(1-Q_0)q(x)} )^2 dx \right) \right)\\
 &= -2 \log \left(1 - \frac{1}{2} I^h \right) \\
  &= 2 \frac{1}{2} I^h (1 + \eta)
\end{align*}
where $|\eta| \leq \frac{I^h}{2 \epsilon}$. The last equality follows from lemma~\ref{lem:log_linearize}.

\end{proof}

\begin{lemma}
\label{lem:log_linearize}
Suppose $x \geq 0$ and $1 \geq \epsilon > 0$, then we have that, for all $0 \leq x < 1-\epsilon$,
\[
\log (1 - x) = - (1 + \eta) x 
\]
where $| \eta| \leq \frac{x}{2 \epsilon}$
\end{lemma}

\begin{proof}
This follows by taking the Taylor expansion of $\log (1 - x)$ around $x = 0$.
\end{proof}

\begin{lemma}
\label{lem:sqrt_linearize}
Define $f(z) =  \frac{1 - \frac{z}{2} - \sqrt{ 1 - z}}{z} $ for $z \leq 1$ and $z \neq 0$ and define $f(0) = 0$. Then we have that,
\[
\left| f(z)  \right| \leq |z|
\]
for all $z \leq 1$.
\end{lemma}

\begin{proof}

Define $f(z) = \frac{1 - \frac{z}{2} - \sqrt{ 1 - z}}{z}$, where we set $f(0) = 0$. Note that $f$ is continuous. 

The derivative of $f$ is 
\[
f'(z) = - \frac{1}{z^2} - \frac{z - 2}{2 z^2 \sqrt{1-z}}
\]

It is straight forward to check that $f'(z) \geq 0$ for all $z < 1$ and that we can define $f'(0) = \frac{1}{4}$ such that $f'(z)$ is continuous.

Therefore, $f(z)$ is monotonic and maximized at $z = 1$, yielding $f(1) = 1/2$ and minimized at $\lim_{z \rightarrow -\infty} f(z) = -\frac{1}{2}$. 

Now we perform case analysis. Suppose $z < -1/2$, then $|f(z)| \leq \frac{1}{2} < |z|$.

Suppose $-1/2 \leq z \leq 1/2$. By Taylor expansion, we have

\[
\sqrt{1 - z} = 1 - \frac{1}{2} z - \frac{1}{8} z^2 - \frac{1}{16} z^3  - ... \frac{(n+1)!!}{2^n n!} z^n - ... 
\]


Therefore,
\begin{align*}
\left| \sqrt{1-z} - (1 - \frac{z}{2}) \right| &\leq
     \frac{1}{8} (|z|^2 + |z|^3 + ... ) \\
  &\leq \frac{1}{8} |z|^2 ( 1 + |z| + |z|^2 + ...) \\
  &\leq \frac{1}{8} |z|^2 \frac{1}{1 - |z|} \\
  &\leq \frac{1}{4} |z|^2 
\end{align*}

Therefore, $|f(z)| \leq \frac{1}{4} |z|$. 

Finally, suppose $z > 1/2$. Then, 

$|f(z)| \leq \frac{1}{2} < z$. 




\end{proof}






\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
