\documentclass{article}
%\documentclass[12pt,pdftex,generic,noinfoline]{imsart}

%TODO
%Modify algorithm to take into account the noise adding state
%Change the tau and other stuff in algorithm descriptions

\RequirePackage[OT1]{fontenc}
\usepackage{amsthm,amsmath,amsfonts,natbib,mathtools,custom_math,amssymb, bbm}
\RequirePackage{hypernat}
\usepackage[ruled,section]{algorithm}
%\usepackage[noend]{algorithmic}
\usepackage{algpseudocode}
\usepackage{graphicx}
%\usepackage[hscale=0.7,vscale=0.8]{geometry}
\usepackage[text={6.0in,8.6in},centering]{geometry}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage{yhmath}
\usepackage[usenames,dvipsnames]{xcolor}
\definecolor{shadecolor}{gray}{0.9}
\definecolor{shadecolor}{gray}{0.9}
\hypersetup{citecolor=blue}
\hypersetup{linkcolor=blue}
\hypersetup{urlcolor=blue}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{color}
%This is for repeating theorems, lemmas
\makeatletter

\newtheorem*{theorem*}{Theorem}

\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
\newenvironment{rep#1}[1]{%
 \def\rep@title{#2 \ref{##1}}%
 \begin{rep@theorem}}%
 {\end{rep@theorem}}}
\makeatother
%end repeat theorems
\newreptheorem{theorem}{Theorem} %repeat theorem
\newreptheorem{lemma}{Lemma}      %repeat lemma
\newreptheorem{proposition}{Proposition} %repeat proposition  

\newcommand{\bin}{\text{bin}}
\allowdisplaybreaks

\renewcommand{\baselinestretch}{1.045}
%\parskip12pt
%\parindent0pt
\setcounter{tocdepth}{2}

\input{macros}

\begin{document}

\begin{center}
{\bf{\Large{Community Recovery on the Weighted Stochastic Block Model and Its Information-Theoretic Limits}}}

\vspace*{.25in}

\begin{tabular}{ccccc}
{\large{Min Xu$^\dagger$}} & \hspace*{.2in} & {\large{Varun Jog$^\ddagger$}} & \hspace*{.2in} & {\large{Po-Ling Loh$^{\ddagger*}$}} \\
{\large{\texttt{minx@wharton.upenn.edu}}} & & {\large{\texttt{vjog@wisc.edu}}} & & {\large{\texttt{loh@ece.wisc.edu}}}
\end{tabular}

\vspace{.2in}

\begin{tabular}{ccc}
Department of Statistics$^\dagger$ & \hspace{.3in} & Departments of ECE$^\ddagger$ \& Statistics$^*$ \\
The Wharton School && Grainger Institute of Engineering \\
University of Pennsylvania && University of Wisconsin - Madison \\ Philadelphia, PA 19104 & & Madison, WI 53706
\end{tabular}
 
\vspace*{.2in}

May 2017

\vspace*{.2in}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Identifying communities in a network is an important problem in many fields, including social science, neuroscience, military intelligence, and genetic analysis. In the past decade, the Stochastic Block Model (SBM) has emerged as one of the most well-studied and well-understood statistical models for this problem. Yet, the SBM has an important limitation: it assumes that each network edge is drawn from a Bernoulli distribution. This is rather restrictive, since weighted edges are fairly ubiquitous in scientific applications, and disregarding edge weights naturally results in a loss of valuable information. In this paper, we study a weighted generalization of the SBM, where observations are collected in the form of a weighted adjacency matrix, and the weight of each edge is generated independently from a distribution determined by the community membership of its endpoints. We propose and analyze a novel algorithm for community estimation in the weighted SBM based on various subroutines involving transformation, discretization, spectral clustering, and appropriate refinements. We prove that our procedure is optimal in terms of its rate of convergence, and that the misclassification rate is characterized by the Renyi divergence between the distributions of within-community edges and between-community edges. In the regime where the edges are sparse, we also establish sharp thresholds for exact recovery of the communities. Our theoretical results substantially generalize previously established thresholds derived specifically for unweighted block models. Furthermore, our algorithm introduces a principled and computationally tractable method of incorporating edge weights to the analysis of network data.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip10pt


\section{Introduction}


The recent explosion of interest in network data has created a need for new statistical methods for analyzing network datasets and interpreting results~\cite{NewEtal06, DavKle10, Jac10, GolEtal10}. One active area of research with diverse applications in many scientific fields pertains to community detection and estimation, where the information available consists of the presence or absence of edges between nodes in the graph, and the goal is to partition the nodes into disjoint groups based on their relative connectivity~\cite{FieEtal85, HarSha00, PriEtal00, ShiMal00, McS01, NewGir04}.

When studying community recovery, a standard model assumption is that, conditioned on the community labels of the nodes of the graph, edges are generated independently according to distributions that depend only upon the community labels of the endpoints of an edge. This is the setting of the Stochastic Block Model (SBM). We consider in this paper the homogeneous version of SBM, where all within-community edges are generated according to the Bernoulli$(p)$ distribution and all between-communities edges are generated according to the Bernoulli$(q)$ distribution.

 Community recovery on the SBM is the problem of estimating the latent cluster memberships of the nodes from an instance of a network generated by the SBM. Astonishing progress has been made on this problem over the past decade, starting from the seminal conjecture of [CITE]. There now exists algorithms that achieve various notions of optimality, including minimum misclustering error rate, sharp detection threshold, and sharp recovery threshold.

A shortcoming of stochastic block model is that the edges are binary, which means that all edges are given equal weights in the determination of the community structure. On real world networks however, pairwise connections often have varying strength and characteristics; for example, some edges on social networks represent connections between close friends and other edges represent connections between distant acquaintances -- these edges should not be given equal consideration for community recovery. 

Many real world networks possess a natural weight structure that reflect the strength and characteristics of its edges. On social networks or cellular networks for example, the frequency of interactions between the individuals or users could quantify the strength of a connection. On gene co-expression networks, edges have weights that range from -1 to 1 that indicate the correlation between the expression levels of a gene pair. On co-citation network, edges have weights that represent the frequency with which two articles are co-cited. On brain neural networks, edge weights may be taken as the level of neural activity between regions in the brain. The connectivity data could be condensed into an adjacency matrix consisting of only zeros and ones, but this would result in a loss of valuable information that could be used to recover node communities.

In this paper, we consider the \emph{weighted} setting of the stochastic block model, where, after an edge is generated from one of two Bernoulli distributions, it is given an edge weight generated from one of two arbitrary densities $p(x), q(x)$, depending on whether the edge is between-cluster or within-cluster. Under this model, we study the problem of estimating the cluster membership from a weighted network, without knowledge of the weight densities $p(x), q(x)$.

A critical assumption that underlies many of the existing work on weighted networks is that there is a separation between the means of between-cluster edge weight distribution $p(x)$ and within-cluster edge weight distribution $q(x)$. This assumption is common because it allows the application of some of the existing algorithms for the unweighted SBM--such as spectral clustering--to a weighted network with little to no modification. We do not make this assumption in our paper: our setting allows $p(x)$ and $q(x)$ to have the same mean. We make no assumption on the means of $p(x)$ and $q(x)$ not because we think that such assumptions are unreasonable, but because the edge weight distributions $p(x), q(x)$ may significantly differ in other aspects such as variance or higher moments. It is necessary to consider information beyond mean separation in order to achieve optimal performance.

%Some of the clustering algorithms that are designed for unweighted networks--spectral clustering for example--can still accept weighted networks as input. One straightforward approach is to plug a weighted network into these methods but this approach typically requires the assumption that there is a separation between the means of between-cluster edge weight distribution $p(x)$ and within-cluster edge weight distribution $q(x)$. We do not make this assumption in this paper: our setting allows $p(x)$ and $q(x)$ to have the same mean. We make no assumption on the means of $p(x)$ and $q(x)$ not because we think that such assumptions are unreasonable, but because $p(x), q(x)$ may significantly differ in other aspects such as variance or higher moments. It is necessary to utilize all available information and not rely solely on mean separation in order to achieve optimal performance.

Other existing approaches to the weighted networks often assume that the $p(x), q(x)$ belong to a parametric family. In our paper however, we let the densities $p(x), q(x)$ be nonparametric and impose only mild regularity conditions. The fact that we do not assume any parametric form of $p(x), q(x)$ adds a new challenge for community recovery: nonparametric estimation of a density is a difficult problem in its own right and it is made much harder in the weighted SBM because one does not know from which density an edge weight is drawn without knowing the latent cluster structure. 

The goal of our paper is to characterize the optimal rate of misclustering error for the weighted stochastic block model. From one side, we prove an information theoretic lower bound on the performance of any community recovery algorithms on the unweighted SBM. [TODO: mention permutation equivariance] From the other side, we present a computationally tractable algorithm whose rate of convergence matches the lower bound. Our results show that the optimal rate on the weighted SBM is governed by the Renyi-divergence of order 1/2 between two mixed distributions that capture both the divergence between the edge probabilities and the divergence between the edge weight densities $p(x)$ and $q(x)$. This generalizes, in a natural way, the results of Zhang and Zhou~\cite{zhangminimax}, which show the optimal rate of the unweighted SBM to be governed by the Renyi-divergence of order 1/2 between two Bernoulli distributions that capture the divergence between the edge probabilities only. 

Furthermore, we show that the optimal rate for the weighted SBM depends on the densities $p(x), q(x)$ \emph{only through} their Renyi-divergence. This implies that the optimal rate is adaptive to the densities $p(x), q(x)$, that is, it is possible, without knowing $p(x)$ and $q(x)$, to achieve same optimal rate as if one \emph{does} know $p(x), q(x)$. And, in the cases where the densities comes from a parametric family, it is possible, without making any parametric assumptions, to get the same optimal rate as if one imposes the true parametric form. This is in constrast to most nonparametric problems in statistics where the nonparametric method usually have slower rate of convergence than parametric methods in the settings where a parametric form is known. This observation reflects an important intuition behind our results, that on the weighted SBM, one does not need to estimate the densities well in order to recover the clusters well. 

The algorithm that we devise is based on discretization. In the cases where the weights are bounded, we discretize the weights with a uniformly spaced binning to convert the weighted SBM into an instance of colored or labeled SBM, where each of the edges are marked with a color from a set of colors whose cardinality is finite but divergent; we then solve the colored SBM by extending the now familiar coarse-to-fine clustering algorithm that computes an initialization through spectral clustering and then performs refinement through node-wise likelihood maximization. In the cases where the weights are unbounded, we reduce the problem to the bound case: we first apply a transformation on the edge weights so that the transformed weights are bounded. Our lower bound analysis uses and extends the change-of-measure proof technique. Our lower bound result applies to all parameters in the parameter space and is thus more than minimax. It does restrict the space of estimators to algorithms whose output does not depend on how the nodes are labeled -- a property we call permutation equivariance -- but this constraint is a mild one and satisfied by all effective clustering algorithms. 


%A standard assumption in statistical modeling is that conditioned on the community labels of the nodes in the graph, edges are generated independently according to fixed distributions governing the connectivity of nodes within and between communities in the graph. This is the setting of the stochastic block model (SBM)~\cite{HolEtal83, HarEtal76, WasAnd87}. In the homogeneous case, edges follow one distribution when both endpoints are in the same community, regardless of the community label; and edges follow a second distribution when the endpoints are in different communities. The majority of existing literature on stochastic block models has focused on the case where no other information is available beyond the unweighted adjacency matrix, and much work in the information theory and statistics has focused on  deriving thresholds for \emph{exact} or \emph{weak} recovery of community labels in terms of the underlying probability parameters and the size of the graph (e.g., \cite{MosEtal12, MosEtal14, Mas14, AbbEtal14, abbe2014exact, AbbSan15, HajEtal14, HajEtal15, zhangminimax}).
%
%However, the pairwise connections in many real-world networks possess a natural weighting structure~\cite{New04, BocEtal06}. For example, in social networks, information may be available quantifying the strength of a tie, such as the frequency of interactions between the individuals~\cite{Sad72}; in cellular networks, information may be available quantifying the frequency of communication between users~\cite{BloEtal08}; in gene co-expression networks, edges weights range from -1 to 1 and indicate the correlation between the expression levels of a gene pair; and in neural networks, edge weights may symbolize the level of neural activity between regions in the brain~\cite{RubSpo10}. Of course, the connectivity data could be condensed into an adjacency matrix consisting of only zeros and ones, but this would result in a loss of valuable information that could be used to recover node communities.
%
%In this paper, we analyze the ``weighted" setting of the stochastic block model~\cite{aicher2014learning}, where, after an edge is generated from a Bernoulli distribution, it is given an edge weight generated from one of two arbitrary densities $p(x), q(x)$ depending on whether the edge is between-cluster or within-cluster. The weighted SBM presents a serious challenge in the design of algorithms because $p(x), q(x)$ are unknown and must be estimated. Nonparametric estimation of a density is a difficult problem in its own right and it is made much harder in the weighted SBM because one does not know whether an edge weight is drawn from $p(x)$ and $q(x)$ without the latent cluster structure. There are various approaches to the weighted SBM. For example, Newman \cite{New04} assumes that the edge weights have discrete units and then converts a weighted graph into a multigraph; Aicher et al \cite{aicher2014learning} assumes $p(x), q(x)$ to be from a known exponential family and performs variational Bayesian inference. These approaches can be effective but they rely on strong assumptions to simplify the problems and nothing is known about their theoretical properties. 
%
%Our paper proposes a new discretization based approach that imposes weak assumptions and possesses strong guarantees. In the case of finitely-supported distributions, which correspond to a ``labeled" or ``colored" SBM, we demonstrate a method for choosing an initial label on which we apply a standard SBM estimation method to obtain an initial clustering. We then show how to use this initial rough clustering, together with the full set of edge labels, to obtain more accurate estimates of the true cluster assignments. In the case of continuous weight distributions, we propose a discretization strategy that will allow us to apply a recovery algorithm for the labeled case after appropriate preprocessing. Our method does not rely on prior knowledge of the densities $p(x)$ and $q(x)$ and does not rely on parametric assumptions.
%
%Importantly, we show that the output of our algorithm is optimal, in the sense that under mild regularity assumptions on $p(x)$ and $q(x)$, the misclustering error of our algorithm converges to zero at an optimal rate. Our analysis generalizes the results of Zhang and Zhou~\cite{zhangminimax} and Gao et al~\cite{GaoEtal15}, which show that the optimal rate of convergence of unweighted SBM is driven by the Renyi divergence of order $1/2$ between two Bernoulli distributions, corresponding to the probability of generation for within-community and between-community edges. In fact, a similar phenomenon holds for the weighted SBM setting in our paper: the optimal error rate is also driven by a Renyi divergence of order $1/2$ between two mixed distributions that capture both the divergence between the edge probabilities and the divergence between the edge weight densities $p(x)$ and $q(x)$. Note that in order to achieve the optimal error rate, our discretization strategy must be chosen carefully when $p(x)$ and $q(x)$ are continuous distributions. Our proposed algorithm first transforms the distributions to be supported on $[0,1]$, then bins the interval appropriately; in general, since $p(x)$ and $q(x)$ may vary with the size of the graph, the number of bins used will also need to grow slowly as the number of nodes increases. Our results has an interesting implication: although our algorithm is nonparametric, it is adaptive in the sense that it achieves the same optimal rate even if the edge weight densities $p(x), q(x)$ take on a parametric form such as Gaussian or Laplace. This is in contrast to most problems in statistics where nonparametric methods usually have slower rate of convergence than parametric methods in settings where a parametric form is known. This observation captures an important intuition behind our results, that on the weighted SBM, one do not need to estimate the densities well in order to cluster well.
%

%Our method first discretizes the continuous edge weights to convert the weighted network into a labeled network~\cite{yun2016optimal}, which can then be clustered by modifying existing techniques for the unweighted stochastic block model. This method is computationally tractable and does not impose any parametric assumption on the weight densities.

%We also explore the related problem of exact recovery for weighted SBMs. Exact recovery refers to the case where the communities are partitioned perfectly, and a corresponding estimator is called \emph{strongly consistent}. We analyze the performance of our algorithm in the case when the average number of edges scales according to $\Theta(\log n)$, known as the \emph{sparse} regime in SBM literature. Again, we show that the thresholds for exact recovery may be expressed in terms of the Renyi divergence between weighted distributions, in the sense that our algorithm exactly recovers the true community labels when the Renyi divergence exceeds a certain threshold, and every algorithm fails with nontrivial probability when the Renyi divergence lies below the threshold.

%% TODO CONTINUE!

[TODO: fix this to the new structure]
The remainder of the paper is organized as follows: Section~\ref{sec:formulation} introduces the mathematical framework of the weighted stochastic block model and defines the problems which we are trying to solve. Section~\ref{sec:method} describes our proposed algorithm for finding communities on the weighted SBM. In Section~\ref{sec:rate} we provide the statements of our main results concerning the behavior of our algorithm in terms of misclassification error rates and exact recovery. Section~\ref{SecProofs} highlights the key technical components employed in the analysis of our algorithm. We close in Section~\ref{sec:conclusion} with further implications of our work and open questions related to our results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Notation}

For a positive integer $n$, we use $[n]$ to denote the set $\{1, ..., n\}$ and $S_n$ to denote the set of permutations on $[n]$. We let $o(1)$ denote a sequence indexed by $n$ that tends to 0 as $n \rightarrow \infty$ and $\Theta(1)$ denote a sequence indexed by $n$ that is bounded away from 0 and $\infty$ as $n \rightarrow \infty$. For two real numbers $a,b$, we let $a \vee b$ denote $\max(a,b)$ and $a \wedge b$ denote $\min(a,b)$.

\section{Model and problem formulation}
\label{sec:formulation}

We begin with a formal definition of the weighted stochastic block model and a description of the community recovery problem. 

\subsection{Weighted Stochastic Block Model}

We let $n$ denote the number of nodes in the network and $K \geq 2$ denote the number of communities. We suppose that the communities are approximately balanced in that there exists a \emph{cluster-imbalance constant} $\beta$ such that the cluster size $n_k$ for each cluster $k = 1, \dots, K$ satisfies $\frac{\beta n}{K} \geq n_k \geq \frac{n}{\beta K}$.

\begin{definition}
We let $\sigma_0 \,:\, [n] \rightarrow [K]$ denote the true cluster assignment, i.e., for each node $u$, $\sigma_0(u) \in \{1,2, \dots, K\}$ represent the cluster label of $u$. Because a clustering does not depend on how the clusters are labeled, we say that two cluster assignments $\sigma, \sigma'$ are \textbf{equivalent} if there exists a permutation $\tau \in S_K$ such that $\tau \circ \sigma = \sigma'$. \end{definition}


For the homogeneous unweighted stochastic block model, the parameters consists of $(n, K, \beta, \sigma_0)$ in conjunction with between-cluster edge probability $p$ and within-cluster edge probability $q$. The unweighted SBM is then the following probability distribution over adjacency matrices $A \in \{0,1\}^{n \times n}$:

\begin{definition} (Unweighted Homogeneous Stochastic Block Model)\\
For all pairs $\{ (u, v) \,:\, u,v \in [n], u < v \}$, the binary entries $A_{uv}$'s are generated independently as such: 
  \[
A_{uv} \sim \left\{ \begin{array}{cc}
 Ber(p) & \trm{ if } \sigma_0(u) = \sigma_0(v) \\
 Ber(q) & \trm{ if } \sigma_0(u) \neq \sigma_0(v). 
\end{array} \right.
\]
\end{definition}

In the more general case of \emph{heterogenous} SBM, we have a matrix $P \in \R^{K \times K}$ of probabilities instead of two numbers $p,q$. The edge random variables are generated independently as $A_{uv} \sim Ber(P_{\sigma_0(u), \sigma_0(v)})$. We focus on the homogeneous case in this paper and discuss how our results may extend to the heterogenous setting.

To incorporate real-valued edge weights in the SBM, we model each edge weight as a random variable; an edge weight, conditioned on the event that the edge exists, has the density $p(x)$ if the corresponding edge is between-cluster and $q(x)$ if the corresponding edge is within-cluster. The parameters for the weighted SBM consists of $(n, K, \beta, \sigma_0)$ in addition to $P_0, Q_0$, which are the edge \emph{absence} probabilities, and $p(x), q(x)$, which are the edge weight densities. We let $S$ denote the support of $p(x), q(x)$, which could be either $S=[0,1]$, $S = \R$, or $S = \R^+$. The weighted SBM is then the following probability distribution over adjacency matrices $A \in S^{n \times n}$.

\begin{definition}
\label{def:weighted_homo_sbm1}
(Weighted Homogeneous Stochastic Block Model) \\
For all pairs $\{ (u, v) \,:\, u,v \in [n], u < v \}$, 
 we first independently generate the edge presence indicator variables $Z_{uv}$'s:
\[
Z_{uv} \sim 
    \left\{ \begin{array}{cc}
    Ber(1-P_0) & \trm{ if } \sigma_0(u) = \sigma_0(v) \\
    Ber(1-Q_0) & \trm{ if } \sigma_0(u) \neq \sigma_0(v).
   \end{array} \right.
\]
and then we independently generate the real-valued edge weights:
\[
A_{uv} \sim \left\{ \begin{array}{cc} 
     0 & \trm{ if } Z_{uv} = 0 \\
     p(x) & \trm{ if } Z_{uv} = 1 \trm{ and } \sigma_0(u) = \sigma_0(v) \\
     q(x) & \trm{ if } Z_{uv} = 1 \trm{ and } \sigma_0(u) \neq \sigma_0(v). 
\end{array} \right.
\]
\end{definition}

We can define the weighted SBM more succinctly by defining probability measures $P, Q$ to be mixed distributions where the singular part of $P$ is a point mass at $0$ with probability $P_0$ and the continuous part $P$ is $(1-P_0) p(x)$ and likewise for $Q$. Under this representation, the weighted SBM is the following equivalent definition:

\begin{definition} (Weighted Homogeneous SBM)\\
For all pairs $\{ (u, v) \,:\, u,v \in [n], u < v \}$, the real-valued entries $A_{uv}$'s are generated independently as such: 
\begin{align*}
A_{uv} \sim \left\{ 
   \begin{array}{cc} 
   P & \trm{ if } \sigma_0(u) = \sigma_0(v) \\
   Q & \trm{ if } \sigma_0(u) \neq \sigma_0(v)
   \end{array} \right.
\end{align*}
\end{definition}

We observe that if $P, Q$ are Bernoulli distributions without any continuous parts, then the weighted SBM reduces to the unweighted version. It is possible to generalize the weighted SBM to a \emph{weighted and labeled} stochast block model by letting the singular parts of $P, Q$ possess additional point masses. We focus on the weighted SBM in this paper but our theory extends to simple cases of weighted and labeled SBM as well.


\begin{figure}[htp]
\centering
\includegraphics[scale=0.35]{../figs/weightedSBM.jpg}
\caption{Weighted stochastic block model}
\label{fig:weighted_stochastic_block_model}
\end{figure}


\subsection{Community estimation}

Given an observation $A \in S^{n \times n}$ generated from the SBM, the problem of community estimation is to estimate the true cluster membership structure $\sigma_0$. We assume throughout the paper that the number of clusters $K$ is known so that the only information available to a community recovery algorithm consists of $A$ and $K$. [TODO:Discuss choosing $K$]

We evaluate the performance of a community recovery algorithm by looking at its misclustering error. Before formally defining misclustering error, let us first define some notation. For a clustering algorithm $\hat{\sigma}$, let $\hat{\sigma}(A) \,:\, [n] \rightarrow [K]$ be the clustering outputted by $\hat{\sigma}$ when given $A$ as an input. 

\begin{definition}
We define the \emph{misclustering error } to be
\[
l(\hat{\sigma}(A), \sigma_0) \equiv \min_{\tau \in S_K} \frac{1}{n} d_H(\hat{\sigma}(A),\, \tau \circ \sigma_0 ),
\]
where $d_H(\cdot, \, \cdot)$ denotes the Hamming distance. We then define the risk of an estimator as
\[
  R(\hat{\sigma}, \sigma_0) = \E l(\hat{\sigma}(A), \sigma_0)
\]
where the expectation is taken with respect to both the random network $A$ and potential randomness in the algorithm $\hat{\sigma}$. The loss function $l(\cdot, \cdot)$ invovles a minimization over the set of permutations $\tau$ on $K$ cluster labels because clusterings are invariant with respect the labeling of the clusters.
\end{definition}

The goal of this paper is to characterize the minimal achievable risk for community recovery on the weighted SBM in terms of the parameters $(n, K, \beta, \sigma_0, P_0, Q_0, p(x), q(x))$.


\subsection{Permutation Equivariance}

The cluster structure in a network does not depend on how the nodes are labeled. Therefore, it is natural to consider only estimation algorithms that output equivalent clusterings when given graph-isomorphic networks as inputs. We formally define this property as \emph{permutation equivariance}. Permutation equivariance is a natural condition that most papers assume implicitly; we give a formal treatment here in order to state our lower bound.

\begin{definition}
  \label{defn:permutation_equivariance_deterministic}
  For an $n \times n$ matrix $A$ and a permutation $\pi \in S_n$, define $\pi A$ as an $n \times n$ matrix such that $A_{uv} = [\pi A]_{\pi(u), \pi(v)}$. In other words, if for all nodes $u$, one relabels $u$ as $\pi(u)$, then $\pi A$ is the resulting adjacency matrix. 

  Let $\hat{\sigma}$ be a deterministic clustering algorithm; i.e., $\hat{\sigma}(A)$ is a clustering $[n] \rightarrow [K]$ for any $n \times n$ matrix $A$. $\hat{\sigma}$ is \emph{permutation equivariant} if, for any $A$ and any $\pi \in S_n$,
  \[
    \hat{\sigma}(\pi A) \circ \pi \trm{  is equivalent to  } \hat{\sigma}(A)
  \]
\end{definition}
In other words, there exists $\tau \in S_K$ such that $\hat{\sigma}(\pi A) \circ \pi = \tau \circ \hat{\sigma}(A)$, i.e., $l(\hat{\sigma}(A), \hat{\sigma}(\pi A) \circ \pi) = 0$. We note that $\hat{\sigma}(\pi A)$ by itself is not equivalent to $\hat{\sigma}(A)$ because the nodes in $\pi A$ are labeled with respect to the permutation $\pi$.

It is straightforward to extend definition~\ref{defn:permutation_equivariance_deterministic} to randomized algorithms.
\begin{definition}
  \label{permutation_equivariance_stochastic}
A randomized clustering algorithm $\hat{\sigma}$ is permutation equivariant if, for all $A$ and $\pi \in S_n$, and for all functions $\sigma \,:\, [n] \rightarrow [K]$,
\[
P\Big( \hat{\sigma}(A) \trm{ equivalent to }  \sigma \Big) = 
P\Big(  \hat{\sigma}(\pi A) \trm{ equivalent to } \sigma \Big)
\]
where the probability is taken with respect to randomization in the algorithm $\hat{\sigma}$.
\end{definition}

Permutation equivariance is a natural property satisfied by all the clustering algorithms that are proposed in literature except perhaps those that use extra side information in addition to the given network. In section~\ref{sec:lower_bound}, we study permutation equivariance in detail and give some properties of permutation equivariant estimators.


\section{Overview of main results}

Our results are asymptotic in $n$ and we treat $P_0, Q_0, p(x), q(x), \sigma_0$ as varying with $n$; at various places, we do not explicitly show this dependence in our notation in order to simplify the presentation. We hold $K, \beta$ to be fixed with respect to $n$.

Let $P$ be the probability measure on $S$ (recall that $S$ can be $[0,1],$, $\R$, or $\R^+$) induced by $(P_0, p(x))$, that is, the singular part of $P$ is a point mass with probability $P_0$ at $0$ and the continuous part of $P$ has $(1-P_0)p(x)$ as its Radon-Nikodym derivative with respect to the Lebesgue measure. Let $Q$ be defined likewise. The optimal rate of misclustering error naturally depends on the extent to which $P$ and $Q$ are different. The notions of divergence between $P$ and $Q$ that arises in the analysis of the weighted stochastic block model is the Renyi-divergence of order $1/2$, which we denote by $I$:

\begin{align}
  I = -2 \log \int \left( \frac{dP}{dQ} \right)^{1/2} dQ
  = -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0)p(x)q(x) } dx \right)
\end{align}

Since $P,Q$ depends on $n$, $I$ depends on $n$ as well. The fact that $P, Q$ vary with $n$ is analogous to analyses on the unweighted SBM [TODO:cite] where people make the common sparse graph assumption that $p, q \rightarrow 0$ as $n \rightarrow \infty$. Zhang and Zhou [TODO:cite] considers a more general setting where $|p - q|$ is assumed to converge to 0. In similar fashion, we focus on the case where $P$ and $Q$ tend toward each other in the sense that $I \rightarrow 0$; this case encompasses the sparse graph setting where the edge absence probabilities $P_0, Q_0 \rightarrow 1$ as $n \rightarrow \infty$.  [TODO:why large I is uninteresting]

Under the setting where $I = o(1)$, we can characterize $I$ in terms of the Hellinger distance: [TODO:reference lemma]
\begin{align}
I &= \left( (\sqrt{P_0} - \sqrt{Q_0})^2 + \int (\sqrt{(1-P_0)p(x)} - \sqrt{(1-Q_0)q(x)} )^2 dx \right) (1 + o(1)) \nonumber \\ 
 &= \left( \underbrace{(\sqrt{P_0} - \sqrt{Q_0})^2 + (\sqrt{1-P_0} - \sqrt{1-Q_0})^2}_{\textrm{term 1}} + 
             \underbrace{\sqrt{(1-P_0)(1-Q_0)} \int (\sqrt{p(x)} - \sqrt{q(x)} )^2 dx}_{\textrm{term 2}} \right) \nonumber \\
             %
             & \qquad \cdot (1 + o(1)) \label{eqn:I_decompose} 
\end{align}

We make two observations from equation~\ref{eqn:I_decompose}. First, we see that $I$ decomposes into two terms: term 1 captures divergence between the edge presence probabilities and term 2 captures the divergence between the
edge weight densities.

Second, under the weighted SBM, a network whose edges are dense, i.e. $P_0, Q_0$ do not tend to 1, is still interesting to analyze because it could be that the edge weight densities are very similar, i.e. $ \int (\sqrt{p(x)} - \sqrt{q(x)} )^2 dx \rightarrow 0$.




We first state the lower bound.
\begin{theorem*} (Informal Statement) \\
Let $A$ be generated from a weighted SBM with parameters $(n, K, \beta, \sigma_0, P_0, Q_0, p(x), q(x))$. Under regularity condition on the densities $p(x), q(x)$, for any permutation equivariant estimator $\hat{\sigma}$:
\[
  \E l(\hat{\sigma}(A), \sigma_0) \geq \exp\left( - (1 + o(1)) \frac{n I}{\beta K} \right)
\]
\end{theorem*}

For the upper bound, we construct an algorithm $\hat{\sigma}$ whose misclustering error is no more than $\exp\left( - (1 + o(1)) \frac{ n I}{\beta K} \right)$ with high probability.
\begin{theorem*} (Informal Statement)\\
  There exists a permutation equivariant algorithm $\hat{\sigma}$ that satisfies, under regularity conditions on $p(x), q(x)$:
  \[
    P\left( l(\hat{\sigma}(A), \sigma_0) \geq \exp\left( - (1 + o(1)) \frac{n I}{\beta K} \right) \right) \rightarrow 0
  \]

\end{theorem*}

Because the upper bound is stated as convergence in probability, we cannot directly compare it to the lower bound on the risk. However, the same proof of the upper bound can be used to show the following:

\begin{align*}
  & \trm{if $\frac{n I}{\beta K \log n}  \leq 1$} \qquad \E l(\hat{\sigma}(A), \sigma_0) \leq \exp\left( - (1 + o(1)) \frac{n I}{\beta K} \right)  \\
  & \trm{if $\frac{n I}{\beta K \log n}  > 1$} \qquad P\Big( l(\hat{\sigma}(A), \sigma_0) = 0 \Big) \geq 1 - \frac{1}{n} 
\end{align*}

From this, we observe that in the regime where $\frac{n I}{\beta K \log n} \leq 1$, the optimal risk is tightly characterized as $\exp \left( - (1 + o(1)) \frac{ n I}{\beta K} \right)$. In the regime where $\frac{n I}{\beta K \log n} > 1$, the optimal risk is lower bounded by  $\exp \left( - (1 + o(1)) \frac{ n I}{\beta K} \right)$; we have no corresponding upper bound but this second regime is less interesting than the first one because it is possible in this this to recover the true clustering $\sigma_0$ exactly with high probability.

\subsection{Relation to Previous Work}

[TODO: elaborate more]

Our result is a natural generalization of the work by Zhang and Zhou~\cite{zhangminimax} in which they show that the minimax optimal risk for the unweighted stochastic block model is $\exp \left( -(1+o(1)) \frac{ n I_{Ber}}{\beta K} \right)$ where $I_{Ber} = \sqrt{pq} + \sqrt{(1-p)(1-q)}$. If we let $P$ be Bernoulli$(p)$ and $Q$ be Bernoulli$(q)$, then $I$ exactly reduces to $I_{Ber}$. The algorithm that achieves optimality in Zhang and Zhou \cite{zhangminimax} is intractable but a computationally feasible version was later developed by Gao et al \cite{GaoEtal15}. 

Our result is also analogous to that of Yun and Proutiere \cite{yun2016optimal}, who studied, under certain assumptions and with respect to a prior on the cluster assignment $\sigma_0$, the optimal risk for the heterogenous labeled stochastic block model with finite number of labels. They characterize the optimal rate under a seeming different notion of divergence but if we specialize their result to the homogenous setting, we find that their notion of divergence reduces to the Renyi-divergence of order $\frac{1}{2}$ between two discrete distributions over a fixed finite number of labels. 

[TODO: re-structure next paragraph]

Weighted networks have received some attention in the physics community~\cite{New04, barrat2004architecture}, their statistical properties are not well understood. Stochastic block model in particular is rarely studied under the weighted network setting. One exception is the work by the work by Aicher et al \cite{aicher2014learning}, which also proposes a notion of weighted SBM by modeling the edge weights as generated by a distribution from a known exponential family. Hajek et al \cite{hajek2017information} considers a model similar to the one we propose except that it contains one community and that the distributions $P, Q$ are known. 



\subsubsection{Other Notions of Recovery}

A closely related problem is that of finding the exact recovery threshold. We say that the stochastic block model has an exact recovery threshold if there is some function of the parameters $\theta(p, q, n, K, \beta, \sigma_0)$ such that exact recovery is asymptotically almost always impossible if $\theta < 1$ and almost always possible if $\theta > 1$. For the homogeneous unweighted stochastic block model, Abbe et al \cite{abbe2014exact} have shown that, when $\beta=1, K=2, 1 - P_0 = \frac{a \log n}{n}$, and $1 - Q_0 = \frac{b \log n}{n}$  (that is, the average degree is of order $\log n$) for some constant $a,b$, then the exact threshold is $\sqrt{a} - \sqrt{b}$ , that is, no exact recovery algorithm can succeed if $\sqrt{a} - \sqrt{b} < 1$ and there exists a recovery algorithm that can succeed with probability tending to one if $\sqrt{a} - \sqrt{b} > 1$.  This result was generalized by Zhang and Zhou \cite{zhangminimax} beyond the $\log n$ degree setting where $ \frac{n I_{\trm{Ber}}}{K \log n}$ was shown to be the threshold. Apart from exact recovery (also known as strong consistency), a notion of detection threshold has also been considered~\cite{MosEtal14}. 

[TODO: relate our result to exact recovery threshold]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Estimation algorithm}
\label{sec:method}

The weighted stochastic block model presents an extra layer of difficulty because the densities $p(x)$ and $q(x)$ are unknown. One consequence of not knowing $p(x)$ and $q(x)$ is that the MLE does not exist, for the same reason that the MLE does not exist for nonparametric density estimation. This remains true even if we restrict $\mathcal{P}$ to be the set of all smooth densities with, say, bounded second derivatives. A natural thought is to estimate the edge weight densities $p(x)$ and $q(x)$, but this is hindered by the fact that we do not know whether a edge weight observation originates from $p(x)$ or $q(x)$ without knowing the cluster structure.

[TODO: Why can't we just plug the weighted network into spectral clustering?]

[TODO: plug in two sample test?]

Our approach is appreciably different and consists of combining the idea of discretization from nonparametric density estimation with clustering techniques for the unweighted stochastic block model. 

\subsection{Algorithm overview}

We now outline the main components of our algorithm. The key ideas are to convert the edge weights into a finite set of labels by discretization, and then cluster nodes on the labeled network. We first provide a broad overview of our algorithm and then describe each step in detail. Given a weighted network represented as an adjacency matrix $A$, our estimation method has four steps. We summarize the flow of the algorithm below and also in Figure~\ref{fig:method_pipeline1}:


\begin{enumerate}
\item \textbf{Transformation \& discretization.} We take as input a weighted matrix $A$ and apply an invertible transformation function $\Phi \,:\, S \rightarrow [0,1]$ to obtain a matrix $\Phi(A)$ with weights between 0 and 1.

Next, we divide the $[0,1]$ interval into $l=1, \dots, L$ equally-spaced subintervals, which we call bins. We replace the real-valued weight entries $\Phi(A)$ with a categorical label $l \in \{1, \dots, L\}$: $[\Phi(A)]_{uv}$ is assigned label $l$ if the value $[\Phi(A)]_{uv}$ falls into bin $l$. We output a network with each edge assigned one of $L$ possible colors. We continue to denote the adjacency matrix by $A$.

\item \textbf{Add noise.} For a fixed constant $c > 0$, let $\delta = \frac{cL}{n}$. We perform the following process on every edge of the labeled graph, independently of other edges: With probability $1-\delta$, keep an edge as it is, and with probability $\delta$, erase the edge and replace it with an edge with label uniformly drawn from the set of $L$ labels. Again, we continue to denote the modified adjacency matrix as $A$.

\item \textbf{Initialization Parts 1 \& 2.} For each color $l$, we create a sub-network by including only edges of color $l$. For each sub-network, we perform spectral clustering. We output $l^*$, the color that induces the maximally separated spectral clustering. 

Let $A_{l^*}$ be the adjacency matrix for color $l^*$. For each $u \in \{1, \dots, n\}$, we perform spectral clustering on $A_{l^*} \setminus \{u\}$, which denotes the adjacency matrix with vertex $u$ removed. We output $n$ clusters $\tilde{\sigma}_1, \dots, \tilde{\sigma}_n$, where $\tilde \sigma_u$ is a clustering on $\{1, 2, \dots, n\} \setminus \{u\}$, for $1 \leq u \leq n$.

\item \textbf{Refinement \& consensus.} From each $\tilde \sigma_u$, we generate a clustering $\hat \sigma_u$ on $\{1, 2, \dots, n\}$ which retains the assignments specified by $\tilde \sigma_u$ for $\{1, 2, \dots, n\} \setminus \{u\}$, and assigns $\hat \sigma_u(u)$ by maximizing the likelihood taking into account only the neighborhood around $u$. 

We then align the cluster assignments made in the previous step. 
\end{enumerate}

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4, trim={0 6.5in 0 0}]{../figs/method_pipeline1.pdf}
\caption{\textcolor{red}{Add a box indicating the add noise step. Pipeline for the our proposed algorithm}}
\label{fig:method_pipeline1}
\end{figure}

\subsection{Transformation and discretization}

These two steps are straightforward: In the transformation step, we apply an invertible CDF function $\Phi \,:\, \mathbb{R} \rightarrow [0,1]$ as the transformation function onto all the edge weights so that each entry of $\Phi(A)$ lies in the interval $[0,1]$. In the discretization step, we divide the interval $[0,1]$ into $L$ equally-spaced bins of the form $[a_l, b_l],$ where $a_1 = 0, b_L = 1$, and $b_l - a_l = \frac{1}{L}$. An edge is assigned the label $l$ if the weight of that edge lies in bin $l$. 

\begin{algorithm}
\caption{Transformation and Discretization}
\label{alg:transform_and_discretize}
\textbf{Input:} A weighted network $A$, a positive integer $L$, and an invertible function $\Phi \,:\, \mathbb{R} \rightarrow [0,1]$.\\
\textbf{Output:} A labeled network $A$ with $L$ labels\\

\begin{algorithmic}
\State Divide $[0,1]$ into $L$ bins, labeled $Bin_1, \dots, \bin_l$.
\For{every edge $(u,v)$}
   \State let $l$ be the bin in which $\Phi(A_{uv})$ falls.
   \State Give the edge $(u,v)$ the label $l$ in the labeled network $A$
\EndFor
\State Output $A$
\end{algorithmic}
\end{algorithm}

\subsection{Add noise}

This part of the algorithm is required for technical reasons. As detailed in the proof of Proposition~\ref{prop:labeled_sbm_rate} in Appendix~\ref{appendix: first}, deliberately forming a noisy version of the graph barely affects the separation between the distributions specifying within- and between-community edge labels, but has the desirable effect of ensuring that all edge labels occur with probability at least $\frac{c}{n}$. This property is crucial to our analysis in subsequent steps of the recovery algorithm.

In the description of the algorithm below, we treat the label 0 (i.e., an empty edge) as a separate label, so we have a network with a total of $L+1$ labels.

\begin{algorithm}
\caption{Add noise}
\label{alg:noisify}
\textbf{Input:} A labeled network with $L+1$ labels and a constant $c$\\
\textbf{Output:} A labeled network $A$ with $L+1$ labels\\

\begin{algorithmic}
\For{every edge $(u,v)$}
   \State With probability $1-\frac{c(L+1)}{n}$, do nothing. With probability $\frac{c(L+1)}{n}$ replace edge label with a label drawn uniformly at random from $\{0, 1, 2, \dots, L\}$
\EndFor
\State Output $A$
\end{algorithmic}
\end{algorithm}

\subsection{Initialization}

The initialization procedure takes as input a network with edges labeled $\{1, \dots, L\}$. The goal of the initialization procedure is to create a rough clustering $\tilde{\sigma}$ that is suboptimal but still consistent. As outlined in Algorithm~\ref{alg:initialization1}, the rough clustering is based on a single label $l^*$, selected based on the maximum value of the estimated Renyi divergence between within-community and between-community distributions for the unweighted SBMs based on individual labels.

For technical reasons, we actually create $n$ separate rough clusterings $\{\tilde{\sigma}_u \}_{u = 1, \dots, n}$, where each $\tilde{\sigma}_u \,:\, [n-1] \rightarrow [K]$ is a clustering of a network of $n-1$ nodes where node $u$ has been removed. The clusterings $\{\tilde{\sigma}_u\}$ will later be combined into a single clustering algorithm.

\begin{algorithm}[h!]
\caption{Initialization}
\label{alg:initialization1}
\textbf{Input:} A labeled network $A$ with $L$ labels \\
\textbf{Output:} A set of clusterings $\{ \tilde{\sigma}_u \}_{u=1, \dots, n}$, where $\tilde \sigma_u$ is a clustering on $\{1, 2, \dots, n\} \setminus \{u\}$\\

\begin{algorithmic}[1]
\State Separate $A_L$ into $L$ networks $\{ A_l \}_{l=1, \dots, L}$ where $A_l$ contains only edges with label $l$.  \Comment{Stage 1}
\For{each label $l$}
   \State Compute $\bar{d} = \frac{1}{n} \sum_{u=1}^n d_u$ as the average degree.
   \State Perform spectral clustering with $\tau = \bar{d}$ and $\mu \geq C\beta$ to get $\tilde{\sigma}_l$, where $C$ is an appropriately chosen large constant
   \State estimate $\hat{P}_l = 
             \frac{ \sum_{u \neq v \,:\, \tilde{\sigma}_l(u) = \tilde{\sigma}_l(v) } (A_l)_{uv} }
                  { |{u \neq v \,:\, \tilde{\sigma}_l(u) = \tilde{\sigma}_l(v) }| }$ and 
               $\hat{Q}_l = 
            \frac{ \sum_{u \neq v \,:\, \tilde{\sigma}_l(u) \neq \tilde{\sigma}_l(v) } (A_l)_{uv} }
              { |{u \neq v \,:\, \tilde{\sigma}_l(u) \neq \tilde{\sigma}_l(v) }| }$. 
   \State $\hat{I}_l \leftarrow 
               \frac{ (\hat{P}_l - \hat{Q}_l)^2}{\hat{P}_l \vee \hat{Q}_l}$
\EndFor
\State Choose $l^* = \argmax_l \hat{I}_l$. Let $A_{l^*}$ be the network with only edges labeled $l^*$
\For{each node $u$}  \Comment{Stage 2}
   \State Create network $A_{l^*} \setminus \{u\}$ by removing node $u$ from $A_{l^*}$
   \State Perform \textsc{spectral clustering} on $A_{l^*} \setminus \{ u \}$ to get $\tilde{\sigma}_u$
\EndFor 
\State Output the set of clusterings $\{ \tilde{\sigma}_u \}_{u=1, \dots, n}$.
\end{algorithmic}
\end{algorithm}

\paragraph{\textbf{\textsc{Spectral clustering}:}} Note that Algorithm~\ref{alg:initialization1} involves several applications of \textsc{spectral clustering}. We describe the spectral clustering algorithm used as a subroutine in Algorithm~\ref{alg:spectral} below:

\begin{algorithm}[h!]
\caption{\textsc{Spectral clustering}}
\label{alg:spectral}
\textbf{Input:} An unweighted network $A$, trim threshold $\tau$, number of communities $K$, tuning parameter $\mu$ \\
\textbf{Output:} A clustering $\sigma$ \\

\begin{algorithmic}[1]
\State For each node $u$ whose degree $d_u \geq \tau$, set $A_{uv} = 0$ to get $T_{\tau}(A)$
\State Let $\hat{A}$ be the best rank-$K$ approximation to $T_{\tau}(A)$ in spectral norm, formed by truncating the SVD
\State For each node $u$, define the neighbor set $N(u) = \{ v \,:\, \| \hat{A}_u - \hat{A}_v \|_2^2 \leq \mu K^2 \frac{\bar{d}}{n} \}$
\State Initialize $S \leftarrow 0$. Select node $u$ with the most neighbors and add $u$ into $S$ as $S[1]$
\For{$i = 2, \dots, K$}
    \State Among all $u$ such that $|N(u)| \geq \frac{n}{\mu K}$, select 
           $u^* = \argmax_u \min_{v \in S} \| \hat{A}_u - \hat{A}_v \|_2$
    \State Add $u^*$ into $S$ as $S[i]$.
\EndFor
\For{$u = 1, \dots, n$}
    \State Take $\argmin_i \| \hat{A}_u - \hat{A}_{S[i]} \|_2$ and assign $\sigma(u) = i$
\EndFor
\end{algorithmic}
\end{algorithm}

Importantly, note that we may always choose the parameter $\mu$ sufficiently large such that Algorithm~\ref{alg:spectral} generates a set $S$ with $|S| = K$.

\subsection{Refinement and consensus}

Our refinement and consensus steps closely follow the method described by Gao et al.~\cite{GaoEtal15}. In the refinement step, we use the set of initial clusterings $\{\tilde{\sigma}_u\}_{u=1, \dots, n}$ to generate a more accurate clustering for the labeled network by locally maximizing an approximate log-likelihood expression for each of the nodes $u=1, \dots, n$. The consensus step then resolves a cluster label consistency problem arising after the refinement stage. 

\begin{algorithm}
\caption{Refinement}
\label{alg:refinement}
\textbf{Input:} A labeled network $A$ and a set of rough clusterings $\{\tilde{\sigma}_u\}_{u=1, \dots, n}$, where $\tilde \sigma_u$ is a clustering on the set $\{1, 2, \dots, n\} \setminus \{u\}$ for each $u$ \\
\textbf{Output:} a clustering $\hat{\sigma}$ over the whole network

\begin{algorithmic}[1]
\For{each node $u$}
   \State Estimate $\{ \hat{P}_l, \hat{Q}_l\}_{l=0,...,L}$ from $\tilde{\sigma}_u$
   \State Let $\hat{\sigma}_u : [n] \rightarrow [K]$ where 
       $\hat{\sigma}_u(v) = \tilde{\sigma}_u(v)$ for all $v \neq u$ and 
   \[
    \hat{\sigma}_u(u) = \argmax_k \sum_{v \,:\, \tilde{\sigma}_u(v) = k ,\, v\neq u} 
         \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
     \]    
\EndFor 
\State Let $\hat{\sigma}(1) = \hat{\sigma}_1(1)$  \Comment{Consensus Stage}
\For{each node $u \neq 1$}
\[
\hat{\sigma}(u) = \argmax_k | \{ v \,:\,  \hat{\sigma}_1(v) = k \} \cap
                                 \{ v \,:\, \hat{\sigma}_u(v) = \hat{\sigma}_u(u) \}|
\]
\EndFor
\State Output $\hat{\sigma}$
\end{algorithmic}

\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Optimal Misclustering Error}
\label{sec:rate}

In this section, we give the formal statements of our main results. 

Literatures on the unweighted SBM typically study the sparse graph case where the within-cluster probability $p$ and the between-cluster probability $q$ are dependent on $n$ and both converging to 0. Zhang and Zhou~\cite{zhangminimax} considers a more general setting where $|p - q|$ is assumed to converge to 0. In similar fashion, we also assume that the probability measures $P, Q$ vary with $n$ tend toward each other in the sense that $I \rightarrow 0$.

In this section, we explicitly use subscript to denote all quantities that vary with $n$. For example, we write $p_n(x), q_n(x)$ in place of $p(x), q(x)$ and we write
\[
  I_n = -2 \log \left( \sqrt{P_{0,n} Q_{0,n}} + \int_S \sqrt{(1-P_{0,n})(1-Q_{0,n})p_n(x) q_n(x)} dx \right)
\]

\paragraph{\textbf{Assumption A0:}} There exist absolute constants $c_0 > 0$ such that $\frac{1}{c_0} \leq \frac{1-P_{0,n}}{1-Q_{0,n}} \leq c_0$. If $P_{0,n} \vee Q_{0,n} > 0$, we also assume $\frac{1}{c_0} \leq \frac{P_{0,n}}{Q_{0,n}} \leq c_0$.\\

Assumption A0 states that the density of within-community edges has the same order as the density of between-community edges. This assumption is standard in the existing literature on unweighted stochastic block models.



Let $\mathcal{P}$ be the set of all densities on $S$ and let $(\mathcal{P} \times \mathcal{P})^\infty$ denote the set of infinite sequences of density pairs, i.e.,  $(\mathcal{P} \times \mathcal{P})^\infty = \left\{
  (p_n, q_n)_{n = 1,..., \infty} \,:\, p_n, q_n \geq 0,\, \int_S p_n d\lambda = \int_S q_n d\lambda = 1 \right\}$, where $\lambda$ denotes the Lebesgue measure.


\subsection{Upper Bound}

\begin{definition}
Let $S$ be either $[0,1]$ or $\R$ or $\R^+$. We say that $\Phi \,:\, S \rightarrow [0,1]$ is a \emph{transformation function} if it is a differentiable bijection (hence a cumulative distribution function), and $\phi = \Phi'$ satisfies (a) $\int_S \phi(x)^s dx < \infty$ for any $0 < s < 1$, and (b) $\left| \frac{\phi'(x)}{\phi(x)} \right|$ is bounded. 
\end{definition}

For $S = [0,1]$, we always take $\Phi$ as the identity. For $S = \R$ or $\R^+$, $\Phi$ must be chosen so that $\phi$ is not too heavy tailed in order to satisfy (a) and not too light tailed in order to satisfy $(b)$. For example, we may take $\phi$ as:
\begin{align}
\phi(x) = \frac{e^{1-\sqrt{x+1}}}{4},  & 
     \qquad \textrm{when $S = \R^+$}, \label{eqn:phi_defn1} \\ 
\phi(x) = \frac{e^{1-\sqrt{|x|+1}}}{8}, & 
   \qquad \textrm{when $S = \R$.} \label{eqn:phi_defn2}
\end{align}
The transformation function $\Phi$ induces a probability measure on $S$, and we let $\Phi \{ \cdot \}$ denote the $\Phi$-measure of a set.


\begin{definition}
We call a nonnegative function $f(x)$ is $(c_{s1}, c_{s2}, C_s)$-\emph{bowl-shaped} if $f(x)$ is nonincreasing for all $x \leq c_{s1}$, nondecreasing for all $x \geq c_{s2}$, and bounded by $C_s$ for all $x \in [c_{s1}, c_{s2}]$.
\end{definition}

[TODO: describe the two cases on $H_n$]

\subsubsection{The case $H_n = \Theta(1)$}


We state the required assumptions and then present our main result.


Define $\mathcal{G}^{\textrm{upper}}_\Phi \subset (\mathcal{P} \times \mathcal{P})^\infty$ as sequences of densities $p_n(x), q_n(x)$ that satisfy $H \equiv \int_S (\sqrt{p_n(x)} - \sqrt{q_n(x)})^2 dx = \Theta(1)$ as well as the following regularity conditions:
%\paragraph{\textbf{Regularity conditions:}} 

\begin{enumerate}
\item[A1] $p_n(x), q_n(x) > 0$ on the interior of $S$, $\sup_x \left\{p_n(x) \vee q_n(x)\right\} < \infty$, and $\inf_{x \in S} \left\{\frac{p_n(x) \vee q_n(x)}{\phi(x)}\right\} < \infty$. 
\item[A2] For an absolute constant $r \geq 8$, $\int \left| \log \frac{p_n(x)}{q_n(x)} \right|^r \phi(x) dx < \infty$.
\item[A3] There exists a function $h_n(x)$ such that
\begin{itemize}
\item[(a)] $h_n(x) \geq \max \left\{  \left|\frac{q'_n(x)}{q_n(x)} \right|,  \left|\frac{p'_n(x)}{p_n(x)}\right| \right\} $,
\item[(b)] $h_n(x)$ is $(c_{s1}, c_{s2}, C_s)$-bowl-shaped for some absolute constants $c_{s1}, c_{s2}$, and $C_s$, and
\item[(c)] For some constant $t$ such that $\frac{4}{r} < 2t < 1$, we have
$$ \sup_n \int |h_n(x)|^{2t} \phi(x) dx < \infty.$$
\end{itemize}

\item[A4]  $(\log p_n)'(x), (\log q_n)'(x) \geq (\log \phi)'(x)$ for all $x < c_{s1}$, and $ (\log p_n)'(x), (\log q_n)'(x) \leq (\log \phi)'(x)$ for all $x > c_{s2}$.
\end{enumerate}

We also note that we allow $c_{s1} = 0$ in the case where $S=\R^+$ and we allow $c_{s1} = 0$ and $c_{s1}=1$ in the case where $S = [0,1]$.

These conditions depend on the choice of $\Phi$, but it is enough to choose $\phi$ as a heavy-tailed density where all moments exist in order for the class $\mathcal{G}^{\textrm{upper}}_\Phi$ to be very broad. In particular, we show in section~\ref{sec:examples_unbounded_support} that choosing $\Phi$ as \ref{eqn:phi_defn1} or \ref{eqn:phi_defn2} suffices for $\mathcal{G}^{\textrm{upper}}_\Phi$ to encompass Gaussian, Laplace, and other broad classes of densities. 

We then have our upper bound.
\begin{theorem}
  \label{thm:weighted_sbm_rate2}
Suppose $I_n \rightarrow 0$ and $n I_n \rightarrow \infty$. Let $\hat{\sigma}$ be the algorithm described in section~\ref{sec:method} with transformation function $\Phi$ and discretization level $L_n$ chosen such that $L_n \rightarrow \infty$ and $\frac{n I_n}{ L_n \exp(L_n^{1/r}) } \rightarrow \infty$. Suppose that $P_{0,n}$ and $Q_{0, n}$ satisfy Assumption A0 and $(p_n(x), q_n(x))_n \in \mathcal{G}^{\textrm{upper}}_\Phi$, that is, they satisfy Assumptions A1--A4 with respect to $\Phi$. Then,
\[
\lim_{n \rightarrow \infty} P \left\{
     l(\hat{\sigma}(A), \sigma_0) \leq \exp\left( - \frac{nI_n}{\beta K} (1 + o(1)) \right)
    \right\} \rightarrow 1.
\]
\end{theorem}
For the proof of Theorem~\ref{thm:weighted_sbm_rate2}, see Appendix~\ref{AppThmRate2}.



\subsubsection{The case $H_n = o(1)$}


We begin by stating the required assumptions.


Define $\mathcal{G}^{\textrm{upper} \prime}_\Phi \subset (\mathcal{P} \times \mathcal{P})^\infty$ as sequences of densities $p_n(x), q_n(x)$ that satisfy $H_n \equiv \int_S (\sqrt{p_n(x)} - \sqrt{q_n(x)})^2 dx = o(1)$ as well as the following regularity conditions:

%\noindent \textbf{Regularity conditions:} 
\begin{enumerate}
\item[A1'] $p_n(x), q_n(x) > 0$ on the interior of $S$, $\sup_n \sup_x \left\{p_n(x) \vee q_n(x)\right\} < \infty$, and $\sup_n \inf_{x \in S} \left\{\frac{p_n(x) \vee q_n(x)}{\phi(x)}\right\} < \infty$. 
\item[A2'] There is an absolute constant $\rho$ such that there exist subintervals $R_n$ of $S$ where
\begin{itemize}
\item[(a)] $\frac{1}{\rho} \leq \frac{p_n(x)}{q_n(x)} \leq \rho$ for $x \in R_n$ and
\item[(b)] $\Phi\{R_n^c\} = o(H_n)$.
\end{itemize}
\item[A3'] Denoting $\alpha_n^2 = \int_{R_n} q_n(x) \left( \frac{p_n(x) - q_n(x)}{q_n(x)} \right)^2 dx$ and $\gamma(x) = \frac{q_n(x) - p_n(x)}{\alpha_n}$, there exists an absolute constant $r > 4$ such that
$$\sup_n \int_{R_n} q_n(x) \left| \frac{\gamma_n(x)}{q_n(x)} \right|^r dx  < \infty,$$
\item[A4'] There exists a function $h(x)$ such that
\begin{itemize}
\item[(a)] $h_n(x) \geq \max \left\{  \left|\frac{\gamma_n'(x)}{q_n(x)} \right|,  \left|\frac{q'_n(x)}{q_n(x)}\right|, \left | \frac{\gamma_n(x)}{q_n(x)} \right|  \right\} $, 
\item[(b)] $h_n(x)$ is $(c_{s1}, c_{s2}, C_s)$-bowl-shaped for absolute constants $c_{s1}, c_{s2}$, and $C_s$, and
\item[(c)] for an absolute constant $t$ where $\frac{4}{r} < 2t  < 1$, we have
$$\sup_n \int_{R_n} |h_n(x)|^{2t} \phi(x) dx < \infty.$$
\end{itemize}

\item[A5']  $(\log p_n)'(x), (\log q_n)'(x) \geq (\log \phi)'(x)$ for all $x \leq c_{s1}$, and $ (\log p_n)'(x), (\log q_n)'(x) \leq (\log \phi)'(x)$ for all $x \geq c_{s2}$.
\end{enumerate}
Again, we note that we allow $c_{s1} = 0$ in the case where $S=\R^+$ and we allow $c_{s1} = 0$ and $c_{s1}=1$ in the case where $S = [0,1]$.

We have the following result:
\begin{theorem}
  \label{thm:weighted_sbm_rate1}
  Suppose $I_n \rightarrow 0$ and $nI_n \rightarrow \infty$. 
  Let $\hat{\sigma}$ be the algorithm described in Section~\ref{sec:method} with transformation $\Phi$ and discretization level $L_n$ chosen such that $L_n \rightarrow \infty$, $L_n = o(\frac{1}{H_n})$, and $L_n = o(nI_n)$. Suppose $P_{0,n}, Q_{0,n}$ satisfy Assumption A0 and $(p_n(x), q_n(x))_n \in \mathcal{G}^{\textrm{upper} \prime}_\Phi$, that is, they satisfy Assumptions A1'--A5' with respect to $\Phi$. Then,
\[
\lim_{n \rightarrow \infty} P \left\{
     l(\hat{\sigma}(A), \sigma_0) \leq \exp\left( - \frac{nI_n}{\beta K} (1 + o(1)) \right)
    \right\} \rightarrow 1.
\]
\end{theorem}
The proof of Theorem~\ref{thm:weighted_sbm_rate1} is outlined in Appendix~\ref{AppThmRate1}. 


\subsubsection{Additional discussion of assumptions}

It is crucial to note that our algorithm does not require any prior knowledge about the form of $p_n(x)$ and $q_n(x)$: the same algorithm and guarantees apply so long as $(p_n(x), q_n(x))_n \in \mathcal{G}^{\textrm{upper}}_\Phi$, i.e. they satisfy conditions A1-A4 or if  $(p_n(x), q_n(x))_n \in \mathcal{G}^{\textrm{upper}\prime}_\Phi$, i.e. they satisfy conditions A1'-A5'. As we show in section~\ref{sec:examples_unbounded_support}, the conditions are mild and satisfied by the Gaussian, Laplace scale and location family, Gamma shape and scale family, as well as other broad families of distributions.

To aid the reader, we now provide a brief, non-technical interpretation of the regularity conditions described above.

\paragraph{\textbf{Interpretation of Conditions A1--A4, A1'--A5':}}

\begin{enumerate}
\item[A1] Condition A1 is simple; the last part states that $\phi$ must have a tail at least as heavy as that of $p_n(x)$ and $q_n(x)$. 
\item[A2] Condition A2 requires the likelihood ratio to be integrable. It is analogous to a bounded likelihood ratio condition but much weaker.
\item[A3] Condition A3 controls the smoothness of the derivatives of $\log p_n(x)$ and $\log q_n(x)$. We add a mild bowl-shape constraint for technical reasons related to the analysis of binning. 
\item[A4]  Condition A4 is a mild shape constraint on $p_n(x)$ and $q_n(x)$. When $S=\R$, this condition essentially requires $p_n, q_n$ to be monotonically increasing in $x$ for $x \rightarrow -\infty$ and decreasing in $x$ for $x \rightarrow \infty$.
\end{enumerate}

An analogous interpretation may be used to describe to the conditions A1'--A5'. We must pay special attention to A2' and A3' however because of the $H_n = o(1)$ assumption. In condition A2', we require that the likelihood ratio $\frac{p_n(x)}{q_n(x)}$ be bounded away from 0 and $\infty$ except on a region $ R_n^c \subseteq \R_n$. Since $H \rightarrow 0$, we the densities $p_n(x)$ and $q_n(x)$ are becoming increasingly similar and $R_n^c$ is shrinking. We require that the measure of $R_n^c$, with respect to $\Phi$, shrinks faster than $H$. This condition intuitively states that $|\frac{p_n(x)}{q_n(x)}|$ and its reciprocal tend to infinity slowly with respect to $x$. %For heavier-tailed functions $\Phi$,  A2' is a stronger condition on $\frac{p_n(x)}{q_n(x)}$; for lighter-tailed functions $\Phi$, Assumption A2' is a looser condition.

In condition A3', note that $H \rightarrow 0$ implies $\alpha \rightarrow 0$, as well, so $\gamma_n(x) = \frac{p_n(x) - q_n(x)}{\alpha_n}$ is a function of constant order. The integrability condition on $\gamma_n(x)$ states that $|p_n(x) - q_n(x)|$ must converge to 0 almost uniformly for all $x$ in the region $R_n$. (Having an $L_\infty$-bound on $\gamma_n$ would imply uniform convergence.) 

[TODO:how does choice of $\Phi$ affect anything?]

%%%%%%


\subsubsection{Examples for $S = [0,1]$}
\label{sec:examples_bounded_support}

When $S=[0,1]$, we always take $\Phi$ as the identity. The transformation is not needed when $S=[0,1]$, but we still define $\Phi$ so that we can present the results in a unified format.

In the case where $H_n = \Theta(1)$, the simplest example of $p_n(x), q_n(x)$ that satisfy conditions A1-A4 is if $p_n(x), q_n(x)$ are bounded away from 0 and $\infty$ on $[0,1]$ uniformly in $n$ and if $|p_n'(x)|, |q_n'(x)|$ are also bounded uniformly in $n$. A1-A4 however is much more general in that it allows $p_n, q_n$ to be 0 and $p_n', q_n'$ to be infinity at the boundary points $0, 1$. In the case where $H_n = o(1)$, $p_n, q_n$ satisfy A1'-A5' if they satisfy the boundedness conditions described above and if the function $x \mapsto \frac{p_n(x) - q_n(x)}{\| p_n - q_n\|_2}$ is also, uniformly in $n$, bounded away from 0 and infinity in its value and bounded away from infinity in its first derivative.

\subsubsection{Examples for $S = \R$ or $\R^+$}  
\label{sec:examples_unbounded_support}

We start with a proposition that allows us to generate large classes of examples. The proposition considers the case where the $p_n, q_n$ both belong to a family of distributions that is smoothly parametrized by a parameter vector $\theta$.

Let $f_\theta \,:\, S \rightarrow \R$ be a class of function indexed by a parameter vector $\theta \in \Theta \subseteq \R^{d_{\Theta}}$. Suppose $\Theta$ is compact and the class $\{ f_{\theta} \}_{\theta}$ satisfy the following conditions with respect to $\Phi$:

\begin{enumerate}
\item[B1] $\inf_{\theta \in \Theta} \inf_{x} \left\{\log \phi(x) - f_\theta(x) \right\} > -\infty$.
\item[B2] The Fisher information matrix $G_\theta := \int_S (\nabla_{\theta} f_{\theta}(x)) 
                                    (\nabla_{\theta} f_{\theta}(x))^\tran 
                      \exp( f_{\theta}(x)) dx$
is full-ranked:
   \[
 0<   c_{\min} <  \inf_{\theta \in \Theta} \lambda_{min}(G_\theta) \leq \sup_{\theta \in \Theta} \lambda_{max}(G_{\theta}) < c_{\max} < \infty.
\]
for absolute constants $c_{\min}$ and $c_{\max}$.
\item[B3] There exist $g_1(x) \geq \sup_{\theta \in \Theta} \| \nabla_\theta f_\theta(x) \|$ and $g_{2,\theta}(x) \geq \max\left\{\| \nabla_\theta f'_\theta(x) \|, |f'_\theta(x)| \right\}$  such that $g_1$ and $g_{2, \theta}$ are $(c_{s1}, c_{s2}, \tilde{C}_s)$-bowl-shaped, and
\[
   \int g_1(x)^r \phi(x) dx < \infty, \quad \text{and} \quad 
   \sup_{\theta \in \Theta} \int g_{2, \theta}(x)^{4t} \phi(x) dx < \infty,
\]
where $t$ and $r$ are constants satisfying $\frac{8}{r} \le 4t < 1$ .
\item[B4] $\inf_{\theta \in \Theta} f'_\theta(x) \geq (\log \phi)'(x) $ for all $x \leq c_{s1}$, and $\sup_{\theta \in \Theta} f'_\theta(x) \leq (\log \phi)'(x) $ for all $x \geq c_{s2}$.
\end{enumerate}

We then have the following result, proved in Appendix~\ref{sec:theta_rate_proof}:

\begin{proposition}
\label{prop:theta_rate}
Let $\{f_\theta\}_{\theta \in \Theta}$ be a class of functions that satisfy assumptions B1--B5 with respect to a transformation function $\Phi$. Let $\theta_{1,n}, \theta_{0,n}$ be two sequences of parameters in $\Theta$. Let $p_n(x) = \exp( f_{\theta_{1,n}}(x))$ and $q_n(x) = \exp( f_{\theta_{0,n}}(x))$. Then, we have:
\begin{enumerate}
\item[(a)] If $\| \theta_{1,n} - \theta_{0,n} \|_2 = \Theta(1)$, then the sequence $(p_n, q_n) \in \mathcal{G}_{\Phi}^{\textrm{upper}}$.
\item[(b)] If $\| \theta_{1,n} - \theta_{0,n} \|_2 = o(1)$, then the sequence $(p_n, q_n) \in \mathcal{G}_{\Phi}^{\textrm{upper}\prime}$.
\end{enumerate}
\end{proposition}

Proposition~\ref{prop:theta_rate} is useful for generating various examples of densities belonging to $\mathcal{G}_{\Phi}^{\textrm{upper}}$ or $\mathcal{G}^{\textrm{upper} \prime}_{\Phi}$. For all the examples we show, it suffices to choose the transformation function as:

\begin{align}
\phi(x) = \frac{e^{1-\sqrt{x+1}}}{4},  & 
     \qquad \textrm{when $S = \R^+$}, \label{eqn:phi_defn1} \\ 
\phi(x) = \frac{e^{1-\sqrt{|x|+1}}}{8}, & 
                                          \qquad \textrm{when $S = \R$.} \label{eqn:phi_defn2}
\end{align}
These functions $\phi$ are similar to a generalized normal density, modified so that $\left| \frac{\phi'(x)}{\phi(x)} \right|$ is bounded. It is easy to verify that $\Phi(x) = \int_0^x \phi(t) dt$ (respectively, $\Phi(x) = \int_{-\infty}^x \phi(t) dt$) is a valid transformation function.

\begin{example}
\label{ExaLocScale}
(Location-scale family over $\R$)\\
Let $\exp(f(x))$ be a positive base density over $\R$. Define $\theta = (\mu, \sigma)$ and $\Theta = [- C_{\mu}, C_{\mu}] \times \left[\frac{1}{c_{\sigma}}, c_{\sigma}\right]$ for some absolute constants $C_{\mu}$ and $c_{\sigma}$, and let 
\[
f_{\theta} = f\left( \frac{ x - \mu}{\sigma} \right) - \log \sigma
\]

If $f(x)$ satisfies the conditions
\begin{itemize}
\item[(a)] $|f^{(k)}(x)|$ is bounded for some $k \geq 2$, and
\item[(b)] there exist absolute constants $c$ and $M$ such that $f'(x) > M$ for $x < -c$ and $f'(x) < -M$ for $x > c$,
\end{itemize}
then $\{f_{\theta} \}_{\theta \in \Theta}$ satisfy Assumptions B1--B4 when $\phi$ is chosen according to equation~\eqref{eqn:phi_defn2}. Details are provided in Appendix~\ref{sec:appendix_examples}. \\


For a sequence of $\{(\mu_{1,n}, \sigma_{1,n}), (\mu_{0,n}, \sigma_{0,n})\}_n \subset (\Theta \times \Theta)^\infty$, let $p_n, q_n$ be defined as:
\[
  p_n(x) =  \frac{1}{\sigma_{1,n}} \exp\left( f\left( \frac{x - \mu_{1,n}}{\sigma_{1,n}} \right) \right) \quad
  q_n(x) =  \frac{1}{\sigma_{0,n}} \exp\left( f\left( \frac{x - \mu_{0,n}}{\sigma_{0,n}} \right) \right)   
\]
where $f$ satisfy assumptions $(a)$ and $(b)$ from above. 

As a direct consequence of proposition~\ref{prop:theta_rate}, we have that, if $|\mu_{1,n} - \mu_{0,n}| + |\sigma_{1,n} - \sigma_{0,n}| = \Theta(1)$, then $(p_n, q_n)_n \in \mathcal{G}^{\textrm{upper}}_\Phi$, that is, $(p_n, q_n)_n$ satisfies assumptions A1'--A4' and if  $|\mu_{1,n} - \mu_{0,n}| + |\sigma_{1,n} - \sigma_{0,n}| = o(1)$, then $(p_n, q_n)_n \in \mathcal{G}^{\textrm{upper} \prime}_\Phi$, that is, $(p_n, q_n)_n$ satisfies assumption A1--A5.

  

The above assumptions on $f(x)$ are not strong, and are satisfied for \textbf{Gaussian location-scale families}, where the base density is the standard Gaussian density with
\[
f(x) = -x^2 - \frac{1}{2} \log 2 \pi,
\]
and \textbf{Laplace location-scale families}, where the base density is the standard Laplace density with
\[
f(x) = - |x| - \log 2.
\]
\end{example}

We emphasize that we make no assumption on any separation between $\mu_{1,n}, \mu_{0,n}$. Our results apply even when $\mu_{1,n} = \mu_{0,n}$ for all $n$ which implies that $p_n(x)$ and $q_n(x)$ have the same mean. 


\begin{example}
\label{ex:scale_on_R_plus}
  (Scale family over $\R^+$)\\
  
If the base density $\exp(f(x))$ is supported and positive on $\R^+$, we can define a scale family parametrized by $\sigma$ as 
$\exp( f(\frac{x}{\sigma}) )$. Let $\theta = (\sigma)$ and $\Theta = \left[\frac{1}{c_{\sigma}}, c_{\sigma} \right]$ for some absolute constant $c_{\sigma}$. 

And let 
\[
f_{\theta} = f\left( \frac{x}{\sigma} \right) - \log \sigma
\]


Again, if $f(x)$ satisfy conditions
\begin{itemize}
\item[(a)] $|f^{(k)}(x)|$ is bounded for some $k \geq 2$, and
\item[(b)] there exist absolute constants $c > 0$ and $M$ such that $f'(x) < -M$ for $x > c$,
\end{itemize}

then $\{f_{\theta} \}_{\theta \in \Theta}$ satisfy Assumptions B1--B4 when $\phi$ is chosen according to equation~\eqref{eqn:phi_defn1}. 

Again, for a sequence of $\{\sigma_{1,n}, \sigma_{0,n}\}_n \subset (\Theta \times \Theta)^\infty$, let $p_n, q_n$ be defined as:
 
$$
p_n(x) = \frac{1}{\sigma_{1,n}}  \exp\left( f\left( \frac{x}{\sigma_{1,n}}  \right) \right), \quad \textrm{and} \quad 
q_n(x) = \frac{1}{\sigma_{0,n}} \exp\left( f \left( \frac{x}{\sigma_{0,n}}  \right) \right).
$$ 


As a direct consequence of proposition~\ref{prop:theta_rate}, we have that, if $|\sigma_{1,n} - \sigma_{0,n}| = \Theta(1)$, then $(p_n, q_n)_n \in \mathcal{G}^{\textrm{upper}}_\Phi$, that is, $(p_n, q_n)_n$ satisfies assumptions A1'--A4' and if  $ |\sigma_{1,n} - \sigma_{0,n}| = o(1)$, then $(p_n, q_n)_n \in \mathcal{G}^{\textrm{upper} \prime}_\Phi$, that is, $(p_n, q_n)_n$ satisfies assumption A1--A5.

An example of a $f$ that satisfies condition (a) and (b) is 
\[
  f(x) = -x
\]
which forms the base density of the \textbf{exponential distributions}.

\end{example}

We end this section with the Gamma distributions on $\R^+$, which falls outside the above example but satisfy assumption B1-B4 nevertheless.

\begin{example} (Gamma distribution)\\

Let $\theta = (\alpha, \beta)$ and $\Theta = [\frac{1}{C}, C]^2$ for some absolute constant $C$, and let
$$
f_{\theta}(x) = (\alpha - 1) \log x - \beta x + \alpha \log \beta - \log \Gamma(\alpha).
$$
where $\Gamma(\cdot)$ is the Gamma function. If we choose $\phi$ as equation~\eqref{eqn:phi_defn1}, then $\{ f_\theta \}_{\theta \in \Theta}$ satisfies Assumptions B1--B4. [TODO:proof reference]



For a sequence of $\{(\alpha_{1,n}, \beta_{1,n}), (\alpha_{0,n}, \beta_{0,n})\}_n \subset (\Theta \times \Theta)^\infty$, let $p_n, q_n$ be defined as:
\[
p_n(x) = \frac{\beta_{1,n}^{\alpha_{1,n}}}{\Gamma(\alpha_{1,n})} x^{\alpha_{1,n} - 1} e^{- \beta_{1,n} x}, \quad \text{and} \quad
q_n(x) = \frac{\beta_{0,n}^{\alpha_{0,n}}}{\Gamma(\alpha_{0,n})} x^{\alpha_{0,n} - 1} e^{- \beta_{0,n} x},
\]
defined over $\R^+$. Then, as a direct consequence of proposition~\ref{prop:theta_rate}, we have that, if $|\alpha_{1,n} - \alpha_{0,n}| + |\beta_{1,n} - \beta_{0,n} |  = \Theta(1)$, then $(p_n, q_n)_n \in \mathcal{G}^{\textrm{upper}}_\Phi$, that is, $(p_n, q_n)_n$ satisfies assumptions A1'--A4' and if  $ |\alpha_{1,n} - \alpha_{0,n}| + |\beta_{1,n} - \beta_{0,n}| = o(1)$, then $(p_n, q_n)_n \in \mathcal{G}^{\textrm{upper} \prime}_\Phi$, that is, $(p_n, q_n)_n$ satisfies assumption A1--A5.

\end{example}

We note that our results apply even when $\frac{\alpha_{1,n}}{\beta_{1,n}} = \frac{\alpha_{0,n}}{\beta_{0,n}}$ for all $n$ which implies that $p_n(x)$ and $q_n(x)$ have the same mean.

%%%%%%

\subsection{Lower bound}
\label{sec:lower_bound}


As with the upper bound analysis, the condition required for the our lower bound depends on whether $H \equiv \int_S (\sqrt{p_n(x)} - \sqrt{q_n(x)})^2 dx$ is $o(1)$ or $\Theta(1)$. We capture these conditions by defininng sets of sequences $\mathcal{G}^{\textrm{lower}}_\Phi$ and $\mathcal{G}^{\textrm{lower}\prime}_\Phi$.

Define $\mathcal{G}^{\textrm{lower}} \subset (\mathcal{P} \times \mathcal{P})^\infty$ as the sequences of densities that satisfy $\int_S (\sqrt{p_n(x)} - \sqrt{q_n(x)})^2 dx = \Theta(1)$ and
\begin{align*}
  \sup_n \int p_n(x) \left| \log \frac{p_n(x)}{q_n(x)} \right|^2 dx &< \infty \\
  \sup_n \int q_n(x) \left| \log \frac{p_n(x)}{q_n(x)} \right|^2 dx &< \infty \\
\end{align*}

A comparison of these conditions with assumptions A1'-A4' shows that $\mathcal{G}^{\textrm{lower}} \supseteq \mathcal{G}^{\textrm{upper}}_\Phi$ for any $\Phi$. To see this, observe that, by assumption A1', $\left| \log \frac{p_n(x)}{q_n(x)} \right|^2$ must be integrable with respect to $\phi(x)$. 


When $H_n = o(1)$, we impose much stronger condition on $p_n, q_n$--we require the likelihood ratio to be bounded. Define $\mathcal{G}^{\textrm{lower} \prime} \subset (\mathcal{P} \times \mathcal{P})^\infty$ as the sequences of densities that satisfy $\int_S (\sqrt{p_n(x)} - \sqrt{q_n(x)})^2 dx = o(1)$ and
\[
  \sup_n \sup_x  \left| \log \frac{p_n(x)}{q_n(x)} \right| < \infty
\]
In contrast to the $H_n = \Theta(1)$ case, the bounded likelihood ratio condition that defines $\mathcal{G}^{\textrm{lower}\prime}$ is generally more restrictive than assumptions A1-A5. However, $\mathcal{G}^{\textrm{lower}\prime}$ still has significant overlap with $\mathcal{G}^{\textrm{upper} \prime}_\Phi$ for $\Phi$ defined as \ref{eqn:phi_defn1} or \ref{eqn:phi_defn2}.


\begin{theorem}
\label{thm:lower_bound}
Suppose we have $K$ clusters, of which at least one has size $\frac{n}{\beta K}$ and at least one has size $\frac{n}{\beta K}+1$, for some constant $\beta \geq 1$. Let $\sigma_0$ denote the true clustering. Suppose $I_n \rightarrow 0$ and $P_{0,n}$ and $Q_{0,n}$ satisfy Assumption A0, and let $(p_n(x), q_n(x))$ be a sequence of densities either in  $\mathcal{G}^{\textrm{lower}}$ or in  $\mathcal{G}^{\textrm{lower} \prime}$. Then any permutation-equivariant algorithm $\hat{\sigma}$ satisfies the following:
\begin{itemize}
\item[(i)] If $nI_n \rightarrow \infty$, then $\E l(\hat{\sigma}(A), \sigma_0) \ge \exp \left( - (1 + o(1)) \frac{ n I_n}{\beta K} \right)$.
\item[(ii)] If $nI_n \rightarrow c < \infty$, for some constant $c$, then $\E  l(\hat{\sigma}(A), \sigma_0) \geq c' > 0$, for some constant $c'$.
\end{itemize}
\end{theorem}

Theorem~\ref{thm:lower_bound} applies to any parameters $(p_n(x), q_n(x), P_0, Q_0, K, \beta, \sigma_0)$ that satisfy the assumptions, rather than being a minimax lower bound involving a supremum over a parameter space.


\begin{remark}
It is interesting to observe that Theorem~\ref{thm:lower_bound}, in conjunction with Theorem~\ref{thm:weighted_sbm_rate1}, shows that one does not have to pay a price for making nonparametric assumptions: Our nonparametric method achieves the optimal rate of recovery even if the densities $p_n(x)$ and $q_n(x)$ take on a parametric form. This seemingly counterintuitive phenomenon arises because the cost of discretization is reflected in the $o(1)$ term in the exponent and is thus of lower order. 
\end{remark}


The proof of theorem is given in section~\ref{sec:lower_bound_proof} of the appendix and is inspired by the change of measure technique employed by Yun and Proutiere [TODO:cite].

We give a brief sketch of the proof here. Without loss of generality, let cluster 1 and 2 be the clusters in $\sigma_0$ that have sizes $\frac{n}{\beta K}$ and $\frac{n}{ \beta K}+1$ respectively. We consider another cluster assignment $\sigma_0^2$ that differ from $\sigma_0$ on only one node -- a node in cluster 2 is re-assigned to cluster 1 for $\sigma_0^2$. 

Since $\sigma_0, \sigma_0^2$ are identical in all other aspects, including the sizes of all the clusters, we can leverage the symmetry of permutation equivariance

[TODO:summarize proof]




\section{Proof sketch: Recovery algorithm}
\label{SecProofs}

A large portion of the Appendix is devoted to proving that our recovery algorithm succeeds and achieves the optimal error rates. We provide an outline of the proofs here.

We divide our argument into propositions that focus on successive stages of our algorithm. A birds-eye view of our method reveals that it consists of two major components: (1) convert a weighted network into a labeled network, and then (2) run a community recovery algorithm on the labeled network. The first component involves two steps, transformation and discretization. 

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4, trim={0 6.5in 0 .8in}]{../figs/method_pipeline2.pdf}
\caption{\textcolor{red}{The add noise component also goes into the blue section. Analysis of the right-most blue region is in subsection~\ref{sec:labeled_sbm_analysis}, of the middle green region in subsection~\ref{sec:discretization_analysis}, and of the left-most red region in subsection~\ref{sec:transformation_analysis}}}
\label{fig:method_pipeline1}
\end{figure}

\subsection{Analysis of community recovery on a labeled network}
\label{sec:labeled_sbm_analysis}

The workhorse behind our algorithm is a subroutine (right-most blue region in Figure~\ref{fig:method_pipeline1}) for recovering communities on a network where the edges have discrete labels $l=1, \dots, L_n$. The following proposition characterizes the rate of convergence of the output of the subroutine, where within-community edges are assigned edge labels with probabilities $\{P_{l,n}\}$, and between-community edges are assigned edge labels according to $\{Q_{l,n}\}$. 

\begin{proposition}
\label{prop:labeled_sbm_rate}
Suppose the edge label probabilities satisfy $\frac{1}{\rho_n} \leq \frac{P_{l,n}}{Q_{l,n}} \leq \rho_n$ for a sequence $\rho_n = \Omega(1)$. Define $I_{L_n} = -2 \log \sum_{l=0}^{L_n} \sqrt{P_{l,n} Q_{l,n}}$ and suppose $I_{L_n} \rightarrow 0$. Suppose ${L_n} = \Omega(1)$ and $\frac{n I_{L_n}}{{L_n} \rho^4_n} \rightarrow \infty$. Let $\hat{\sigma}$ be the algorithm [TODO:what algorithm]. Then
\[
\lim_{n \rightarrow \infty} P \left( l(\hat{\sigma}(A), \sigma_0) \leq \exp \left( - \frac{ n I_{L_n}}{ \beta K} (1 + o(1)) \right) \right) \rightarrow 1.
\]
\end{proposition}

Yun and Proutiere~\cite{yun2016optimal} proposed an algorithm for the labeled SBM that achieves the same rate of convergence. Proposition~\ref{prop:labeled_sbm_rate} is more general than their result, however, in that, [TODO:discuss how we relax their assumption], we allow both the number of labels ${L_n}$ and the bound $\rho_n$ on the ratio $\frac{P_{l,n}}{Q_{l,n}}$ to diverge to infinity. This extension is critical in analyzing the weighted SBM, since to achieve consistency for continuous distributions, the discretization level $L_n$ must increase with $n$. 

\subsection{Discretization of the Renyi divergence}
\label{sec:discretization_analysis}

The rate of convergence in Proposition~\ref{prop:labeled_sbm_rate} resembles the expressions in Theorems~\ref{thm:weighted_sbm_rate1} and~\ref{thm:weighted_sbm_rate2}, except the Renyi divergence $I$ is replaced by the discretized Renyi divergence $I_L$. Thus, we may derive Theorems~\ref{thm:weighted_sbm_rate1} and \ref{thm:weighted_sbm_rate2} from Proposition~\ref{prop:labeled_sbm_rate} by showing that $|I_n-I_{L_n}|$ is sufficiently small. It is easy to show that $I_{L_n} \leq I_n$, since discretization always leads to a loss of information. If $p_n(x)$ ad $q_n(x)$ are sufficiently regular in that they may be well-approximated via discretization, one might expect that $I_{L_n}$ to be not much smaller than $I_n$. Proposition~\ref{prop:discretization1} and \ref{prop:discretization2} formalizes this notion.
\medskip


For both of the propositions below, we let $\bin_l = [a_l, b_l]$, for $l=1, \dots, L_n$ be a uniformly-spaced binning of $[0,1]$ and let $P_{l,n} = (1- P_{0,n}) \int_{a_l}^{b_l} p_n(z) dz$ and $Q_{l,n} = (1-Q_{0,n})\int_{a_l}^{b_l} q_n(z) dz$.


\noindent The following proposition is useful for proving Theorem \ref{thm:weighted_sbm_rate2}:

\begin{proposition}
\label{prop:discretization2}
Let $p_n(z), q_n(z)$ be two densities supported on $[0,1]$. Suppose $H_n = \Theta(1)$. Also suppose
\begin{enumerate}
\item[C1]  $p_n(z), q_n(z) > 0$ on $(0, 1)$, and $\sup_z \{p_n(z) \vee q_n(z)\} < \infty$. 
\item[C2] $\int \left| \log \frac{p_n(z)}{q_n(z)} \right| dz < \infty$.
\item[C3]  There exists $h_n(z)$ such that
\begin{itemize}
\item[(a)] $h_n(z) \geq \max \left\{  \left|\frac{p'_n(z)}{p_n(z)} \right|, \left|\frac{q'_n(z)}{q_n(z)}\right|  \right\}$,
\item[(b)] $h_n(z)$ is $(c_{s1}', c_{s2}', C_s')$-bowl-shaped, and
\item[(c)] $\int |h_n(z)|^t dz < \infty$ for some constant $t$ such that  $\frac{2}{r} \le t \le 1$.
\end{itemize}
\item[C4]  $p'_n(z), q'_n(z) \geq 0$ for all $z < c'_{s1}$, and $p'_n(z), q'_n(z) \leq 0$ for all $z > c'_{s2}$.
\end{enumerate}
Suppose $L_n \rightarrow \infty$ and $P_{0,n}, Q_{0,n}$ satisfy assumption A0.
Then
$$ \left| \frac{I_n - I_{L_n}}{I_n} \right| = o(1)$$ 
and $\frac{1}{4c_0} \exp(-L_n^{1/r}) \leq \frac{P_{l,n}}{Q_{l,n}} \leq 4 c_0 \exp(L_n^{1/r})$, for all $l$, where $c_0$ is the constant in assumption A0.
\end{proposition}


\medskip

\noindent The following proposition is useful for proving Theorem~\ref{thm:weighted_sbm_rate1}:
\begin{proposition} 
\label{prop:discretization1}
Let $p_n(z)$ and $q_n(z)$ be two densities supported on $[0,1]$. Suppose $H_n = o(1)$. Let $L_n$ be a sequence such that $L_n \rightarrow \infty$. Suppose the following assumptions are satisfied:
\begin{enumerate}
\item[C1'] $p_n(z), q_n(z) > 0$ on $(0, 1)$, and $\sup_z \{p_n(z) \vee q_n(z)\} < \infty$. 
\item[C2'] There exists a subinterval $R \subseteq [0,1]$ such that
\begin{itemize}
\item[(a)] $\frac{1}{\rho} \leq \left| \frac{p_n(z)}{q_n(z)} \right| \leq \rho$ for all $z \in R$, where $\rho$ is an absolute constant, and
\item[(b)] $\mu\{R_n^c\} = o(H_n)$, where $\mu$ is the Lebesgue measure and $R_n^c \equiv [0,1] \ R_n$.
\end{itemize}
\item[C3'] Let $\alpha_n^2 = \int_{R_n} \frac{(p_n(z) - q_n(z))^2}{q_n(z)} dz$ and $\gamma_n(z) = \frac{q_n(z) - p_n(z)}{\alpha}$, and suppose $\int_{R_n} q_n(z) \left| \frac{\gamma_n(z)}{q_n(z)} \right|^r dz  < \infty$ for an absolute constant $r \geq 4$.
\item[C4'] There exists $h_n(z)$ such that
\begin{itemize}
\item[(a)] $h_n(z) \geq \max \left\{  \left|\frac{\gamma'_n(z)}{q_n(z)} \right|, \left|\frac{q'_n(z)}{q_n(z)}\right|  \right\}$, and
\item[(b)] $h_n(z)$ is $(c_{s1}', c_{s2}', C_s')$-bowl-shaped for absolute constants $c_{s1}', c_{s2}'$, and $C_s'$, and
\item[(c)] $\int_R |h_n(z)|^t dz < \infty$ for an absolute constant $\frac{2}{r} < t < 1$.
\end{itemize}
\item[C5'] $p'_n(z), q'_n(z) \geq 0$ for all $z < c'_{s1}$, and $p'_n(z), q'_n(z) \leq 0$ for all $z > c'_{s2}$.
\end{enumerate}
Suppose $L_n \rightarrow \infty$ and $P_{0,n}, Q_{0,n}$ satisfy assumption A0.
Then
 $$\left| \frac{I_n - I_{L_n}}{I_n} \right| = o(1)$$ 
and $\frac{1}{4\rho c_0} \leq \frac{P_{l,n}}{Q_{l,n}} \leq 4\rho c_0$, for all $l$, where $c_0$ is the constant in A0.
\end{proposition}



\subsection{Analysis of the transformation function}
\label{sec:transformation_analysis}

Propositions~\ref{prop:discretization1} and~\ref{prop:discretization2} consider densities supported on $[0,1]$. This is enough for us because once we transform the densities by an application of $\Phi$, the new densities are compactly supported and, importantly, the Renyi divergence $I$ and the Hellinger divergence $H$ are invariant with respect to the transformation $\Phi$.

To see this, let $p_n(x)$ and $q_n(x)$ denote densities over $S$, and let $p_{\Phi,n}(z)$ and $q_{\Phi,n}(z)$ denote the transformed densities over $[0,1]$. It is easy to see that $p_{\Phi,n}(z) = \frac{p_n(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$ and $q_{\Phi,n}(z) = \frac{q_n(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$. Therefore, via the change of variables $z = \Phi^{-1}(x)$, we have the following relations:
\begin{align*}
\int_S \sqrt{p_n(x)q_n(x)} dx &= \int_0^1 \sqrt{p_{\Phi,n}(z) q_{\Phi,n}(z)} dz, \\
\int_S \left(\sqrt{p_n(x)} - \sqrt{q_n(x)}\right)^2 dx &= \int_0^1 \left(\sqrt{p_{\Phi}(z)} -\sqrt{q_{\Phi}(z)}\right)^2 dz.
\end{align*}
Therefore, the divergences $I$ and $H$ between $p_n(x)$ and $q_n(x)$ are equal to the divergences between $p_{\Phi,n}(z)$ and $q_{\Phi,n}(z)$.

To prove Theorems~\ref{thm:weighted_sbm_rate1} and~\ref{thm:weighted_sbm_rate2}, it thus remains to show that if the densities $p(x)$ and $q_n(x)$ satisfy Assumptions A1--A4 (or A1'--A5'), the transformed densities $p_{\Phi,n}(z)$ and $q_{\Phi,n}(z)$ satisfy Assumptions C1--C4 (or C1'--C5') in Proposition~\ref{prop:discretization1} (or Proposition~\ref{prop:discretization2}). This is done in Propositions~\ref{prop:transformation1} and~\ref{prop:transformation2}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{sec:conclusion}

We have provided a rate-optimal community estimation algorithm for the homogeneous weighted stochastic block model. In the setting where the average degree is of order $\log n$ and the edge weight densities $p(x)$ and $q(x)$ are fixed, we have also characterized the exact recovery threshold. Our algorithm includes a preprocessing step consisting of transforming and discretizing the (possibly) continuous edge weights to obtain a simpler graph with edge weights supported on a finite discrete set. This approach may be useful for other network data analysis problems involving continuous distributions, where discrete versions of the problem are simpler to analyze.

Our paper is a first step toward understanding the weighted SBM under the same mathematical framework that has been so fruitful for the unweighted SBM. It is far from comprehensive, however, and many open questions remain. We describe a few here:
\begin{enumerate}
\item An important problem is to extend our analysis to the case of a \emph{heterogenous} stochastic block model, where edge weight distributions depend on the exact community assignments of both endpoints. In such a setting, Abbe and Sandon~\cite{AbbSan15} and Yun and Proutiere~\cite{yun2016optimal} have shown that a generalized information divergence---the CH divergence---governs the intrinsic difficulty of community recovery. We believe that a similar discretization-based approach should lead to analogous results in the case of a heterogeneous weighted SBM. The key challenge would be to show that discretization does not lose much information with respect to the CH-divergence.
\item Real-world networks often have nodes with very high degrees, which may adversely affect the accuracy of recovery methods for the stochastic block model. To solve this problem, degree-corrected SBMs \cite{zhao2012consistency, gao2016community} have been proposed as an effective alternative to regular SBMs. It is straightforward to extend the concept of degree-correction to the weighted SBM, but it is unclear whether our discretization-based approach would be effective in obtaining optimal error rates.
\item It is easy to extend our results to the weighted \emph{and} labeled SBMs if the number of labels is finite or assumed to be slowly growing. However, this excludes some interesting cases, including the setting where edge labels represent counts  from a Poisson distribution. We suspect that in such a situation, it may be possible to combine low-probability labels in a clever way to obtain a discretization that is again amenable to our approach. 
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\bibliographystyle{plain}
\bibliography{refs}


\appendix

\section{Proof of Proposition~\ref{prop:labeled_sbm_rate}}
\label{appendix: first}

We structure the proof according to the flow of our algorithm. Since this proposition addresses the case of discrete labels, we do not need to consider the ``transformation and discretization" step.

We begin by analyzing Algorithm~\ref{alg:noisify}, where we deliberately add noise to the graph by changing edge colors at random. Although adding random noise destroys information and increases the difficulty of community recovery, Lemma~\ref{lemma: first} in Appendix~\ref{appendix: lemmas for labeled_sbm_rate} shows that the process does not significantly affect the Renyi divergence $I_L$. Furthermore, the new probabilities of edge labels are at least $\frac{c}{n}$, which is important for our later analysis. To simplify notation, we continue to refer the new edge label probabilities as $P_l$ and $Q_l$ throughout the proof.

Next, our algorithm performs spectral clustering using only the edges with label $l$, and calculates $\hat I_l := \frac{(\hat P_l - \hat Q_l)^2}{\hat P_l \vee \hat Q_l}$, where $\hat P_l$ and $\hat Q_l$ are the estimated probabilities obtained by using the output of spectral clustering:
\begin{align*}
\hat P_l &= \frac{ \sum_{u \neq v \,:\, \tilde{\sigma}_l(u) = \tilde{\sigma}_l(v) } (A_l)_{uv} }{ |{u \neq v \,:\, \tilde{\sigma}_l(u) = \tilde{\sigma}_l(v) }| }, \quad \text{and} \quad \hat{Q}_l =  \frac{ \sum_{u \neq v \,:\, \tilde{\sigma}_l(u) \neq \tilde{\sigma}_l(v) } (A_l)_{uv} }{ |{u \neq v \,:\, \tilde{\sigma}_l(u) \neq \tilde{\sigma}_l(v) }| }.
\end{align*}
We then select $l^* \in \arg\max_{1 \le l \le L} \hat I_L$. Note that if $|\hat P_l - P_l|$ and $|\hat Q_l - Q_l|$ are small, $\hat I_l$ provides a measure of how ``good" a color is for clustering: larger values of $\hat I_l$ correspond to greater separation between $P_l$ and $Q_l$. Naturally, the accuracy of the estimated edge probabilities $\hat P_l$ and $\hat Q_l$ depends on the accuracy of the spectral clustering step. Proposition~\ref{prop:estimation_consistency} below makes this statement rigorous. Before stating the proposition, we define the set of ``good" labels as follows:
$$L_1 = \left\{ l : \frac{n(P_l-Q_l)^2}{P_l \vee Q_l} := \frac{\Delta_l^2}{P_l \vee Q_l} \geq 1 \right\}.$$
We bound the difference between the estimated and true probabilities for good and bad colors in Proposition~\ref{prop:estimation_consistency}, the formal statement and proof of which are contained in Appendix~\ref{appendix: hats galore}.

\begin{repproposition}{prop:estimation_consistency}
Suppose $\sigma$ is a clustering with error rate at most $\gamma$; i.e., $l(\sigma, \sigma_0) \leq \gamma$, for sufficiently small $\gamma \ge \frac{1}{n}$. With probability at least $1 - L n^{-(3 + \delta_p)}$, for a small $\delta_p > 0$, the following hold:
\begin{enumerate}
\item For $l \in L_1$, we have $| \hat{P}_l - P_l | \leq \eta \Delta_l$ and $| \hat{Q}_l - Q_l | \leq \eta \Delta_l.$
\item For $l \in L_1^c$, we have $| \hat{P}_l - P_l | \leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}$ and $| \hat{Q}_l - Q_l | \leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}$.
\end{enumerate}
In both cases,  $\eta = C \sqrt{\gamma \log \frac{1}{\gamma}}$, for an absolute constant $C$.
\end{repproposition}

We now work toward obtaining a suitable initial clustering with small error rate $\gamma$. In Proposition~\ref{prop:spectral_analysis}, we show that if the edge probabilities for a particular label are well-separated, the spectral clustering output of Algorithm~\ref{alg:spectral} is reasonably accurate. We provide a rough statement of the proposition here, and refer to Appendix~\ref{appendix: spectral} for the precise statement and proof.
\begin{repproposition}{prop:spectral_analysis}
If $P_l$ and $Q_l$ satisfy $C_1 \frac{(P_l \vee Q_l)}{n (P_l-Q_l)^2} \leq 1$ for an absolute constant $C_1$,  the output $\sigma^l$ of Algorithm~\ref{alg:spectral} satisfies the inequality
\[
l(\sigma^l, \sigma_0) \leq C_2 \frac{(P_l \vee Q_l)}{n (P_l-Q_l)^2},
\]
for a constant $C_2$, with probability at least $1 - n^{-4}$.
\end{repproposition}
Thus, if we want to cluster to an arbitrary degree of accuracy, we need $ \frac{(P_l \vee Q_l)}{n (P_l-Q_l)^2} \to 0$ for at least one well-separated label $l$. We show that the label $l^*$ selected in Algorithm~\ref{alg:initialization1} satisfies $ \frac{(P_{l^*} \vee Q_{l^*})}{n (P_{l^*}-Q_{l^*})^2} \to 0$ in the following proposition. A more detailed restatement and proof is contained in Appendix~\ref{appendix: starry night}.
\begin{repproposition}{prop:initialization_correctness}
With probability at least $1 - 2L n^{-(3+\delta_p)}$, we have $\frac{n (P_{l^*}-Q_{l^*})^2}{\rho_L^4(P_{l^*} \vee Q_{l^*})} \rightarrow \infty$. 
\end{repproposition}

Now let $E_1$ denote the high-probability event that the label $l^*$ is chosen according to Proposition~\ref{prop:initialization_correctness}. We perform spectral clustering $n$ times, omitting one vertex and clustering on the remaining graph each time, and denote the resulting community assignments by $\{\tilde \sigma_u\}_{1 \le u \le n}$. Note that Proposition~\ref{prop:spectral_analysis}, together with a simple union bound, implies that with probability at least $1-n^{-3}$, all clusterings have error rate bounded by $\gamma :=  C\frac{ P_{l^*} \vee Q_{l^*}}{ n ( P_{l^*} - Q_{l^*} )^2}$, for some constant $C$. Denote this event by $E_2$. On $E_1 \cap E_2$, we then have $\gamma \rho_L^4 \rightarrow 0$. Thus, we may apply Proposition~\ref{prop:estimation_consistency} on each clustering $\tilde{\sigma}_u$ to show that the conclusion holds simultaneously for all $\tilde{\sigma}_u$'s, with probability at least $1 - Ln^{-(2 + \delta_p)}$. We denote this last event by $E_3$. Furthermore, $\eta = \Theta \left( \sqrt{\gamma \log \frac{1}{\gamma} } \right)$, so $|\eta \rho_L| \rightarrow 0$.

We now construct the clustering $\hat \sigma_u$ by assigning vertex $u$ to an appropriate community in $\tilde \sigma_u$, using the relation from Algorithm~\ref{alg:refinement}.
% \[
%    \hat{\sigma}_u(u) = \argmax_k \sum_{v \,:\, \tilde{\sigma}_u(v) = k ,\, v\neq u} 
%         \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l). 
%\]    
In Proposition~\ref{prop:single_node_error_bound}, we show that with high probability, the assignment $\hat \sigma_u(u)$ is ``correct." We provide a rough statement of  the proposition here, and defer the exact statement and proof to Appendix~\ref{appendix: single node}:
%We may check that all that all the necessary preconditions hold for applying Proposition~\ref{prop:single_node_error_bound}.
%in particular $|\eta \rho_L| \to 0$ are satisfied when event $E_3$ occurs.
\begin{repproposition}{prop:single_node_error_bound}
Let $\pi_u \in S_K$ be such that $l(\sigma_0, \tilde \sigma_u) = d(\sigma_0, \pi_u(\tilde \sigma_u)).$ Conditioned on $E_1 \cap E_2 \cap E_3$, with probability at least $1 - (K-1)\exp \left( - (1 - o(1)) \frac{n}{\beta K} I_L \right)$, we have $\pi_u^{-1}(\sigma_0(u)) = \hat \sigma_u(u).$
\end{repproposition}
We briefly discuss the uniqueness of $\pi_u$. By construction, the error rate of $\hat{\sigma}_u$ is at most $ \gamma + \frac{1}{n}$, so $l(\sigma_0, \hat{\sigma}_u) < \frac{1}{8 \beta K}$ for sufficiently small $\gamma$.
%Let $\pi_u \in S_K$ denote a permutation such that $d(\sigma_0, \pi_u(\hat{\sigma}_u)) < \frac{n}{8\beta K}$. We argue that $\pi_u$ is the unique permutation that satisfies $l(\sigma_0, \hat \sigma_u) = d(\sigma_0, \pi_u(\hat{\sigma}_u))$. We show this by first observing that
Now note that for any $u$, the minimum cluster size of the clustering $\hat{\sigma}_u$ is at least $\frac{n}{\beta K} - (n \gamma + 1) \geq \frac{ n(1 - \beta K \gamma  - \beta K/n)}{\beta K} \geq \frac{n}{2\beta K}$, for small $\gamma$. A simple argument (cf.\ Lemma~\ref{lem:consensus_uniqueness}) shows that $\pi_u$ is the unique permutation to obtain such a small error rate.
%if a permutation $\pi \in S_K$ and assignment $\sigma'$ exist such that $d(\sigma_0, \pi(\sigma'))$ is at most half the size of the smallest cluster, then $\pi$ is the unique permutation that satisfies $l(\sigma_0, \sigma') = d(\sigma_0, \pi(\sigma'))$.

%Thus, $\sigma_0, \hat \sigma_1$, and $\hat \sigma_u$ are all close to each each other, and it is plausible that
Define $\pi_1$ and $\pi_u$ to be the permutations minimizing $d(\sigma_0, \pi_1(\hat \sigma_1))$ and $d(\sigma_0, \pi_u(\hat \sigma_u))$, respectively, and let $\xi_u$ denote the permutation minimizing $d(\hat \sigma_1, \xi_u(\hat \sigma_u))$.
%by the relation $\xi_u = \pi_1^{-1} \circ \pi_u$. We prove this rigorously, as well.
We know that $d(\sigma_0, \pi_1(\hat{\sigma}_1)) < \frac{1}{8 \beta K}$ and $d(\sigma_0, \pi_u(\hat{\sigma}_u)) < \frac{1}{8 \beta K}$. Thus, the triangle inequality implies
\begin{equation*}
d(\hat{\sigma}_1, \pi_1^{-1}( \pi_u(\hat{\sigma}_u))) = d(\pi_1(\hat{\sigma}_1), \pi_u(\hat{\sigma}_u)) \le d(\sigma_0, \pi_1(\hat{\sigma}_1)) + d(\sigma_0, \pi_u(\hat{\sigma}_u)) < \frac{1}{4 \beta K}.
\end{equation*}
Since the minimum cluster size of both $\hat{\sigma}_1$ and $\hat{\sigma}_u$ is $\frac{n}{2 \beta K}$, Lemma~\ref{lem:consensus_uniqueness} implies that $\xi_u = \pi_1^{-1} \circ \pi_u$; and by Lemma~\ref{lem:consensus_analysis}, we also have $\hat \sigma(u) = \xi_u(\hat \sigma_u(u))$.

Restating Proposition~\ref{prop:single_node_error_bound}, we have
\[
 P\bigg(\hat{\sigma}_u(u) \neq \pi^{-1}_u(\sigma_0(u)) \bigg | E_1 \cap E_2 \cap E_3\bigg) \leq \exp\left( - (1+o(1)) \frac{n I_L}{\beta K} \right).
\]
Furthermore, the left-hand expression is equivalent to $\xi_u^{-1}(\hat{\sigma}(u)) \neq \pi_u^{-1}(\sigma_0(u))$, or $\hat{\sigma}(u) \neq \xi_u \circ \pi_u^{-1}(\sigma_0(u)) = \pi_1^{-1}(\sigma_0(u))$.
%Note that $\hat \sigma_1$ and $\hat \sigma_u$ both have an error rate of at most $\gamma + \frac{1}{n}$, so by the triangle inequality, we have $l(\hat{\sigma}_u, \hat{\sigma}_1) \leq 2\gamma + \frac{2}{n} < \frac{1}{4\beta K}$.

%The final stage of our algorithm is the consensus stage, where we combine the $\hat \sigma_u$'s to produce a clustering $\hat \sigma$ according to the rule
%\[
%\hat{\sigma}(u) = \argmax_k \big| \{ v \,:\,  \hat{\sigma}_1(v) = k \} \cap
%                                 \{ v \,:\, \hat{\sigma}_u(v) = \hat{\sigma}_u(u) \} \big|.
%\]
%This means that if the cluster containing $u$ in $\hat \sigma_u$ has the largest overlap with some cluster $k$ in $\hat \sigma_1$, then $u$ is assigned to cluster $k$; i.e., we assign $\hat \sigma (u) = k$. Lemma~\ref{lem:consensus_analysis} applied to the pair $(\hat{\sigma}_1, \hat{\sigma}_u)$ then shows that $\hat \sigma(u) = \xi_u(\hat \sigma_u(u))$.
%Intuitively, one may guess that the permutation $\xi_u \in S_K$ minimizing $d(\hat \sigma_1, \xi_u(\hat \sigma_u))$ is the consensus function; i.e. Indeed,  
%Thus, we have
%\begin{align*}
%P( \hat{\sigma}(u) \neq \pi_1^{-1} ( \sigma_0(u)) \given E_3) &= P( \hat{\sigma}(u) \neq \xi_u \circ \pi_u^{-1} (\sigma_0(u)) \given E_3)\\
% &=  P( \xi_u^{-1}(\hat{\sigma}(u)) \neq \pi_u^{-1} (\sigma_0(u)) \given E_3)\\
% &= P( \hat{\sigma}_u(u) \neq \pi_u^{-1} (\sigma_0(u)) \given E_3). 
%\end{align*}
Altogether, we conclude that
\begin{align*}
P( \hat{\sigma}(u) \neq \pi_1^{-1} ( \sigma_0(u)) ) &\leq  \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) + P(E_1^c) + P(E_2^c) + P(E_3^c) \\
    &\leq \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) + 2Ln^{-(3 + \delta_p)} + n^{-3} + Ln^{-(2+\delta_p)} \\
%    &\leq  \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) + n^{-(1 + \delta_p)} + n^{-3} + n^{-2} \\
    &\leq  \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) + n^{-(2 + \delta_p)},
\end{align*}
where $\eta' = o(1)$.

Finally, suppose $$\exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) \geq n^{-(1+\delta_p)}.$$
Defining $\eta'' = \eta' + \beta \sqrt{ \frac{K}{ n I_L} } = o(1)$, we have
\begin{align*}
P\left\{ l(\hat{\sigma}, \sigma_0) >   \exp\left( -(1 - \eta'') \frac{n I_L}{\beta K} \right) \right\} & \leq \frac{\E l(\hat \sigma, \sigma_0)}{ \exp\left( -(1 - \eta'') \frac{n I_L}{\beta K} \right)}\\
 & \le \frac{1}{ \exp\left( -(1 - \eta'') \frac{n I_L}{\beta K} \right) } \frac{1}{n} \sum_{u=1}^n P(\hat{\sigma}(u) \neq \pi_1^{-1}(\sigma_0(u))) \\
 &\leq \exp\left\{ -(\eta'' - \eta') \frac{n I_L}{\beta K} \right\} + \frac{ C n^{-(2 + \delta_p)} }{ \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right)  } = o(1).
\end{align*}
On the other hand, if
$$ \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right) \leq n^{-(1 + \delta_p)},$$
we have
\begin{align*}
P \left\{ l(\hat{\sigma}, \sigma_0) >  \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right)  \right\} & \leq P( l(\hat{\sigma}, \sigma_0) > 0 ) \\
%
& \le P(d(\hat{\sigma}, \pi^{-1}(\sigma_0)) > 0) \\
& \leq \sum_{u=1}^n P( \hat{\sigma}(u) \neq \pi_1^{-1}(\sigma_0(u))) \\
&\leq n \exp\left( -(1 - \eta') \frac{n I_L}{\beta K} \right)  + n^{-(1+\delta_p)} = o(1).
\end{align*}
This completes the proof of Proposition~\ref{prop:labeled_sbm_rate}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Supporting results for Proposition~\ref{prop:labeled_sbm_rate}}
\label{appendix: labeled_sbm_rate}

We now provide proofs for the supporting results stated in Appendix~\ref{appendix: first}.

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis of estimation error of $\hat{P}_l$ and $\hat{Q}_l$}
\label{appendix: hats galore}

\begin{proposition}
\label{prop:estimation_consistency}
Let $A$ be the adjacency matrix of a labeled network with true clustering assignment $\sigma_0$. Suppose $\sigma$ is a random initial clustering satisfying $l(\sigma, \sigma_0) \leq \gamma $. Let $\hat{P}_l = \frac{\sum_{u \neq v \,:\, \sigma(u)=\sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {|\{u \neq v: \; \sigma(u) = \sigma(v)\}|}$ and
    $\hat{Q}_l = \frac{\sum_{u \neq v \,:\, \sigma(u) \neq \sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {|\{u \neq v: \; \sigma(u) = \sigma(v)\}|}$ be the MLE of $P_l$ and $Q_l$ based on $\sigma$. Let $\delta_p$ be a positive, fixed, and arbitrarily small real number, and let $c > 0$ be an absolute constant. Then with probability at least $1 - L n^{-(3 + \delta_p)}$, the following hold for all sufficiently small $\gamma$:
\begin{enumerate}
\item For all $l$ such that $P_l \vee Q_l \geq \frac{c}{n}$, if $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq 1$, then
\begin{align*}
 | \hat{P}_l - P_l | &\leq \eta \Delta_l, \quad \text{and} \quad | \hat{Q}_l - Q_l | \leq \eta \Delta_l. 
\end{align*}
\item For all $l$ such that $P_l \vee Q_l \geq \frac{c}{n}$, if $\frac{n \Delta_l^2}{P_l \vee Q_l} \leq 1$, then
\begin{align*}
 | \hat{P}_l - P_l | &\leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}, \quad \text{and} \quad | \hat{Q}_l - Q_l | \leq \eta \sqrt{ \frac{P_l \vee Q_l}{n}}.
\end{align*}
\end{enumerate}
In both cases,  $\eta = C \sqrt{\gamma \log \frac{1}{\gamma}}$, for an absolute constant $C$.
\end{proposition}

\begin{proof}
Our proof proceeds by showing that for any fixed assignment $\sigma$ with error rate bounded by $\gamma$, the event described in Proposition~\ref{prop:estimation_consistency} holds with high probability. For a fixed assignment $\sigma$, we call a random graph a ``bad graph for $\sigma$" if the event does not hold. For each $\sigma$, we upper-bound the probability that a randomly chosen graph lies in the set of bad graphs for $\sigma$; we then use a union bound over all choices of $\sigma$ and all $L$ colors to show that the probability of choosing a bad graph is bounded by $Ln^{-(3+\delta_p)}$.

We begin by bounding the bias of $\hat{P}_l$. We have
\begin{align}
\E(\hat{P_l}) &= 
   \frac{\sum_{u \neq v \,:\, \sigma(u) = \sigma(v)} \left\{
             \mathbf{1}(\sigma_0(u) = \sigma_0(v) ) P_l + 
               \mathbf{1}(\sigma_0(u) \neq \sigma_0(v)) Q_l\right\}}{|\{u \neq v: \; \sigma(u) = \sigma(v)\}|} \nonumber \\
  &= (1 - \lambda) P_l + \lambda Q_l  = P_l + \lambda (Q_l - P_l),
  \label{eqn:bias_simple_bound}
\end{align}
where $\lambda := \frac{\sum_{u \neq v \,:\, \sigma(u) = \sigma(v)} 
     \mathbf{1}(\sigma_0(u) \neq \sigma_0(v)) }{|\{u \neq v: \; \sigma(u) = \sigma(v)\}|}$. Thus, $|\E(\hat{P}_l) - P_l | \leq \lambda |Q_l - P_l|$. Furthermore, if $\hat{n}_k$ denotes the number of vertices in cluster $k$ according to $\sigma$, we have
\begin{align*}
\lambda 
  &= \frac{\sum_{u \neq v \,:\, \sigma(u) = \sigma(v) }\mathbf{1}(\sigma_0(u) \neq \sigma_0(v)) }{|\{u \neq v: \; \sigma(u) = \sigma(v)\}|} = 
   \frac{\sum_k \sum_{u \neq v \,:\, \sigma(u)=\sigma(v)=k} \mathbf{1}(\sigma_0(u) \neq \sigma_0(v))}{\sum_k \hat{n}_k (\hat{n}_k-1)} 
      \\
  &\leq \frac{\sum_k \sum_{u \neq v \,:\, \sigma(u)=\sigma(v)=k} 
       \mathbf{1}( \neg (\sigma_0(u) = \sigma_0(v) =k) )}{\sum_k \hat{n}_k (\hat{n}_k-1)} 
      \\ 
  &\leq \frac{ \sum_k \sum_{u \neq v \,:\, \sigma(u)=\sigma(v)=k} \left\{\mathbf{1}(\sigma_0(v)) \neq k) + \mathbf{1}(\sigma_0(u) \neq k)\right\}}
            {\sum_k \hat{n}_k (\hat{n}_k - 1)}.
\end{align*}
Define $\gamma_k = \frac{1}{n} \sum_{u \,:\, \sigma(u)=k} \mathbf{1}(\sigma_0(u) \neq k)$ to be the error rate within the estimated cluster $k$. Then $\sum_k \gamma_k \le \gamma$ and $\sum_{u \,:\, \sigma(u) = k} \sum_{v \,:\, \sigma(v) = k} \mathbf{1}(\sigma_0(v) \neq k) = \gamma_k n \hat{n}_k$, implying that
\begin{align*}
\lambda \leq \frac{ \sum_k 2 \gamma_k n \hat{n}_k }{\sum_k \hat{n}_k(\hat{n}_k - 1)} = \frac{n}{\sum_k \hat{n}_k (\hat{n}_k - 1) } \sum_k 2 \gamma_k \hat{n}_k \stackrel{(a)}{\leq} \frac{K}{n-K} \sum_k 2 \gamma_k \hat{n}_k \stackrel{(b)}{\leq} 4 \gamma K,
\end{align*}
where $(a)$ uses the fact that
\begin{equation}
\label{EqnSmall}
\sum_k \frac{\hat{n}_k }{n} (\hat{n}_k - 1) = n \sum_k \left( \frac{\hat{n}_k}{n} \right)^2 - 1 \geq \frac{n}{K} - 1,
\end{equation}
and $(b)$ uses the assumption $K < \frac{n}{2}$. 
Altogether, we conclude that $|\E (\hat{P}_l) - P_l | \leq 4 \gamma K \Delta_l$. A similar calculation may be performed for $\hat Q_l$, so
\begin{equation*}
\max\left\{|\E (\hat{P}_l) - P_l |, \; |\E(\hat{Q}_l) - Q_l |\right\} \leq C_2 \gamma \Delta_l,
\end{equation*}
for a constant $C_2 > 0$. To simplify presentation, we define $\eta_1 = C_2 \gamma$, so
\begin{equation} \label{eq: A1.5}
\max\left\{|\E(\hat{P}_l) - P_l |, \; |\E(\hat{Q}_l) - Q_l|\right\} \leq \eta_1 \Delta_l.
\end{equation}
%From this it is clear that $\eta_1$ becomes arbitrarily small if $\gamma$ is made arbitrarily small. 

We now turn to bounding $|\hat{P}_l - P_l|$ and $|\hat{Q}_l - Q_l|$. Denoting $ \tilde{A}_{uv} = \mathbf{1}(A_{ij} = l)$ and using Bernstein's inequality, we have
\[
P\left( \left| \sum_{u,v\,:\, \sigma(u) = \sigma(v)} (\tilde{A}_{uv} - \E \tilde{A}_{uv} ) \right|  > t 
 \right) \leq 2 \exp\left( 
    - \frac{t^2}{ 2 \sum_{u,v \, \sigma(u) = \sigma(v)} \E \tilde{A}_{uv}  + \frac{2}{3}t }
\right).
\]
By equation~\eqref{eqn:bias_simple_bound}, we have
\begin{align*}
\sum_{u,v \, \sigma(u) = \sigma(v)} \E \tilde{A}_{uv} &=
  \sum_k \hat{n}_k (\hat{n}_k - 1) \E \hat{P}_l \leq (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1),
\end{align*}
implying that
\[
P\left( \left| \sum_{u,v\,:\, \sigma(u) = \sigma(v)} (\tilde{A}_{uv} - \E \tilde{A}_{uv} ) \right|  > t 
 \right) \leq 2 \exp\left( 
    - \frac{t^2}{ 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1)  + \frac{2}{3}t } 
\right).
\]
%Our goal is to choose an appropriate $t$ such that the probability is upper bounded by $2\exp( - C_1 \gamma n \log \frac{1}{\gamma} - (3+\delta_p) \log n )$. We choose the following $t$
Let
\begin{multline*}
t^2 = 4 \left\{  \left( 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) \right) 
      \left( 
     C_1\gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right) \right\} \\
      \vee 
   4  \left\{
   \left(  C_1\gamma n \log \frac{1}{\gamma} + (3 + \delta_p) \log n \right)^2 \right\},
\end{multline*}
%We now verify that the probability term is bounded by $2 \exp\left( -  \left(  C_1\gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right) \right)$.
for a constant $C_1$ to be defined later. Let
\begin{align*}
A = 2(P_l \vee Q_l)\sum_k \hat{n}_k (\hat{n}_k - 1), \quad \text{and} \quad B =  C_1\gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n.
\end{align*}
We split into two cases:
\begin{enumerate}
\item Suppose $A \geq B$. Then $t^2 = 4AB$, and the probability term is at most
\begin{equation*}
2 \exp \left( - \frac{4 AB}{A + \frac{4}{3} \sqrt{AB}} \right)  \leq 2 \exp \left( - \frac{4 AB}{A + \frac{4}{3} A} \right) \leq  2 \exp( - B ).
\end{equation*}
\item Suppose $A \leq B$. Then $t^2 = 4B^2$, and the probability term is at most
\begin{equation*}
2 \exp \left( - \frac{4 B^2}{A + \frac{4}{3} B} \right) \leq 2 \exp \left( - \frac{4 B^2}{ B + \frac{4}{3} B} \right) \leq 2 \exp( - B).
\end{equation*}
\end{enumerate}
In either case, with probability at least $1- 2 \exp\left( -  \left(  C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right) \right)$, we have
\begin{align*}
| \hat{P}_l - \E(\hat{P}_l) | =
\frac{\sum_{u \neq v \, \sigma(u) = \sigma(v)} (\tilde{A}_{uv} - \E \tilde{A}_{uv} ) }{
  \sum_{u \neq v} \mathbf{1}( \sigma(u) = \sigma(v) ) } & \le
  \frac{t}{\sum_{u \neq v} \mathbf{1}(\sigma(u) = \sigma(v)) }.
\end{align*}

We now derive a more manageable upper bound for $t$. Using the notation from above, we have $t^2 = \max(4AB, 4B^2) \leq 4(\sqrt{AB} +B)^2$.
%\[
%t^2 \leq 4 \left\{ \sqrt{  \left(2 (P_l \vee Q_l)  \sum_k \hat{n}_k (\hat{n}_k - 1) \right) 
%               \left( C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right)} 
%           + 
%          \left(   C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right) \right\}^2.
%\]
Since $\gamma \geq \frac{1}{n}$, we have $C_1\gamma n \log \frac{1}{\gamma} + (3 + \delta_p) \log n \leq \tilde C_1 \gamma n \log \frac{1}{\gamma}$, implying that
\begin{align*}
 \frac{t}{\sum_{u \neq v} \mathbf{1}(\sigma(u) = \sigma(v)) }
% &\leq 2 \frac{  \sqrt{  \left( 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) \right) 
%                 \left(  C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n \right) }  
%          + 
%               \left(  C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta_p) \log n\right)  }
%        { \sum_{u \neq v} \mathbf{1}(\sigma(u) = \sigma(v) ) }  \\
%        %
  &\leq 2 \frac{\sqrt{2 (P_l \vee Q_l) \tilde C_1 \gamma n \log \frac{1}{\gamma}}}
             {\sqrt{ \sum_k \hat{n}_k (\hat{n}_k - 1)}} + 
        2  \frac{\tilde C_1\gamma n \log \frac{1}{\gamma}}
             {\sum_k \hat{n}_k (\hat{n}_k - 1)} \\
             %
 &\stackrel{(a)}\leq  2 \frac{ \sqrt{2 (P_l\vee Q_l)} \sqrt{ \tilde C_1 \gamma K \log \frac{1}{\gamma}} }
           {\sqrt{n - K}} + 
       2 \frac{\tilde C_1 \gamma K \log \frac{1}{\gamma}}{n - K} \\
       %
 &\stackrel{(b)}\leq 4 \sqrt{ \frac{P_l \vee Q_l}{n} } \sqrt{\tilde C_1 K \gamma \log \frac{1}{\gamma}} + 
       4 \frac{\tilde C_1 K \gamma \log \frac{1}{\gamma}}{n},
\end{align*}
where $(a)$ uses inequality~\eqref{EqnSmall} and $(b)$ uses the assumption $n-K \geq \frac{n}{2}$.

To further simplify the expression, note that $P_l \vee Q_l \geq \frac{c}{n}$ implies $\frac{1}{n} \leq \frac{1}{\sqrt c}\sqrt{ \frac{P_l \vee Q_l}{n} }$, so with probability at least $1 - \exp( - C_1 \gamma n \log \frac{1}{\gamma} - (3 + \delta_p) \log n)$, we have
\begin{align}
| \hat{P}_l - \E(\hat{P}_l)| \leq \sqrt{ \frac{P_l \vee Q_l}{n} } \left( C_1' \sqrt{\gamma \log \frac{1}{\gamma}} + C_2' \gamma \log \frac{1}{\gamma} \right), \label{eqn:variance_bound}
\end{align}
for suitable constants $C_1'$ and $C_2'$. Using a similar calculation, we may show that there exist suitable constants $C_3'$ and $C_4'$ such that
\begin{align*}
| \hat{Q}_l - \E(\hat{Q}_l)| \leq \sqrt{ \frac{P_l \vee Q_l}{n} } \left( C_3' \sqrt{\gamma \log \frac{1}{\gamma}} + C_4' \gamma \log \frac{1}{\gamma} \right).
\end{align*}
When $\gamma$ is sufficiently small, the first term dominates, so we may take choose the right-hand sides to be $\eta_2 \sqrt{ \frac{P_l \vee Q_l}{n} }$, where $\eta_2 = C_3 \sqrt{\gamma \log \frac{1}{\gamma}}$.
%Clearly, $\eta_2$ may be made arbitrarily small by taking $\gamma$ arbitrarily small.

Finally, note that there are at most $\binom{n}{\gamma n} K^{\gamma n}$ possible $\sigma$'s satisfying the error bound. We have
\begin{align*}
\log \left(\binom{n}{\gamma n} K^{\gamma n}\right)
%& = \log \left( \frac{ n(n-1) ...(n-\gamma n+1) }{(\gamma n)!} \right) + \gamma n \log K \\
 & \leq \log \left( \frac{ n^{\gamma n} e^{\gamma n} }
     { (\gamma n)^{\gamma n} } \frac{1}{\sqrt{2\pi \gamma n}} \right) + \gamma n \log K \\
 & \leq \log \left( \frac{ e^{\gamma n} }{\gamma^{\gamma n}} \right) - \frac{1}{2} \log 2 \pi \gamma n + \gamma n \log K \\
 & \leq \gamma n \log \frac{e}{\gamma}  + \gamma n \log K \\
 & = \gamma n \log \frac{Ke}{\gamma}\\
 &\leq C_1 \gamma n\log \frac{1}{\gamma},
\end{align*}
for a suitable constant $C_1$. Taking a union bound across all cluster assignments, we then conclude that the probability of inequality~\eqref{eqn:variance_bound} holding simultaneously for all labels $l$ is at least $1 - L n^{-(3+\delta_p)}$.

Combining inequalities~\eqref{eq: A1.5} and~\eqref{eqn:variance_bound}, we arrive at the bound
\begin{equation}\label{eq: combine}
\max\left\{|P_l - \hat P_l|, \; |Q_l - \hat Q_l|\right\} \leq \eta_1 \Delta_l + \eta_2 \sqrt{\frac{P_l \vee Q_l}{n}}.
\end{equation}
If $\frac{n\Delta_l^2}{P_l \vee Q_l} \geq 1$, we therefore have
\begin{equation*}
\max\left\{|P_l - \hat P_l|, \; |Q_l - \hat Q_l|\right\} \leq \eta_1 \Delta_l + \eta_2 \Delta_l = (\eta_1 + \eta_2) \Delta_l,
\end{equation*}
whereas if $\frac{n\Delta_l^2}{P_l \vee Q_l} < 1$, we have
\begin{equation*}
\max\left\{|P_l - \hat P_l|, \; Q_l - \hat Q_l|\right\} \leq \eta_1\sqrt{\frac{P_l \vee Q_l}{n}}+ \eta_2 \sqrt{\frac{P_l \vee Q_l}{n}} = (\eta_1 + \eta_2)\sqrt{\frac{P_l \vee Q_l}{n}}. 
\end{equation*}
Since $\eta_2 = C_3 \sqrt{\gamma \log \frac{1}{\gamma}}$ dominates $\eta_1 = C_2 \gamma$ for sufficiently small $\gamma$, the desired bounds follow.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Analysis of spectral clustering}
\label{appendix: spectral}
%Define $\bar{d} = \frac{1}{n} \sum_{u=1}^n d_u$ be the average degree.

\begin{proposition}
\label{prop:spectral_analysis}
Suppose an unweighted adjacency matrix $A$ is drawn from a homogeneous stochastic block model with probabilities $p$ and $q$ and cluster imbalance factor $\beta$. Suppose $p, q \geq \frac{c}{n}$. Then there exists a constant $C$ such that if
$$256 \mu \beta C^2 K^3 \frac{(p \vee q)}{n (p-q)^2} \leq 1,$$
the output $\sigma$ of Algorithm~\ref{alg:spectral} with parameters $\mu \geq 32 C^2 \beta$ and $\tau = \bar{d}$ satisfies
\[
l(\sigma, \sigma_0) \leq 64 C^2 \beta  \frac{K^2 (p \vee q) }{n (p-q)^2},
\]
with probability at least $1 - n^{-C'}$, where $C' > 4$.
\end{proposition}

\begin{proof}
Note that the trim parameter $\tau$ is a random variable, since the average degree $\bar d$ is random. Since the community sizes are bounded by $\frac{n}{\beta K}$, we may find constants $C_{d_1} < C_{d_2}= 1$, depending only on $K$ and $\beta$, such that 
$$C_{d_1}n (p \vee q) \leq  \E[\bar d] \leq C_{d_2} n(p \vee q).$$ 
Using Hoeffding's inequality, we conclude that with probability at least $1 - \exp(-nC_{\bar d})$, for some constant $C_{\bar d}$, we have
\begin{equation}
\label{EqnEmerson}
\frac{C_{d_1}}{2} n (p \vee q) \leq  \bar d \leq 2C_{d_2} n(p \vee q).
\end{equation}
We now apply the following lemma:

\begin{lemma} (Lemma 5 of Gao et al.~\cite{GaoEtal15})
\label{lem:trimmed_A_bound}
Let $P \in [0,1]^{n \times n}$ be a symmetric matrix, and let $p_{max} := \max_{u \geq v} P_{uv}$. Let $A$ be an adjacency matrix such that $A_{uu} = 0$ and $A_{uv} \sim Ber(P_{uv})$ for $u < v$. For any $C' > 0$ and $0 < C_1 < C_2$, there exists some $C > 0$ such that
\[
\| T_\tau(A) - P \|_2 \leq C \sqrt{ n p_{max} + 1}, \qquad \forall \tau \in [C_1 (np_{max} + 1), C_2 (np_{max} + 1)],
\]
with probability at least $1-n^{-C'}$.
\end{lemma}

\remark{Lemma 5 from Gao et al.~\cite{GaoEtal15} is stated slightly differently---for any $C' >0$, there exist constants $c$, $C_1$, and $C_2$ such that the result holds with probability at least $1-n^{-C'}$. However, our restatement follows immediately from slight modifications of the proof. Furthermore, note that the statement of Lemma~\ref{lem:trimmed_A_bound} refers to the output of spectral clustering with respect to a fixed trim parameter, but we will apply it in a setting where $\tau$ is random.}

Using Lemma~\ref{lem:trimmed_A_bound} with a fixed $C' > 4$ and constants $C_1 = \frac{C_{d_1}}{2}$ and $C_2 = 2C_{d_2}$, we conclude that there exists a constant $C > 0$ such that
\begin{equation}
\label{EqnTofu}
\| T_{\tau}(A) - P \|_2 \leq C \sqrt{ n (p \vee q) }, \qquad \forall \tau \in [C_1, C_2],
\end{equation}
with probability at least $1 - n^{-C'}$. In particular, this inequality will also hold for the random choice $\tau = \bar d$, by inequality~\eqref{EqnEmerson} above.
%(Note that we may replace $\sqrt{n(p \vee q) + 1}$ by $\sqrt{n(p \vee q)}$, since we assume that $(p \vee q) > \frac{c}{n}$.)
Furthermore, we may assume that $C \geq 1$, since inequality~\eqref{EqnTofu} also holds with $C$ replaced by $\max(1,C).$ Thus,
\begin{align*}
\| \hat{A} - P \|_2 &\leq \| T_\tau(A)  - P \|_2 + \| \hat{A} - T_\tau(A) \|_2 \\
   &\stackrel{(a)}\leq 2\| T_\tau(A) - P \|_2 \\
  &\leq 2C \sqrt{ n (p \vee q)}, 
\end{align*}
where $(a)$ follows because $\hat{A}$ is the best rank-$K$ approximation of $T_\tau (A)$ and $\rank(P) = K$, so $\|T_\tau(A) - \hat{A}\|_2 \le \|T_\tau(A) - P\|_2$ by the Eckart-Young-Mirsky Theorem. This implies that
\begin{align*}
\sum_{u=1}^n \| \hat{A}_u - P_u \|_2^2 &= \| \hat{A} - P \|_F^2 \leq K\| \hat A - P \|_2^2 \leq 4 K C^2 n (p \vee q).
\end{align*}
We now denote the $K$ distinct rows of $P$ by $\{{\cal Z}_i\}_{1 \le i \le K}$, and for a vertex $u$, denote the row $P_u$ by ${\cal Z}(u).$ Note that
\[
\| {\cal Z}_i - {\cal Z}_j \|_2^2 \geq \frac{2n}{\beta K} (p - q)^2, \qquad \forall i \neq j,
\]
since each cluster contains at least $\frac{n}{\beta K}$ vertices.

%If the ${\cal Z}_i$'s were known, we could cluster $v$ by matching $\hat{A}_v$ to the closest ${\cal Z}_i$. We make a mistake only if $\| \hat{A}_u - P_u \|_2^2 = \| \hat{A}_u - {\cal Z}(u) \|_2^2 \geq \frac{1}{\beta K} (p-q)^2 n$. Thus, the number of mistakes we make is bounded by
%\[
%\frac{\sum_{u=1}^n \| \hat{A}_u - P_u \|_2^2}{ \frac{1}{\beta K} (p-q)^2 n } \leq \frac{ 4 K C^2 n (p \vee q)}{ \frac{1}{\beta K} (p-q)^2 n} \leq \frac{4 \beta K^2 C^2 (p \vee q)}{(p-q)^2}. 
%\]
%However, since we do not know the true ${\cal Z}_i$'s, we construct a set $S$ as a surrogate. The set has $K$ points, and every ${\cal Z}_i$ has a point in $S$ that is close to ${\cal Z}_i$ and acts as a surrogate for ${\cal Z}_i$.

A vertex $u$ is considered \emph{valid} if $\| \hat{A}_u - {\cal Z}_i \|_2^2 \leq \frac{1}{16} \frac{1}{\beta K} (p-q)^2 n$ for some ${\cal Z}_i$; otherwise, $u$ is \emph{invalid}. Also define
\begin{equation*}
{\cal Z}^*(u) := \argmin_{{\cal Z}_i} \| \hat{A}_u - {\cal Z}_i \|_2^2,
\end{equation*}
so ${\cal Z}^*(u)$ is the row of $P$ closest to $\hat{A}_u$. Note that if $u$ is valid, then $\| \hat{A}_u - {\cal Z}^*(u) \|_2^2 \leq \frac{1}{16} \frac{1}{\beta K} (p - q)^2 n$.

We show that the set $S$ constructed in Algorithm~\ref{alg:spectral} satisfies the following properties:\\

\noindent \textbf{Claim 1:} $S$ contains only valid points. \\
\textbf{Claim 2:} For every pair of distinct nodes $u,v \in S$, we have ${\cal Z}^*(u) \neq {\cal Z}^*(v)$. \\

We first prove that the proposition follows from the claims. We denote the rows of $\hat A$ corresponding to the members of $S$ by ${\cal S}_i$, assigning indices so that ${\cal S}_i$ is the surrogate for ${\cal  Z}_i$ (i.e., both ${\cal S}_i$ and ${\cal Z}_i$ are associated to a common vertex $u$). In particular, note that
\begin{equation*}
\|{\cal S}_i - {\cal Z}_i\|_2 \le \frac{1}{16} \frac{1}{\beta K} (p-q)^2 n, \qquad \forall 1 \le i \le K,
\end{equation*}
since $S$ only contains valid points. Let ${\cal S}(u)$ be the surrogate of ${\cal Z}(u)$, and denote ${\cal S}^*(u) =\arg\min_{{\cal S}_i} \| \hat A_u - {\cal S}_i \|^2$; i.e., the member of $S$ that is closest to $\hat A_u$. We say that a valid point $u$ is \emph{misclassified} if ${\cal S}^*(u) \neq {\cal S}(u)$. The number of mistakes we make is thus bounded by the number of invalid points plus the number of misclassified valid points. Note that if $u$ is invalid, we have $\|\hat{A}_u - P_u\|_2^2 \ge \frac{1}{16} \frac{1}{\beta K} (p-q)^2n$. We claim that the same inequality holds for any misclassified valid point $u$. 

Consider such a point $u$. Since $u$ is valid, there exists ${\cal Z}_i$ such that
$$ \| \hat A_u - {\cal Z}_i \|^2 \leq \frac{1}{16}\frac{1}{\beta K} (p-q)^2 n.$$
We claim that ${\cal S}^*(u) = {\cal S}_i$. For any $j \neq i$, we have
\begin{align*}
\| \hat A_u - {\cal S}_j \|_2 &\geq \|{\cal Z}_i - {\cal Z}_j \|_2 - \|{\cal Z}_j - {\cal S}_j \|_2 - \|{\cal Z}_i - \hat A_u\|_2 \\
&\geq \sqrt{\frac{2}{\beta K} (p-q)^2 n} - 2\sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n}\\
&> 2\sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n}.
\end{align*}
Furthermore,
\begin{align*}
\| \hat A_u - {\cal S}_i \|_2 &\leq \| \hat A_u - {\cal Z}_i \|_2 + \| {\cal S}_i - {\cal Z}_i \|_2 \leq 2 \sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n}.
\end{align*}
Thus, for any $j \neq i$, we have
\begin{align*}
\| \hat A_u - {\cal S}_j \|_2  > \| \hat A_u - {\cal S}_i \|_2,
\end{align*}
implying that ${\cal S}^*(u) = {\cal S}_i.$

Since $u$ is also misclassified, we have ${\cal S}(u) \neq {\cal S}^*(u) = {\cal S}_i$. Let ${\cal S}(u) = {\cal S}_j$ and ${\cal Z}(u) = {\cal Z}_j$. We have the following sequence of inequalities:
\begin{align*}
\|\hat A_u - {\cal Z}(u)\|_2 &= \| \hat A_u - {\cal Z}_j \|_2 \\
&\geq \| \hat A_u - {\cal S}_j \|_2 - \| {\cal S}_j - {\cal Z}_j \|_2 \\
&\geq 2\sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n} - \sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n}\\
&= \sqrt{\frac{1}{16}\frac{1}{\beta K} (p-q)^2 n},
\end{align*}
which is the bound we wanted to prove.

Finally, we conclude that the number of mistakes incurred by algorithm is bounded by
\[
\frac{\sum_{u=1}^n \| \hat{A}_u - P_u \|_2^2}{ \frac{1}{16 \beta K} (p-q)^2 n } \leq 
        \frac{ 4 K C^2 n (p \vee q)}{ \frac{1}{16 \beta K} (p-q)^2 n} \leq \frac{64 \beta K^2 C^2 (p \vee q)}{(p-q)^2},
\]
as wanted.

\paragraph{\textbf{Proof of Claim 1:}} Recall the notation $N(u) = \{ v \,:\, \| \hat{A}_u - \hat{A}_v \|_2^2 \leq \mu K^2 \frac{\bar{d}}{n} \}$. Furthermore, by a Chernoff bound, we have $\bar{d} \leq 2 (p \vee q) n$ with probability at least $1 - \exp(-C_{\bar d}n)$. 
We condition on this event so that if $v \in N(u)$, then $\| \hat{A}_u - \hat{A}_v \|^2 \leq 2 \mu K^2 (p \vee q)$. We prove the claim by showing that an invalid point $u$ cannot have $\frac{1}{\mu} \frac{n}{K}$ neighbors.

By the definition of invalidity, $ \| \hat{A}_u - {\cal Z}_i \|_2^2 \geq \frac{1}{16 \beta K} (p-q)^2 n$, for any ${\cal Z}_i$. Let $v$ be a neighbor of $u$. By the triangle inequality, we then have
\begin{align*}
\| \hat A_v - {\cal Z}(v) \|_2 &\geq  \|\hat A_u - {\cal Z}(v) \|_2 -  \|\hat A_u - \hat A_v \|_2 \\
&\geq \sqrt{\frac{1}{16 \beta K} (p-q)^2 n} - \sqrt{2 \mu K^2 (p \vee q)}\\
&\stackrel{(a)}\geq  \sqrt{\frac{1}{16 \beta K} (p-q)^2 n} - \sqrt{\frac{1}{64 \beta K} (p-q)^2 n} = \sqrt{\frac{1}{64 \beta K} (p-q)^2 n},
\end{align*}
where $(a)$ follows from our assumption coupled with the choice of $C \geq 1$, which essentially states that
$$2 \mu K^2 (p \vee q) \leq \frac{1}{128 \beta K} (p-q)^2 n < \frac{1}{64 \beta K} (p-q)^2 n.$$
Thus, for every neighbor $v$ of $u$, we must have $\| \hat{A}_v - P_v \|_2^2 \geq \frac{1}{64 \beta K} (p-q)^2 n$. The number of neighbors of $u$ may be bounded by 

\[
\frac{\sum_{v=1}^n \| \hat{A}_v - P_v \|_2^2}{ \frac{1}{64 \beta K} (p-q)^2 n } \leq 
        \frac{ 4 K C^2 n (p \vee q)}{ \frac{1}{64 \beta K} (p-q)^2 n} \leq \frac{256 \beta K^2 C^2 (p \vee q)}{(p-q)^2}. 
\]
By assumption, this quantity is less than $\frac{1}{\mu} \frac{n}{K}$.

\paragraph{\textbf{Proof of Claim 2:}} We first claim that in every cluster, at least half the points $u$ satisfy $\| \hat{A}_u - P_u \|_2^2 \leq \frac{1}{4} \mu K^2 (p \vee q)$. This is because the total error is bounded by $\sum_{u=1}^n \| \hat{A}_u - P_u \|_2^2 \leq 4 K C^2 n (p \vee q)$, so the total number of points that violate the condition is at most $\frac{ 4 K C^2 n (p \vee q)}{ \frac{1}{4} \mu K^2 (p \vee q)} \leq \frac{n}{2 \beta K}$, using the assumption that $\mu \geq 32 C^2 \beta$. 

For two points $u$ and $v$ in the same cluster satisfying $\| \hat{A}_w - P_w \|_2^2 \leq  \frac{1}{4} \mu K^2 (p \vee q)$ for $w \in \{u,v\}$, we also have $\| \hat{A}_u - \hat{A}_v \|_2^2 \leq \mu K^2 (p \vee q)$, by the triangle inequality. Thus, every cluster contains a point $u$ such that $N(u) \geq \frac{n}{2\beta K} \geq \frac{1}{\mu} \frac{n}{K}$, since $\mu \geq 32C^2\beta > 2\beta$ by our choice of $C > 1$.

Suppose that at iteration $r$, the set $S$ consists of points $s_1, \dots, s_r$, where $1 \leq r < K$, and suppose for a contradiction that $s_{r+1}$ is such that ${\cal Z}(s_{r+1}) = {\cal Z}(s_i)$ for some $1 \leq i \leq r$. Since $s_i$ and $s_{r+1}$ are both valid points, the triangle inequality implies
$$\| \hat A_{s_{r+1}} - \hat A_{s_i} \| \leq \frac{1}{4 \beta K} (p - q)^2 n.$$

On the other hand, because $S$ does not yet have cardinality $K$, some ${\cal Z}_j$ must exist that does not have a surrogate in $S$. The cluster that corresponds to ${\cal Z}_j$ must, by our neighborhood size analysis, contain a node $u$ such that $N(u) \geq \frac{1}{\mu} \frac{n}{K}$ and
\[
\| \hat{A}_u - {\cal Z}_j \|_2^2 \leq \frac{1}{4} \mu K^2 (p \vee q)  \leq \frac{1}{16} \frac{1}{\beta K} (p-q)^2 n,
\]
where the second inequality follows by assumption. Since ${\cal Z}_j \neq {\cal Z}(s_i)$ for any $1 \leq i \leq r$, we have $\| {\cal Z}_j - {\cal Z}(s_i) \|_2^2 \geq 2 \frac{1}{\beta K} (p-q)^2 n$, for all $1 \leq i \leq r$. By Claim 1, all $s_i$'s are valid, so
$$\| \hat{A}_{s_i} - {\cal Z}(s_i) \| \leq \frac{1}{16} \frac{1}{\beta K} (p - q)^2 n.$$ 
Hence, by the triangle inequality, we have $\| \hat{A}_u - \hat{A}_{s_i} \|_2^2 \geq \frac{1}{\beta K} (p - q)^2 n$, for all $s_i \in S$. This is a contradiction because $u$ is further from every point in $S$ than $s_{r+1}$, so our assumption that ${\cal Z}(s_{r+1}) =  {\cal Z}(s_i)$ must be incorrect.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\subsection{Choosing the label $l^*$}
\label{appendix: starry night}

First,  we show that for sufficiently well-separated labels, $\hat I_l$ is close to $\frac{( P_l -  Q_l)^2}{P_l \vee Q_l}$. If the probabilities are not well-separated, we claim that $\hat I_l$ is negligibly small.

\begin{proposition}
\label{prop:initial_guarantee}
Suppose $\frac{1}{\rho_L} \leq \frac{P_l}{Q_l} \leq \rho_L$ for all $l$. Let $\sigma^l$ be the output of spectral clustering based on $\tilde{A}_{ij} = \mathbf{1}(A_{ij} = l)$, and let $\hat{P}_l$ and $\hat{Q}_l$ be estimates of $P_l$ and $Q_l$ constructed from $\sigma^l$. There exist positive constants $C_{test}, C_1, C_2$, $C$, and $\delta_p$ such that, with probability at least $1 - Ln^{-3 + \delta_p}$, we have the following:

\begin{enumerate} 
\item For all labels $l$ satisfying $P_l \vee Q_l > \frac{c}{n}$ and $\Delta_l \geq \sqrt{C_{test}} \sqrt{ \frac{P_l \vee Q_l}{n}}$,
\begin{align}
C_1 \frac{ | P_l - Q_l |}{\sqrt{P_l \vee Q_l}}  \leq \frac{ | \hat{P}_l - \hat{Q}_l| }{\sqrt{ \hat{P}_l \vee \hat{Q}_l }} \leq  
C_2 \frac{ | P_l - Q_l | }{\sqrt{ P_l \vee Q_l}}.
\end{align}

\item For all labels satisfying $P_l \vee Q_l > \frac{c}{n}$ and $\Delta_l <  \sqrt{C_{test}} \sqrt{ \frac{P_l \vee Q_l}{n} }$,

\begin{align}
\frac{ | \hat{P}_l - \hat{Q}_l|}{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} \leq C \sqrt{ \frac{1}{n} }.
\label{eqn:bad_l_initialization}
\end{align}
\end{enumerate}
\end{proposition}

\begin{proof}
Recall from Proposition~\ref{prop:estimation_consistency} that given a clustering with error rate $\gamma$, and under the assumptions  $P_l \vee Q_l > \frac{c}{n}$ and $\Delta_l^2 \geq \frac{P_l \vee Q_l}{n}$, the estimated probabilities $\hat P_l$ and $\hat Q_l$ satisfy 
\begin{align*}
|\hat P_l - \hat P| &\leq \eta \Delta_l, \quad \text{and} \quad |\hat Q_l - \hat Q| \leq \eta \Delta_l,
\end{align*}
with probability at least $1-n^{-(3+\delta_p)}$. We first pick a value of $\gamma$ such that $\eta < \frac{1}{4}$. We now ensure that the error rate obtained from Proposition~\ref{prop:spectral_analysis} matches our choice of $\gamma$. Recall that Proposition \ref{prop:spectral_analysis} states that under the assumptions $P_l \vee Q_l > \frac{c}{n}$ and 
$$ C_1 \frac{P_l \vee Q_l}{n(P_l - Q_l)^2} \leq 1,$$
we have
$$l(\sigma, \sigma_0) \leq C_2 \frac{P_l \vee Q_l}{n(P_l - Q_l)^2},$$
for appropriate constants $C_1$ and $C_2$. In particular, for $C_{test} \ge 1$ sufficiently large,
\begin{align*}
C_1 \frac{P_l \vee Q_l}{n(P_l - Q_l)^2} & \leq \frac{C_1}{C_{test}} < 1, \quad \text{ and}\\
l(\sigma, \sigma_0) & \leq C_2 \frac{P_l \vee Q_l}{n(P_l - Q_l)^2} \leq \frac{C_2}{C_{test}} < \gamma,
\end{align*}
for all labels $l$ such that $\Delta_l \geq \sqrt{C_{test}} \sqrt{\frac{P_l \vee Q_l}{n}}$.
%We pick $C_{test}$ to be the maximum of $1$ and this value, so the results from Proposition \ref{prop:estimation_consistency} may also be applied for all colors satisfying the bound $\Delta_l \geq \sqrt{C_{test}} \sqrt{\frac{P_l \vee Q_l}{n}}$.
%In the rest of the proof, we will use the bounds from Proposition~\ref{prop:estimation_consistency} and the fact that $|\eta| < \frac{1}{4}.$ To bound $\frac{ | \hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}}$, we bound the numerator and denominator separately. First, we bound the numerator:
Next, note that
\begin{align*}
| \hat{P}_l - \hat{Q}_l | &\leq |\hat{P}_l - P_l| + |P_l - Q_l| + |\hat{Q}_l - Q_l| \leq 2 \eta \Delta_l + \Delta_l \leq \frac{3}{2} \Delta,
\end{align*}
and
\begin{align*}
| \hat{P}_l - \hat{Q}_l | &\geq  |P_l - Q_l| - |\hat{Q}_l - Q_l| - |\hat{P}_l - P_l| \geq  \Delta_l - 2\eta \Delta_l \geq \frac{1}{2} \Delta_l.
\end{align*}
Furthermore,
\begin{align*}
\hat{P}_l \vee \hat{Q}_l &\leq (P_l \vee Q_l) + \eta \Delta_l \leq (P_l \vee Q_l) + \eta (P_l \vee Q_l) \leq \frac{5}{4} (P_l \vee Q_l),
\end{align*}
and
\begin{equation*}
\hat{P}_l \vee \hat{Q}_l \ge (P_l \vee Q_l) - \eta \Delta_l \ge \frac{3}{4} (P_l \vee Q_l).
\end{equation*}
%For the lower bound on the denominator, we first observe that $\hat{P}_l \geq P_l - |\eta| \Delta_l $ and that 
%$\hat{Q}_l \geq Q_l - |\eta| \Delta_l$. Let us suppose without loss of generality that $P_l \geq Q_l$. Then
%\[
%\hat{P}_l \vee \hat{Q}_l \geq (P_l \vee Q_l) - |\eta| \Delta_l \geq \frac{3}{4} (P_l \vee Q_l),
%\]
We conclude that
\[
\frac{1}{\sqrt{5}} \frac{\Delta_l}{P_l \vee Q_l} \leq \frac{ | \hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} \leq \frac{3}{\sqrt{3}} \frac{\Delta_l}{P_l \vee Q_l}.
\]
%This completes the proof for all colors satisfying $\Delta_l^2 \geq C_{test} \frac{P_l \vee Q_l}{n}$. 

Now suppose $\Delta_l^2 < C_{test} \frac{P_l \vee Q_l}{n}$. Note that this does not necessarily imply  $\Delta_l^2 \leq  \frac{P_l \vee Q_l}{n}$, since $C_{test} \geq 1$. However, we may still take the maximum of the bounds provided in Proposition~\ref{prop:estimation_consistency}, so with probability at least $1 - L n^{-(3+\delta_p)}$,
\begin{align*}
|\hat{P}_l - P_l| \leq \eta \left( \Delta_l \vee \sqrt{\frac{P_l \vee Q_l}{n} } \right) \leq \frac{\sqrt{C_{test}}}{4}\sqrt{ \frac{P_l \vee Q_l}{n} },
\end{align*}
using the choice $\eta \leq \frac{1}{4}$. An analogous bound holds for $|\hat{Q}_l - Q_l|$. Hence,
\begin{align*}
|\hat{P}_l - \hat{Q}_l| &\leq \Delta_l + | \hat{P}_l - P_l| + |\hat{Q}_l - Q_l| \\
    &\leq \Delta_l + \frac{\sqrt{C_{test}}}{2} \sqrt{ \frac{P_l \vee Q_l}{n} } \\
    &\leq \frac{3}{2} \sqrt{C_{test}} \sqrt{ \frac{P_l \vee Q_l}{n} }.
\end{align*}
%To bound the denominator term $\sqrt{ \hat{P}_l \vee \hat{Q}_l}$, note that $\hat{P}_l \geq P_l - \frac{\sqrt{C_{test}}}{4}\sqrt{ \frac{P_l \vee Q_l}{n} }$ and $\hat{Q}_l \geq Q_l - \frac{\sqrt{C_{test}}}{4} \sqrt{ \frac{P_l \vee Q_l}{n}}$. Suppose without loss of generality that $P_l \geq Q_l$, so $P_l = (P_l \vee Q_l)$. Then
Now note that
\begin{align*}
\hat{P}_l \vee \hat{Q}_l &\geq (P_l \vee Q_l) - \frac{\sqrt{C_{test}}}{4} \sqrt{ \frac{P_l \vee Q_l}{n}} \geq C'(P_l \vee Q_l).
\end{align*} 
for some constant $C'$.
%where we have used the assumption that $P_l \vee Q_l \geq \frac{c}{n}$. Thus,
It follows that
\begin{align*}
\frac{| \hat{P}_l - \hat{Q}_l| }{\sqrt{ \hat{P}_l \vee \hat{Q}_l} } \leq C \sqrt{ \frac{1}{n} }.
\end{align*}
\end{proof}

%We then argue that 
%$$I_L = \Theta\left( \sum_{l \in L_1} \frac{\Delta_l^2} {P_l \vee Q_l} \right),$$
%from which we conclude the existence of a label $l$ for which $\frac{n( P_l -  Q_l)^2}{P_l \vee Q_l}$ is arbitrarily large.

We apply Proposition~\ref{prop:initial_guarantee} to conclude that Algorithm~\ref{alg:initialization1} succeeds in choosing a color $l^*$ for which $\frac{n( P_{l^*} -  Q_{l^*})^2}{P_{l^*} \vee Q_{l^*}}$ is arbitrarily large:

\begin{proposition}
\label{prop:initialization_correctness}
Suppose $a_n := \frac{ n I_L}{L \rho^4_L} \rightarrow \infty$. For sufficiently large $n$, with probability at least $1 - 2L n^{-(3+\delta_p)}$, we have $\frac{n (P_{l^*}-Q_{l^*})^2}{(P_{l^*} \vee Q_{l^*}) \rho^4_L} \ge C a_n$, for some constant $C$. 
\end{proposition}

\begin{proof}
Let $C_{test}$ be the constant in Proposition~\ref{prop:initial_guarantee}. By Lemma~\ref{lem:friday_night}, we know that  $I_L$ is of the same order as $\sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l}$, implying the existence of a label $l$ such that $\Delta_l \geq C_{test} \sqrt{\frac{P_l \vee Q_l}{n}}$ and $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq  C \frac{n I_L}{L} = C a_n \rho^4_L$, for a constant $C$. Suppose the event of Proposition~\ref{prop:initial_guarantee} holds, which happens with probability at least $1 - L n^{-(3+\delta_p)}$. 

\textbf{Step 1.} We claim that $l^*$ satisfies $\Delta_{l^*} \geq C_{test} \sqrt{ \frac{P_l \vee Q_l}{n}} $. Let $l$ be a label such that $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq C a_n \rho^4_L$, and suppose the claim is false. By Proposition~\ref{prop:initial_guarantee} and the maximality of $l^*$, we have
\begin{align*}
\frac{| \hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} 
\stackrel{(a)}  \leq \frac{| \hat{P}_{l^*} - \hat{Q}_{l^*} | }{\sqrt{ \hat{P}_{l^*} \vee \hat{Q}_{l^*}}} 
         \leq C \sqrt{ \frac{1}{n}},
\end{align*}
Proposition~\ref{prop:initial_guarantee} also implies that
\begin{align*}
\frac{| \hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} 
  \geq C' \frac{ | P_l - Q_l|}{\sqrt{P_l \vee Q_l}}
   \geq C'' \sqrt{\frac{a_n \rho^4_L}{n}}.
\end{align*}
However, this is a contradiction, since $a_n \rightarrow \infty$ and $\rho_L \geq 1$.

\textbf{Step 2:} Again, let $l$ be a label such that $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq C a_n \rho^4_L$. By Proposition \ref{prop:initial_guarantee}, we then have
\begin{align*}
\frac{ |P_{l^*} - Q_{l^*}|}{\sqrt{ P_{l^*} \vee Q_{l^*}}} &\geq 
C \frac{|\hat{P}_{l^*} - \hat{Q}_{l^*} | }{\sqrt{ \hat{P}_{l^*} \vee \hat{Q}_{l^*} }} 
\geq
C \frac{|\hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} \geq C' \frac{|P_l - Q_l|}{\sqrt{P_l \vee Q_l}} \geq C'' \sqrt{\frac{a_n \rho^4_L}{n} },
\end{align*}
implying the desired result.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%

\subsection{Analysis of error probability for a single node}
\label{appendix: single node}

\begin{proposition}
\label{prop:single_node_error_bound}
Let $u$ be an arbitrary fixed node, and let $\tilde \sigma_u$ be the output of Algorithm \ref{alg:initialization1}. Suppose $\pi_u \in S_K$ satisfies
$$l(\sigma_0, \tilde \sigma_u) = d(\sigma_0, \pi_u(\tilde \sigma_u)),$$
where both $l$ and $d$ are taken with respect to the set $\{1, 2, \dots, n\} \setminus \{u\}$. %Further suppose that the conditions of Proposition \ref{prop:labeled_sbm_rate} hold. 
Conditioned on the events that the error rate $\gamma$ of $\tilde \sigma_u$ satisfies $\gamma \rho_L^4 \to 0$, and also the event that the result of Proposition~\ref{prop:estimation_consistency} holds for a sequence $\eta$ satisfying $\eta \rho_L^2 \rightarrow 0$, we have
\[
\pi_u^{-1}(\sigma_0(u)) = \argmax_k \sum_{v\,: \tilde \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l),
\]
with probability at least $1 - (K-1)\exp \left( - (1 - o(1)) \frac{n}{\beta K} I_L \right)$.\end{proposition}

\begin{proof}
Throughout the proof, we assume that $n$ is large enough so $\frac{1}{2} \sum_l (\sqrt{P_l} - \sqrt{Q_l})^2 \leq \frac{1}{2}$. Suppose without loss of generality that $\sigma_0(u) = 1$.  We misclassify $u$ into community $k$ if 
\begin{align*}
\sum_{v\,:\, \tilde \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
&\geq 
 \sum_{v\,:\, \tilde \sigma_u(v)=1} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l),
\end{align*}
or equivalently,
\begin{align}
\sum_{v \,:\, \tilde \sigma_u(v) = k} \bar{A}_{uv} - \sum_{v\,:\, \tilde \sigma_u(v) = 1} \bar{A}_{uv}
\label{eqn:bad_event1}
&\geq 0, 
\end{align}
where $\bar{A}_{uv} \equiv \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l)$. Note that the edges from $u$ are independent of the clustering $\tilde \sigma_u$, since this clustering was obtained by running the algorithm with vertex $u$ excluded. 

Define $m_1 = |\{ v \,:\, \tilde \sigma_u(v) = 1 \}|$ and $m_k = | \{ v \,:\, \tilde \sigma_u(v) = k \}|$, and let $m_1' = \{ v \,:\, \tilde \sigma_u(v) = 1 ,\, \sigma_0(v) = 1\}$ be the points correctly clustered by $\sigma_u$. Let $m_k' = \{ v \,:\, \tilde \sigma_u(v) \neq  1,\, \sigma_0(v) = k \}$ denote the points correctly classified by $\tilde \sigma_u$ in community $k$. With these definitions, the probability of the bad event in equation~\eqref{eqn:bad_event1} is the probability of the event
\begin{align*}
\left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m'_k} \tilde{X}_i + \right) - 
\left( \sum_{i=1}^{m_1'} \tilde{X}_i + \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) &\geq 0,
\end{align*}
where $\tilde{X}_i = \log \frac{\hat{P}_l}{\hat{Q}_l}$ with probability $P_l$ and $\tilde{Y}_i = \log \frac{\hat{P}_l}{\hat{Q}_l}$ with probability $Q_l$. (For simplicity, we abuse notation and write $\tilde Y_i$ and $\tilde X_i$ in both bracketed terms. These random variables are not the same, but are independent and identical copies.) This is equal to the probability of the event
\begin{align*}
\exp \left( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i - 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) \right) &\geq 1.
\end{align*}
We further bound this probability as follows:
\begin{align*}
& P \left( \exp \left( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i- 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) \right) \geq 1 \right) \\ 
     %
&\leq \E \left[ 
\exp\left( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i - 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) \right)
 \right] \\ 
 %
&=  \E[\exp( t \tilde{Y}_i) ]^{m_k'} 
     \E[ \exp(t \tilde{X}_i ]) ^{m_k - m_k'}  
    \E[ \exp( - t \tilde{X}_i)] ^{m_1'} 
    \E[ \exp( -t \tilde{Y}_i )] ^{m_1 - m_1'} \\
    %
&= \left( \sum_l e^{t \log \frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k'}  
      \left( \sum_l e^{t \log \frac{\hat{P}_l}{\hat{Q}_l}} P_l \right)^{m_k - m_k'} 
      \left( \sum_l e^{- t \log \frac{\hat{P}_l}{\hat{Q_l}} } P_l \right)^{m_1'}
     \left( \sum_l e^{-t \log \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{m_1 - m_1'}.
\end{align*}
We will set $t = \frac{1}{2}$, in which case
\begin{align}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k'}
 \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l \right)^{m_k - m_k'}
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l \right)^{m_1 - m_1'}
       \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1'} \nonumber \\
=&  \left( \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right)^{m_k - m_k'}
 \left( \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right)^{m_1 - m_1'}  
   \label{eqn:excess_error_term} \\
 & \left( \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{m_k} 
    \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1}.
   \label{eqn:Ihat_error_term} 
\end{align} 
We bound terms~\eqref{eqn:excess_error_term} and~\eqref{eqn:Ihat_error_term} separately. Loosely speaking, we will show that term~\eqref{eqn:excess_error_term} is bounded in magnitude by $\exp( o(I_L) \frac{n}{K} )$, and term~\eqref{eqn:Ihat_error_term} is bounded by $\exp( - \frac{n}{\beta K} (1 + o(1) )I_L)$.\\ 

\noindent \textbf{Bound for term~\eqref{eqn:excess_error_term}.} 
We derive a number of separate lemmas bounding various intermediate terms in the computation. In particular, we use the bounds from Lemmas~\ref{lem:sqrt_ratio_times_ql}, \ref{lem:sqrt_ratio_pl_ql_minus_1}, and~\ref{lem:friday_night} in the following sequence of inequalities:
\begin{align*}
\left| 1 -  \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right|
 &= \left| \frac{ \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} (P_l - Q_l) }
     { \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l } \right| \\
% 1
&\stackrel{(a)}\leq \frac{8}{\sum_l \sqrt{P_l Q_l}} 
     \left| \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} }(P_l - Q_l) \right| \\
% 2
&\stackrel{(b)}\leq 16 \left|  \sum_{l} \left( \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right) (P_l - Q_l)  \right| \\
% 3
&\leq 16 \left| 
     \sum_{l \in L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} - 1 \right)(P_l - Q_l) 
     \right| + 16 \sum_{l \notin L_1} \left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} - 1 \right| |P_l - Q_l| \\
% 4
&\stackrel{(c)}\leq 16 \sum_{l \in L_1} \frac{\Delta^2_l}{Q_l}(1+ \eta') + 
      \sum_{i \notin L_1} 32 \rho_L \frac{\Delta_l}{\sqrt{n (P_l \vee Q_l)}} \\
%
&\leq 16 \rho_L \sum_{l \in L_1} \frac{\Delta^2_l}{P_l \vee Q_l} (1+ \eta') + 
      \sum_{l \notin L_1} 32 \rho_L \frac{\Delta_l}{\sqrt{n (P_l \vee Q_l)}} \\
% 5
&\stackrel{(d)}\leq C I_L  \rho_L(1 + \eta') + C' \rho_L \frac{L}{n} \stackrel{(e)} \leq C \rho_L I_L. 
\end{align*}
In $(a)$, we have used Lemma~\ref{lem:sqrt_ratio_times_ql}. In $(b)$, we have used the fact that $\sum_l  \sqrt{P_l Q_l}  \to 1$, so this sum exceeds $\frac{1}{2}$ when $n$ is sufficiently large. In $(c)$, we have employed Lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1}, which appropriately bounds the term $\left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} - 1 \right)$. Here, $\eta' = o(1)$. Inequality $(d)$ follows from Lemma~\ref{lem:friday_night}. Finally, inequality $(e)$ follows from the assumption that $\frac{I_L n}{L \rho_L^4} \rightarrow \infty$ (note that $\rho_L \geq 1$) and by appropriately redefining $\eta'$). Identical analysis shows that
\[
\left| 1 - \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right| 
\leq C\rho_LI_L. 
\]
Finally, note that $ |x| \leq \exp( | 1 - x | )$, so term~\eqref{eqn:excess_error_term} may be bounded as
\begin{align*}
  \left( \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right)^{m_k - m_k'}
 \left( \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right)^{m_1 - m_1'}  &\leq \exp\left( C \rho_L I_L (m_k - m_k' + m_1 - m_1')\right) \\
&\leq \exp( C I_L \rho_L \gamma n).
\end{align*}
Since $\gamma \rho_L = o(1)$,  we conclude that term~\eqref{eqn:excess_error_term} is bounded by $\exp \left( \frac{n}{K}o(I_L)\right)$, as desired.\\
 
\noindent \textbf{Bound for term~\eqref{eqn:Ihat_error_term}.} Let 
$\hat{I} = - \log \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right) \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right) $. 
With this definition, we have
\begin{align*}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k} 
       \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1} 
&= \exp( - \hat{I} )^{\frac{m_k + m_1}{2}}  \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}}.
\end{align*}
We claim that the following statements are true:
\begin{enumerate}
\item $ m_1, m_k \geq \frac{n}{\beta K} (1 -\beta K \gamma)$.
\item $\hat{I} \geq I_L(1 + o(1))$.
\item $\left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} = \exp\left(\frac{n}{K}  o(I_L)\right)$.
\end{enumerate}
Let us first assume these statements are true, and bound term~\eqref{eqn:Ihat_error_term}. We have
\begin{align*}
& \exp( - \hat{I} )^{\frac{m_1 + m_k}{2}}  \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} \\
 %
& \qquad \leq  \exp \left( - I_L(1+o(1)) \frac{n}{\beta K} \cdot (1 -\beta K \gamma) + \frac{n}{K} o(I_L)\right)  \leq \exp \left( - (1+o(1)) \frac{n}{\beta K} I_L  \right), 
\end{align*}
where the last inequality holds because $\gamma = o(1)$. It remains to prove the three claims.\\

\noindent \textbf{Claim 1:} This is straightforward. The labeling $\tilde \sigma_u$ has at most $\gamma n$ errors, so $m_1 \geq m_1' \geq \frac{n}{\beta K} - \gamma n$. A similar argument works for $m_k$.\\

\noindent \textbf{Claim 2:} We show that the estimation error of $\hat{P}_l, \hat{Q}_l$ does not make $\hat{I}$ too small. We begin by writing
\begin{align*}
\hat{I} - I_L &= - \log \frac{ 
     \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)
     \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)}{ 
          \left( \sum_l \sqrt{P_l Q_l} \right)^2 }.
          \end{align*}
Let us first consider the numerator:
\begin{align*}
& \left( \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)
\left( \sum_l \sqrt{ \frac{\hat{Q}_l}{\hat{P}_l}} P_l \right) \\
&= \left( \sum_l \sqrt{ P_l Q_l} \sqrt{ \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l}} \right) 
     \left( \sum_l \sqrt{P_l Q_l} \sqrt{ \frac{P_l}{\hat{P}_l} \frac{\hat{Q}_l}{ Q_l}} \right) \\
&= \sum_l P_l Q_l + 2\sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} + 
   \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
&= \left( \sum_l \sqrt{P_l Q_l} \right)^2 + \sum_{l < l'} 
                  \sqrt{P_l Q_l P_{l'} Q_{l'}} \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right), 
\end{align*}
where we define 
$$T_{l,l'} = \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} \frac{P_{l'}}{\hat{P}_{l'}} \frac{\hat{Q}_{l'}}{Q_{l'}}.$$
%It will be later shown that $T_{l,l'} \rightarrow 1$ and thus, continuing equation~\ref{eqn:Ihat_Istar},
Furthermore,
\begin{align}
\hat{I} - I_L &= - \log \left( 1 + \frac{ \sum_{l<l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right)}
    { \left( \sum_l \sqrt{ P_l Q_l} \right)^2 }  \right) \nonumber \\
     &  \geq  - \log \left( 1 + 4 \sum_{l<l'} \sqrt{P_l Q_l P_{l'} Q_{l'}}  
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right)  \right) 
  \quad \trm{(assuming $\sum_l \sqrt{P_l Q_l} \geq 1/2$)} \nonumber \\
   & \geq - 4 \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right). \label{eqn:Ihat_Istar2}
\end{align}
We now bound $|T_{l,l'} - 1|$:
\begin{align*}
|T_{l,l'} - 1| &= \left| \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} 
      \frac{P_{l'}}{\hat{P}_{l'}} \frac{\hat{Q}_{l'}}{Q_{l'}} - 1 \right| \\
 &= \left| \left( 1 - \frac{P_l - \hat{P}_l}{P_l} \right)
    \left( 1 - \frac{\hat{Q}_l - Q_l}{\hat{Q}_l} \right)
   \left( 1- \frac{\hat{P}_{l'} - P_{l'}}{\hat{P}_{l'}}\right)
   \left( 1 -  \frac{Q_{l'}- \hat{Q}_{l'}}{Q_{l'}} \right) -1 \right| \\
&\stackrel{(a)} \leq 2\left( \frac{|P_l - \hat{P}_l|}{P_l} +  \frac{|\hat{Q}_l - Q_l|}{\hat{Q}_l}
           +   \frac{| \hat{P}_{l'} - P_{l'}|}{\hat{P}_{l'}} +
               \frac{| Q_{l'} - \hat{Q}_{l'} | }{Q_{l'}} \right) \\
&\stackrel{(b)} \leq 4\left( \frac{|P_l - \hat{P}_l|}{P_l} +  \frac{|\hat{Q}_l - Q_l|}{Q_l}
           +   \frac{| \hat{P}_{l'} - P_{l'}|}{P_{l'}} +
               \frac{| Q_{l'} - \hat{Q}_{l'} | }{Q_{l'}} \right), 
\end{align*}
where $(a)$ and $(b)$ follow from Lemma~\ref{lem:bound_ratio_P_Pl}. Since we only work with pairs $(l, l')$ such that $l' > l$, we may choose any ordering we like. Thus, suppose the $l$'s are ordered in decreasing order of $\frac{|\hat{P}_l - P_l|}{P_l} + \frac{|\hat{Q}_l - Q_l|}{Q_l}$. For all pairs $l < l'$, we then have
\[
| T_{l,l'} - 1 | \leq 8
    \left( \frac{|\hat{P}_l - P_l|}{P_l} + \frac{|\hat{Q}_l - Q_l|}{Q_l} \right).
\]
By Proposition~\ref{prop:estimation_consistency}, we have
\begin{align*}
\frac{|P_l - \hat{P}_l|}{P_l} + \frac{|\hat{Q}_l - Q_l|}{Q_l} &\leq \eta \Delta_l \left(\frac{1}{P_l} + \frac{1}{Q_l}\right)
\leq \frac{\eta \Delta_l}{P_l \vee Q_l} \cdot 2\rho_L
\leq \eta' \frac{\Delta_l}{P_l \vee Q_l},
\end{align*}
for any $l \in L_1$. For $l \notin L_1$, we have
\begin{align*}
\frac{|P_l - \hat{P}_l|}{P_l} \leq \eta \sqrt{\frac{P_l \vee Q_l}{n P_l^2}} = \eta \frac{P_l \vee Q_l}{P_l} \sqrt{\frac{1}{n(P_l \vee Q_l)}} \leq \eta \rho_L \sqrt{\frac{1}{n(P_l \vee Q_l)}} \leq \eta' \sqrt{\frac{1}{n(P_l \vee Q_l)}},
\end{align*}
and similarly for the $\frac{|\hat{Q}_l - Q_l|}{Q_l}$ term. Plugging these bounds into the previous derivation, we obtain
\begin{equation*}
|T_{l,l'} - 1| \leq \begin{cases}
\eta'  \frac{\Delta_l}{P_l \vee Q_l}, & \text{for } l \in L_1, \\
%
\eta' \frac{1}{\sqrt{ n (P_l \vee Q_l)}}, & \text{for } l \notin L_1.
\end{cases}
\end{equation*}
We now use the Taylor approximation of $\sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2$ around $T_{l,l'}=1$:
\begin{align*}
\sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} -2  &= 
  \frac{1}{4} (T_{l,l'} - 1)^2 + O (T_{l,l'}-1)^3. 
\end{align*}
Continuing the bound~\eqref{eqn:Ihat_Istar2}, we then obtain
\begin{align*}
\hat{I} - I_L &\geq - 4 \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
&\geq - 4 \sum_{l \in L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
    %
    & \qquad \qquad
     - 4 \sum_{l \notin L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T_{l,l'}} + \frac{1}{\sqrt{T_{l,l'}}} - 2 \right) \\
  &\geq - \sum_{l \in L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
             \eta' \left( \frac{\Delta_l}{P_l \vee Q_l}  \right)^2 
        - \sum_{l \notin L_1} \sum_{l' > l} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
             \eta' \frac{1}{n (P_l \vee Q_l)} \\
&\geq - \eta' \left( \sum_{l \in L_1} \frac{\Delta_l^2 \sqrt{P_l Q_l}}{(P_l \vee Q_l)^2} \right)
 	\left( \sum_{l'}  \sqrt{P_{l'}Q_{l'}} \right) 
	- \eta' \left( \sum_{l \notin L_1} \frac{\sqrt{P_lQ_l}}{n(P_l \vee Q_l)} \right) 
          \left( \sum_{l'} \sqrt{P_{l'} Q_{l'} } \right) \\
 &\geq - \eta' \left( \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} \right)
         \left( \sum_{l'}  \sqrt{P_{l'}Q_{l'}} \right) 
       - \eta' \left( \sum_{l \notin L_1} \frac{1}{n} \right) 
          \left( \sum_{l'} \sqrt{P_{l'} Q_{l'} } \right) \\
 &\stackrel{(a)}=  -o(I_L),
\end{align*}
where $(a)$ follows from the fact that $\sum_{l'} \sqrt{P_{l'} Q_{l'}} \leq 1$, the statement $\sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} = \Theta(I_L)$ from Lemma~\ref{lem:friday_night}, and our assumption that  $\sum_{l \notin L_1} \frac{1}{n} \leq \frac{L}{n} = o(I_L)$. This proves the claim.\\

\noindent \textbf{Claim 3.} 
We rewrite the term in claim 3 as follows:
\begin{align*}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} \\
&= \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} 
   \left( \frac{\sum_l \sqrt{\hat{P}_l \hat{Q}_l}}{\sum_l \sqrt{\hat{P}_l \hat{Q}_l}} \right)^{\frac{m_1 - m_k}{2}} \\
&=  \left( 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} \hat{P_l} } \right)^{\frac{m_1 - m_k}{2}}. 
\end{align*}
Assume $m_k \geq m_1$. The reverse case may be analyzed in an identical manner. We may rewrite the term as
\begin{align*}
\left( 1 + 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l)}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( 1+ \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} (\hat{P}_l - P_l)}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l } \right)^{\frac{m_k - m_1}{2}}. 
\end{align*}
Note that $\sum_l \sqrt{P_l Q_l} \to 1$, so Lemma~\ref{lem:sqrt_ratio_times_ql} implies that the denominators are $\Theta(1).$ We bound the numerator as follows:
\begin{align*}
\left| \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l) \right|  &= 
  \left|  \sum_l \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) 
 \right| \\
 %
& \leq \left| \sum_{l \in L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) \right| +  %next term
  \left| \sum_{l \notin L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) \right| \\
%
& \stackrel{(a)} \leq \left| \sum_{l \in L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) \eta \Delta_l \right| +  %next term
  \left| \sum_{l \notin L_1} \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) \eta \sqrt{ \frac{P_l \vee Q_l}{n}}  \right| \\
%
&\stackrel{(b)} \leq \sum_{l \in L_1} \eta \frac{\Delta_l^2}{Q_l} + \sum_{l \notin L_1} \eta \rho_L \frac{1}{n} \\
%
&\leq \eta \rho_L \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} + \sum_{l \notin L_1} \eta \rho_L \frac{1}{n} \\
%
& \stackrel{(c)} \leq \eta' I_L + \eta' \frac{L}{n}  \\
%
&\stackrel{(d)}  \leq \eta' I_L.  
\end{align*}
In the above sequence of inequalities, step $(a)$ follows from Proposition~\ref{prop:estimation_consistency}, step $(b)$ follows from Lemma~\ref{lem:sqrt_ratio_pl_ql_minus_1}, step $(c)$ follows from Lemma \ref{lem:friday_night} and the assumption $\eta \rho_L \to 0$, and step $(d)$ follows from our assumption $\frac{L}{n} = o(I_L)$. Thus, we obtain
\begin{align*}
& \left( 1 + 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l)}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}}
   \left( 1+ \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} (\hat{P}_l - P_l)}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l } \right)^{\frac{m_k - m_1}{2}} \\
         %
& \qquad \leq \exp\left( (m_k - m_1) \log(1 + o(I_L) ) \right) \leq \exp \left( \frac{n}{K} o(I_L) \right) ,
\end{align*}
proving the claim.\\

\noindent \textbf{Combining bounds for  terms~\eqref{eqn:excess_error_term} and~\eqref{eqn:Ihat_error_term}:}  Multiplying the bounds for terms~\eqref{eqn:excess_error_term} and~\eqref{eqn:Ihat_error_term} shows that the probability of misclassifying $u$ into some cluster $k \neq 1$ is at most $\exp\left((1 + o(1)) \frac{nI_L}{\beta K} \right)$. Taking a union bound over all clusters $k \neq 1$ completes the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%

\subsection{Additional lemmas for Proposition~\ref{prop:labeled_sbm_rate}}
\label{appendix: lemmas for labeled_sbm_rate}

\begin{lemma}\label{lemma: first}
Let $L$, $P_l$, $Q_l$, $\rho_L$, and $I_L$ satisfy the assumptions in Proposition~\ref{prop:labeled_sbm_rate}. Define the new probabilities of edge labels as follows:
\begin{equation*}
P_l' := P_l(1-\delta) + \frac{\delta}{L+1}, \quad \text{and} \quad
Q_l' := Q_l(1-\delta) + \frac{\delta}{L+1},
\end{equation*}
for all $0 \leq l \leq L$, where $\delta = \frac{c(L+1)}{n}$. Let $I_L'$ denote the  Renyi divergence between $P_l'$ and $Q_l'$. Then for all sufficiently large $n$, we have $P_l', Q_l' > \frac{c}{n}$ for all $0 \leq l \leq L$, and 
\begin{equation*}
I_L' = I_L(1+o(1)).
\end{equation*}
\end{lemma}

\begin{proof}
Clearly, $P_l', Q_l' > \frac{c}{n}$. For the second part of the lemma, we begin by writing
\begin{align*}
I_L &= -2\log \sum_{l=0}^L \sqrt{P_lQ_l} = -2\log\left(1 - \frac{1}{2}\sum_{l=0}^L (\sqrt P_l - \sqrt Q_l)^2\right) =\left( \sum_{l=0}^L (\sqrt P_l - \sqrt Q_l)^2\right) (1+o(1)).
\end{align*}
Similarly, we have $I_L' = \left( \sum_{l=0}^L (\sqrt {P_l'} - \sqrt {Q_l'})^2\right) (1+o(1))$, so it is enough to show that 
$$\left( \sum_{l=0}^L (\sqrt P_l - \sqrt Q_l)^2\right) = \left( \sum_{l=0}^L (\sqrt {P_l'} - \sqrt {Q_l'})^2\right) (1+o(1)).$$ 

We consider two cases: $\rho_L = \omega(1)$ and $\rho_L = \Theta(1)$. If $\rho_L = \omega(1)$, we choose $a = \frac{nI_L}{\rho_L(L+1)}$. If $\rho_L = \Theta(1)$, we choose $a = o\left(\frac{nI_L}{L+1}\right)$ such that $a \to \infty$. Note that in both cases, we have $\frac{a}{\rho_L} \to \infty$ and $\frac{a(L+1)}{n} = o(I_L)$. We now break the set of labels into two groups, where $G_1$ contains all labels satisfying $P_l \vee Q_l \leq \frac{a}{n}$, and $G_2$ = $G_1^c$. 

Let $\Delta_l := |P_l - Q_l|$ and $\Delta_l' := |P_l' - Q_l'|$. For labels in $G_1$, we have $\Delta_l \leq \frac{a}{n}$. Thus,
\begin{align*}
(\sqrt{P_l} - \sqrt{Q_l})^2 = \frac{\Delta_l^2}{(\sqrt P_l + \sqrt Q_l)^2} \leq \Delta_l \leq \frac{a}{n},
\end{align*}
and
\begin{align*}
(\sqrt{P_l'} - \sqrt{Q_l'})^2 = \frac{(\Delta_l')^2}{(\sqrt {P_l'} + \sqrt {Q_l'})^2} \leq \Delta_l' = (1-\delta)\Delta_l \leq (1-\delta) \frac{a}{n}.
\end{align*}
Therefore,
\begin{align*}
\left | \sum_{l \in G_1} (\sqrt{P_l} - \sqrt{Q_l})^2 - \sum_{l \in G_1} (\sqrt{P_l'} - \sqrt{Q_l'})^2 \right | & \leq \frac{a(L+1)}{n} = o(I_L).
\end{align*}
For labels in $G_2$, we may write
\begin{align*}
&\left | \sum_{l \in G_2} (\sqrt{P_l} - \sqrt{Q_l})^2 - \sum_{l \in G_2} (\sqrt{P_l'} - \sqrt{Q_l'})^2 \right | = \sum_{l \in G_2} \frac{\Delta_l^2}{(\sqrt P_l + \sqrt Q_l)^2} \left | 1 - (1-\delta)^2\frac{(\sqrt P_l + \sqrt Q_l)^2}{(\sqrt{P_l'} + \sqrt{Q_l'})^2} \right |.
\end{align*}
We analyze the term inside the absolute value as follows:
\begin{align*}
\frac{(\sqrt P_l + \sqrt Q_l)^2}{(\sqrt{P_l'} + \sqrt{Q_l'})^2} &= \left(\frac{\sqrt P_l + \sqrt Q_l}{\sqrt P_l \sqrt{\frac{P_l'}{P_l}} + \sqrt Q_l \sqrt{\frac{Q_l'}{Q_l}}}\right)^2 =  \left(\frac{\sqrt P_l + \sqrt Q_l}{\sqrt P_l \sqrt{1-\delta + \frac{c}{nP_l}} + \sqrt Q_l \sqrt{1-\delta + \frac{c}{nQ_l}}}\right)^2.\\
\end{align*}
Since $P_l \vee Q_l > \frac{a}{n}$, we have
\begin{align*}
\frac{1}{n(P_l \vee Q_l)} < \frac{1}{a} = o(1), \quad \text{so} \quad \frac{1}{nP_l} \leq \frac{\rho_L}{n(P_l \vee Q_l)} < \frac{\rho_L}{a} = o(1).
\end{align*}
Furthermore, since $\delta = o(1)$, we have
\begin{align*}
\left(\frac{\sqrt P_l + \sqrt Q_l}{\sqrt P_l \sqrt{1-\delta + \frac{c}{nP_l}} + \sqrt Q_l \sqrt{1-\delta + \frac{c}{nQ_l}}}\right)^2 &= \left(\frac{\sqrt P_l + \sqrt Q_l}{\sqrt P_l (1+o(1)) + \sqrt Q_l (1+o(1))}\right)^2 = 1+o(1).
\end{align*}
Hence, we may conclude that
\begin{align*}
 \sum_{l \in G_2} \frac{\Delta_l^2}{(\sqrt P_l + \sqrt Q_l)^2} \left | 1 - (1-\delta)^2\frac{(\sqrt P_l + \sqrt Q_l)^2}{(\sqrt{P_l'} + \sqrt{Q_l'})^2} \right | &=  \sum_{l \in G_2} \frac{\Delta_l^2}{(\sqrt P_l + \sqrt Q_l)^2} \cdot o(1) = o(I_L).
\end{align*}
Combining the results for labels in $G_1$ and $G_2$, we conclude that $I_L' = (1+o(1)) I_L.$
\end{proof}

We often use the bound $\frac{1}{2} P \leq \hat{P}_l \leq 2 P_l$. The following lemma justifies this:
\begin{lemma}
\label{lem:bound_ratio_P_Pl}
Let $l$ be any label and suppose $\frac{1}{\rho_L} \leq \frac{P_l}{Q_l} \leq \rho_L$ where $\rho_L > 1$; also suppose $P_l, Q_l \geq \frac{c}{n}$. Conditioned on the event that the 
conclusion of Proposition~\ref{prop:estimation_consistency} holds with a sequence $\eta$ such that $\eta \rho_L^2 \rightarrow 0$, we have
\[
\max_l \frac{|\hat{P}_l - P_l|}{P_l} \rightarrow 0,
\quad \trm{and} \quad
\max_l \frac{|\hat{Q}_l - Q_l|}{Q_l} \rightarrow 0.
\]
In particular, for sufficiently small $\eta$, we have $\frac{1}{2} P_l \leq \hat{P}_l \leq 2P_l$, and likewise for $Q_l$.
\end{lemma}


\begin{proof}

We prove the statement first for $P_l$; the same argument applies to $Q_l$. By Proposition~\ref{prop:estimation_consistency}, we have that either $| \hat{P}_l - P_l | \leq \eta \Delta_l$ or $| \hat{P}_l - P_l | \leq \eta \sqrt{ \frac{P_l \vee Q_l}{n} }$. First suppose $| \hat{P}_l - P_l | \leq \eta \Delta_l$. Then
\begin{align*}
\frac{ | \hat{P}_l - P_l |}{P_l} \leq \eta \frac{\Delta_l}{P_l} \leq \eta (1+\rho_L) \rightarrow 0.
\end{align*}
If instead $| \hat{P}_l - P_l | \leq \eta \sqrt{ \frac{P_l \vee Q_l}{n} }$, we have
\begin{align*}
\frac{| \hat{P}_l - P_l |}{P_l} \leq \eta \sqrt{ \frac{\rho_L}{ P_l n}} \leq \eta \sqrt \rho_L \sqrt{ \frac{1}{c}} \rightarrow 0,
\end{align*}
where we use the fact that $\frac{P_l \vee Q_l}{P_l}$ is at most $\rho_L$.
\end{proof}



\begin{lemma}
\label{lem:sqrt_ratio_pl_ql_minus_1}
Suppose $\frac{1}{\rho_L} \leq \frac{P_l}{Q_l} \leq \rho_L$ and $P_l, Q_l \geq \frac{c}{n}$ for all $l$, where $\rho_L > 1$. Conditioned on the event that the 
conclusion of Proposition~\ref{prop:estimation_consistency} holds for a sequence $\eta$ such that $\eta \rho_L^2 \rightarrow 0$:
\begin{enumerate}
\item 
For all $l$ satisfying  $n \frac{\Delta_l^2}{P_l \vee Q_l} \geq 1$, we have  
\[
\left| \sqrt{ \frac{\hat{P}_l }{\hat{Q}_l} } - 1 \right|
                    \leq \left| \frac{P_l - Q_l}{Q_l} \right| (1 + \eta'),
\]
where $\eta' \rightarrow 0$ and $\eta'$ does not depend on the color $l$. 

\item
For all $l$ satisfying $n \frac{\Delta_l^2}{P_l \vee Q_l} < 1$, we have
\[
\left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right| \leq
 2 \rho_L \frac{1}{\sqrt{n  (P_l \vee Q_l) } }.
\]

\end{enumerate}
By symmetry, the same bounds also hold for $\sqrt{ \frac{\hat{Q}_l}{\hat{P}_l}} - 1$.
\end{lemma}

\begin{proof}
First suppose $l$ satisfies $\frac{n \Delta_l^2}{P_l \vee Q_l} \geq 1$. By Lemma~\ref{lem:bound_ratio_P_Pl}, we have $ \frac{\hat{Q}_l - Q_l}{Q_l} = \eta'$, where $\eta' \rightarrow 0$. In the following derivation, we use $\eta'$ to denote a sequence such that $\eta' = o(1)$; the actual value of $\eta'$ may change from instance to instance. We use $\eta$ to denote a sequence where $\eta \rho_L = o(1)$. We have
\begin{align*}
\frac{\hat{P}_l}{\hat{Q}_l} - 1&= 
      \frac{ \hat{P}_l - P_l + P_l }{ \hat{Q}_l - Q_l + Q_l} -1 = \frac{  \frac{\hat{P}_l - P_l}{Q_l} + \frac{P_l}{Q_l}}
       { \frac{\hat{Q}_l - Q_l}{Q_l} + 1} - 1 \\
       %
 &= \left( \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} \right)
    \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta')  \right) -1  \\
    %
 &= \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} 
     - \frac{P_l}{Q_l} \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta') 
     - \frac{\hat{P}_l - P_l}{Q_l} \frac{\hat{Q}_l - Q_l}{Q_l}(1+ \eta') 
   - 1 \\
   %
 &\stackrel{(a)}= \frac{P_l}{Q_l} + \frac{\eta \Delta_l}{Q_l} 
     + \rho_L \frac{\eta \Delta_l}{Q_l} (1+\eta') + \frac{\eta \Delta_l}{Q_l} \eta' 
   - 1 \\
   %
 &= \frac{P_l}{Q_l} + \eta \frac{\Delta_l}{Q_l}
     + \eta \rho_L \frac{\Delta_l}{Q_l}
   - 1 \\
   %
 &= \frac{P_l - Q_l}{Q_l} ( 1 + \eta').
\end{align*}
In $(a)$, we have used the fact that $| \hat{P}_l - P_l| \leq \eta \Delta_l$ and $| \hat{Q}_l - Q_l| \leq \eta \Delta_l$ by Proposition~\ref{prop:estimation_consistency}. Applying the inequality $|\sqrt{x} - 1| \leq |x-1|$ then completes the proof of the first case.

The proof of the second case is almost identical. Suppose $l$ satisfies $\frac{n\Delta_l^2}{P_l \vee Q_l} < 1$. Then
\begin{align*}
| \hat{P}_l - P_l | &= \eta \sqrt{ \frac{P_l \vee Q_l}{n} }, \quad \text{and} \quad | \hat{Q}_l - Q_l | = \eta \sqrt{ \frac{P_l \vee Q_l}{n} }.
\end{align*}
By Lemma~\ref{lem:bound_ratio_P_Pl}, we have $\frac{\hat{Q}_l - Q}{Q_l} = \eta'$, where $\eta' = o(1)$.
%In the following derivation, we use $\eta'$ to denote a sequence such that $\max_l |\eta'| = o(1)$; the actual value of $\eta'$ may change from instance to instance. We have the following equalities:
Hence,
\begin{align*}
\frac{\hat{P}_l}{\hat{Q}_l} - 1 &= 
     \left( \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} \right)
     \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} ( 1 + \eta') \right) - 1 \\
     %
 &= \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} 
     - \frac{P_l}{Q_l} \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta') 
     - \frac{\hat{P}_l - P_l}{Q_l} \frac{\hat{Q}_l - Q_l}{Q_l}(1+ \eta') 
   - 1  \\
   %
&= \frac{P_l }{Q_l}  + \eta \sqrt{\frac{P_l \vee Q_l}{nQ_l^2}} + \rho_L \eta \sqrt{\frac{P_l \vee Q_l}{nQ_l^2}} + \eta \sqrt{\frac{P_l \vee Q_l}{nQ_l^2}} -1\\ 
%
&= \frac{P_l }{Q_l}  + \rho_L \eta \sqrt{\frac{(P_l \vee Q_l)^2}{nQ_l^2(P_l \vee Q_l)}} -1\\ 
%
&= \frac{P_l - Q_l}{Q_l}  + \eta \rho_L^2 \sqrt{\frac{1}{n(P_l \vee Q_l)}}\\
%
 &= \frac{P_l - Q_l}{Q_l} + \eta' \rho_L \sqrt{ \frac{1}{ n (P_l \vee Q_l)} } .
\end{align*}
Using the inequality $|\sqrt{1+x}-1| \leq x$ for $x \geq 0$, we conclude that
\begin{align*}
\left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right| &= 
 \left|  \sqrt{ 1 + \frac{P_l - Q_l}{Q_l} + \eta' \rho_L \frac{1}{\sqrt{n (P_l \vee Q_l)} }}
   -1  \right| \\
  &\leq \left| \frac{P_l - Q_l}{Q_l} + \eta' \rho_L \frac{1}{\sqrt{ n (P_l \vee Q_l)}}    \right| \\
 &\stackrel{(a)} \leq 2 \rho_L \frac{1}{\sqrt{n (P_l \vee Q_l)} }, 
\end{align*}
where $(a)$ follows because we have assumed that $\frac{n\Delta_l^2}{P_l \vee Q_l} <1$, implying
$$ \left| \frac{P_l - Q_l}{Q_l} \right|\leq \sqrt{\frac{P_l \vee Q_l}{n Q_l^2}} =  \sqrt{\frac{(P_l \vee Q_l)^2}{n Q_l^2(P_l \vee Q_l)}} \leq \rho_L \frac{1}{\sqrt{n(P_l \vee Q_l)}}.$$
\end{proof}

The following lemma provides a bound for $\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } Q_l$:
\begin{lemma}
\label{lem:sqrt_ratio_times_ql}
Suppose
\[
\frac{|\hat{Q}_l - Q_l|}{Q_l} = \eta', \quad \text{and} \quad \frac{|\hat{P}_l - P_l|}{P_l} = \eta',
\]
where $\eta' = o(1)$. For all sufficiently small $\eta$, we have
\[
  \frac{1}{8} \sqrt{P_l Q_l} \le \frac{1}{2} \sqrt{\hat{P}_l \hat{Q}_l} \le \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } Q_l.
\]
\end{lemma}


\begin{proof}
We have the sequence of equalities
\begin{align*}
\sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l  &= \sqrt{ \hat{P}_l \hat{Q}_l} \frac{Q_l}{\hat{Q}_l} = \sqrt{\hat{P}_l \hat{Q}_l} \frac{1}{ \frac{\hat Q_l - {Q}_l}{Q_l} + 1 } \\
&= \sqrt{\hat{P}_l \hat{Q}_l} 
  \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} (1 + \eta') \right) = \sqrt{\hat{P}_l \hat{Q}_l} (1 - \eta),
\end{align*}
where the penultimate equality uses the fact that $\frac{\hat{Q}_l - Q_l}{Q_l} = \eta' \rightarrow 0$. Taking $\eta$ sufficiently small yields the upper bound.

For sufficiently small $\eta$, we also have $\hat{P}_l \geq \frac{1}{2} P_l$ and $\hat{Q}_l \geq \frac{1}{2} Q_l$, yielding the lower bound.
\end{proof}

\begin{lemma}\label{lem:friday_night}
Define $L_1 = \{ l \,:\, \frac{n \Delta_l^2}{P_l \vee Q_l} \geq C_{test}^2 \}$. Then 
\begin{align}
\label{EqnChick}
 C_1\sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} \leq I_L \leq  C_2 \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l},
 \end{align}
for some constants $C_1$ and $C_2$. 
\end{lemma}
\begin{proof}
Throughout the proof, let $\eta'$ denote a sequence converging to 0, and let $C$ denote a $\Theta(1)$ sequence that may change from line to line. First observe that
\begin{align*}
I_L  = -2 \log \sum_l \sqrt{P_l Q_l} = -2 \log \left( 1 - \frac{\sum_l (\sqrt P_l - \sqrt Q_l)^2}{2}\right).
\end{align*}
Using the fact that $I_L \to 0$ as $n \to \infty$, so $\sum_l (\sqrt P_l - \sqrt Q_l)^2 \to 0$, we have the following bound for sufficiently large $n$:
\begin{align*}
\frac{1}{2} \sum_l (\sqrt{P_l} - \sqrt{Q_l} )^2 \leq I_L \leq 2 \sum_l (\sqrt{P_l} - \sqrt{Q_l} )^2.
 \end{align*}
Since $\sum_l (\sqrt{P_l} - \sqrt{Q_l} )^2 = \sum_l \frac{\Delta_l^2}{(\sqrt{P_l} + \sqrt{Q_l})^2}$, there exist constants $\tilde C_1$ and $ \tilde C_2$ such that
\begin{align*}
\tilde C_1\sum_{l } \frac{\Delta_l^2}{P_l \vee Q_l} \leq I_L \leq  \tilde C_2 \sum_{l } \frac{\Delta_l^2}{P_l \vee Q_l}.
\end{align*}
%Observe that
%\begin{align*}
%\sum_{l } \frac{\Delta_l^2}{P_l \vee Q_l} &= \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} + \sum_{l \in L_1^c} \frac{\Delta_l^2}{P_l \vee Q_l}.
%\end{align*}
%The contribution of the colors in $L_1^c$ is bounded as
Now note that
\begin{align*}
\sum_{l \in L_1^c} \frac{\Delta_l^2}{P_l \vee Q_l} &\leq \frac{LC_{test}^2}{n},
\end{align*}
and by our assumptions, $\frac{L}{n} = o(I_L)$. Hence, the sum over labels in $L_1$ must be $\Theta(I_L)$, which is equivalent to inequality~\eqref{EqnChick}.
%i.e., there exist constants $C_1$ and $C_2$ such that
%$$ C_1\sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l} \leq I_L \leq  C_2 \sum_{l \in L_1} \frac{\Delta_l^2}{P_l \vee Q_l}.$$
\end{proof}

We state Lemma 4 from Gao et al.~\cite{GaoEtal15}, which analyzes the consensus step of the algorithm:
\begin{lemma} 
\label{lem:consensus_analysis}
Let $\sigma$ and $\sigma'$ be two clusters such that, for some constant $C \geq 1$, the minimum cluster size is at least $ \frac{n}{Ck}$. Define the map $\xi \,:\, [k] \rightarrow [k]$ according to
\begin{equation*}
\xi(k) = \argmax_{k'} | \{ v \,:\, \sigma(v) = k \} \cap \{ v \,:\, \sigma'(v) = k' \} |.
\end{equation*}
If $\min_{\pi \in S_k} l(\pi(\sigma), \sigma') < \frac{1}{Ck}$, we have $\xi \in S_k$ and $l(\xi(\sigma), \sigma') = \min_{\pi \in S_k} l(\pi(\sigma), \sigma')$. 
\end{lemma}

We include a simple additional lemma:
\begin{lemma}
\label{lem:consensus_uniqueness}
Let $\sigma, \sigma' \,:\, [n] \rightarrow [K]$ be two clusterings where the minimum cluster size of $\sigma$ is $T$. Let $\pi, \xi \in S_K$ be such that $d(\pi(\sigma), \sigma') < \frac{T}{2n}$ and $d(\xi(\sigma), \sigma') < \frac{T}{2n}$. Then $\pi = \xi$.
\end{lemma}

\begin{proof}
Suppose the contrary, and choose any $k$ such that $\pi(k) \neq \xi(k)$. We then have
\begin{align*}
| \{ \sigma(u) = k \} \cap \{ \sigma'(u) \neq \pi(k) \} | < n \cdot d(\pi(\sigma), \sigma') < \frac{T}{2},
\end{align*}
implying that $| \{ \sigma(u) = k \} \cap \{ \sigma'(u) = \pi(k) \}| > \frac{T}{2}$. But then
\begin{align*}
n \cdot d(\xi(\sigma), \sigma') &\geq | \{ \sigma(u) = k \} \cap \{\sigma'(u) \neq \xi(k) \} | \geq | \{ \sigma(u) = k\} \cap \{ \sigma'(u) = \pi(k) \}| \geq \frac{T}{2},
\end{align*}
a contradiction.
\end{proof}

%*********************************************************************************************************%
%*********************************************************************************************************%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%*********************************************************************************************************%
%*********************************************************************************************************%

\section{Proof of Proposition~\ref{prop:discretization1}}

%For convenience, we restate the proposition:
%\begin{repproposition} {prop:discretization1}
%Let $p(x), q(x)$ be two densities supported on $[0,1]$. Suppose that $H \equiv \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = o(1)$ and suppose they satisfy the following assumptions:

%\begin{enumerate}
%\item[C1] $p(z), q(z) > 0$ on $(0, 1)$, and $\sup_n \sup_z p(z) \vee q(z) < \infty$. 
%\item[C2] There exists $R$ a subinterval of $[0,1]$ such that (a) for all $z \in R$, $\frac{1}{\rho} \leq \left| \frac{p(z)}{q(z)} \right| \leq \rho$ where $\rho$ is an absolute constant and (b) $\mu\{R^c\} = o(H)$ where $\mu$ is the Lebesgue measure.

%\item[C3] Define $\alpha^2 = \int_R \frac{(p(z) - q(z))^2}{q(z)} dz$ and $\gamma(z) = \frac{q(z) - p(z)}{\alpha}$. Suppose $\sup_n \int_R q(z) \left| \frac{\gamma(z)}{q(z)} \right|^r dz  < \infty$ for an absolute constant $r \geq 4$.
%\item[C4] There is some $h_{n}(z)$ such that (1) $h_{n}(z) \geq \max \left\{  \left|\frac{\gamma'(z)}{q(z)} \right|, 
% \left|\frac{q'(z)}{q(z)}\right|  \right\} $, 
%(2) $h_{n}(z)$ is $(c_{s1}', c_{s2}', C_s')$-bowl-shaped for absolute constants $c_{s1}', c_{s2}', C_s'$, and (3) $\sup_n \int_R |h_{n}(z)|^t dz < \infty$ for an absolute constant $1 > t > 2/r$. 
%\item[C5] $p'(z), q'(z) \geq 0$ and for all $z < c'_{s1}$ and $p'(z), q'(z) \leq 0$ for all $z > c'_{s2}$.
%\end{enumerate}

%Suppose $\frac{1}{c_0} \leq \frac{1 - P_0}{1-Q_0} \leq c_0$. Let $\\bin_l = [a_l, b_l]$ for %$l=1,...,L$ be a uniformly spaced binning of the interval $[0,1]$ and let
%\begin{align*}
%\tilde P_l &= (1- P_0) \int_{a_l}^{b_l} p(x) dx := (1-P_0) P_l\quad \text{ and }\\
%\tilde Q_l &= (1-Q_0)\int_{a_l}^{b_l} q(x) dx := (1-Q_0) Q_l.
%\end{align*}
%Suppose $L \leq \frac{2}{H}$. Define 
%\begin{align*}
%I &= -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0) p(x) q(x)} dx \right) \quad \text{ and }\\
%I_L &= -2 \log \left( \sqrt{P_0 Q_0} + \sum_{l=1}^L \sqrt{\tilde P_l \tilde Q_l} \right).
%\end{align*}
%Then, we have that
% $$\left| \frac{I - I_L}{I} \right| = o(1),$$ 
%and that $\frac{1}{2\rho c_0} \leq \frac{\tilde P_l}{\tilde Q_l} \leq 2\rho c_0$ for all $l$. 
%\end{repproposition}

\begin{proof}
We use the notation
\begin{align*}
\tilde P_l & := (1- P_0) \int_{a_l}^{b_l} p(x) dx := (1-P_0) P_l, \\
\tilde Q_l & := (1-Q_0)\int_{a_l}^{b_l} q(x) dx := (1-Q_0) Q_l.
\end{align*}
We first show that the likelihood ratio $\frac{\tilde P_l}{\tilde Q_l} = \frac{1-P_0}{1-Q_0} \frac{P_l}{Q_l}$ satisfies the claimed bounds. Consider an $l$ such that $\bin_l \cap R^c = \emptyset$. For all $x \in \bin_l$, we have $\frac{1}{\rho} \leq \frac{p(x)}{q(x)} \leq \rho$ by Assumption C2'. It follows that $\frac{P_l}{Q_l} = \frac{\int_{\bin l} p(x)dx}{\int_{\bin_l} q(x)dx} \leq \rho.$ The lower bound is derived in the same manner. Since $\frac{1-P_0}{1-Q_0} \in \left[\frac{1}{c_0}, c_0\right]$, we conclude that $\frac{1}{\rho c_0} \le \frac{P_l}{Q_l} \le \rho c_0$.

Now suppose $\bin_l \cap R^c \neq \emptyset$. Since $R$ is an interval and that $\mu\{R^c\} = o(H)$, and since $L \leq \frac{2}{H}$, we conclude that $\mu\{R^c\} < \frac{1}{2L}$ for all sufficiently large $n$. Thus, only bins $[0, \frac{1}{L}]$ and $[1-\frac{1}{L}, 1]$ can satisfy $\bin_l \cap R^c \neq 0$. Note that Assumption C5' implies both $p(x)$ and $q(x)$ are increasing in $\left[0, \frac{1}{L}\right]$ and decreasing in $\left[1-\frac{1}{L}, 1\right]$. Define $P_l' = \int_{\bin_l \cap R} p(x) dx$ and $Q'_l = \int_{\bin_l \cap R} q(x) dx$, and define $P_l'' = \int_{\bin_l \cap R^c} p(x) dx$ and $Q''_l = \int_{\bin_l \cap R^c} q(x) dx$. Then $P_l = P_l' + P_l''$ and $Q_l = Q_l' + Q_l''$. Note that $\frac{1}{\rho} \leq \frac{P'_l}{Q'_l} \leq \rho$ by the same argument as before. Furthermore, using the monotonic properties of $p(x)$ and $q(x)$ in the relevant intervals, we have
\[
P'_l \geq \min_{x \in \bin_l \cap R} \frac{p(x)}{2L} \geq \max_{x \in \bin_l \cap R^c} \frac{p(x)}{2L} \geq P''_l,
\]
where the first inequality follows because $\mu(R^c) \leq \frac{1}{2L}$, and the second inequality follows from Assumption C5'. Similarly, $Q'_l \geq Q''_l$. Thus,
\begin{align*}
\frac{P_l}{Q_l} &\leq \frac{2 P'_l}{ Q'_l} \leq 2\rho, \quad \text{and} \quad \frac{P_l}{Q_l} \geq \frac{P'_l}{2 Q'_l} \geq \frac{1}{2\rho}.
\end{align*}
Using the bound on $\frac{1-P_0}{1-Q_0}$ completes the proof.\\

We now proceed with bounding $|I-I_L|$. Using the simple relation between Renyi divergence and Hellinger distance detailed in Lemma~\ref{lem:renyi_hellinger}, we have
\begin{align*}
I &= (1+o(1))\left\{ (\sqrt{P_0} - \sqrt{Q_0} )^2 + 
          \int \left( \sqrt{(1-P_0)p(x)} - \sqrt{(1-Q_0)q(x)} \right)^2 dx \right\} \\
  &= (1+o(1)) \Bigg\{ 
       (\sqrt{P_0} - \sqrt{Q_0} )^2 + (\sqrt{1- P_0} - \sqrt{1-Q_0} )^2 \\
       %
       & \qquad \qquad \qquad \qquad + \sqrt{(1-P_0)(1-Q_0)} \int \left( \sqrt{p(x)} - \sqrt{q(x)} \right)^2 dx \Bigg\}.
\end{align*}
Likewise,
\begin{align*}
I_L &= (1+o(1))\left\{ (\sqrt{P_0} - \sqrt{Q_0} )^2 + 
          \sum_{l=1}^L ( \sqrt{\tilde P_l} - \sqrt{\tilde Q_l})^2 dx \right\} \\
  &= (1+o(1)) \left\{ 
       (\sqrt{P_0} - \sqrt{Q_0} )^2 + (\sqrt{1- P_0} - \sqrt{1-Q_0} )^2
     + \sqrt{(1-P_0)(1-Q_0)} \sum_{l=1}^L (\sqrt{{P}_l} - \sqrt{{Q}_l})^2 \right\}.
\end{align*}
The key step in completing our proof is the following proposition, proved in Appendix \ref{appendix: d1}:
\begin{proposition}
\label{prop:H_HL_convergence1}
Under Assumptions C1'--C5', we have
\[
\left| \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx - \sum_{l=1}^L (\sqrt{{P}_l} - \sqrt{{Q}_l})^2 \right| = o\left( \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right).
\]
\end{proposition}
The claimed result follows from Proposition~\ref{prop:H_HL_convergence1} by noticing that
\begin{align*}
I_L &= (1 + o(1))  \Big\{ 
       (\sqrt{P_0} - \sqrt{Q_0} )^2 + (\sqrt{1- P_0} - \sqrt{1-Q_0} )^2 \\
       %
       & \qquad \qquad + \sqrt{(1-P_0)(1-Q_0)} (1+o(1)) \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx  \Big\} \\
  &= (1 + o(1)) I.
\end{align*}

The proof of Proposition~\ref{prop:H_HL_convergence1} contains a number of subparts, which we briefly outline below. Since $p(x)$ and $q(x)$ are easier to handle on the interval $R$, we initially only concern ourselves with comparing 
$$H_R := \int_R (\sqrt{p(x)} - \sqrt{q(x)})^2 dx, \quad \text{and} \quad H^R_L := \sum_{l=1}^L (\sqrt{{P}_l'} - \sqrt{{Q}_l'})^2.$$
We first notice that $\{ \bin_l \} \cap R$ constitute an approximately-uniform binning of $R$; i.e., there exist constants $c_\bin$ and $C_\bin$ such that $\frac{c_\bin}{L} \leq |\bin_l \cap R| \leq \frac{C_\bin}{L}.$ This is reasoned as follows: Since $R$ is an interval, we know that $\bin_l \cap R$ is an interval, as well. The inequality $| \bin_l \cap R^c | \leq \mu\{ R^c \} \leq \frac{1}{2L}$ then implies $\frac{1}{2L} \leq |\bin_l \cap R| \leq \frac{1}{L}.$

In a series of lemmas, we show that the approximately-uniform binning of $R$ leads to several useful bounds on $H^R$ and $H^R_L$. In particular, Lemma~\ref{prop:d_dL_convergence} shows that as long as $L$ grows, we have $d_L \to \frac{1}{4}$, where $d_L := \sum_l Q_l' \left(\frac{1}{2} \frac{\gamma_l'}{Q_l'}\right)$
%\quad \text{and} \quad \int_R q(x) \left(\frac{1}{2}\frac{\gamma(x)}{q(x)}\right)^2 dx \stackrel{(a)}= \frac{1}{4}.$$
and $\gamma_l' = Q_l'-P_l'$.
%and equation $(a)$ follows from the definition of $\alpha$ in Assumption C.3.
In Lemma~\ref{prop:continuous_hellinger_chi_square}, we show that 
$H^R = \frac{\alpha^2}{4} (1+\eta)$,
where $\eta = \Theta(\alpha)$. Similarly, Lemma~\ref{prop:discrete_hellinger_chi_square} establishes that $H^R_L = d_L\alpha^2(1+\eta_L)$. We combine the results of Lemmas~\ref{prop:d_dL_convergence}, \ref{prop:continuous_hellinger_chi_square}, and~\ref{prop:discrete_hellinger_chi_square} in Lemma~\ref{prop:H_HL_convergence_R}, to show that $|H^R-H^R_L| = o(H^R)$.
%Finally, we use Lemma~\ref{prop:H_HL_convergence_R} to complete the proof of Proposition~\ref{prop:H_HL_convergence1}.
The last step is to bound the difference between the sums and integrals over $R$ and the entire real line.
\end{proof}

\section{Appendix for Proposition~\ref{prop:discretization1}}

\subsection{Proof of Proposition~\ref{prop:H_HL_convergence1}}
\label{appendix: d1}
%\begin{proposition}
%\label{prop:H_HL_convergence1}
%Suppose assumptions C1-C5 hold. Then we have that
%\[
%\left| \frac{
%         \sum_l (\sqrt{P_l} - \sqrt{Q_l})^2 }{ \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx} - 1 \right| \rightarrow 0.
%\] 
%\end{proposition}

%\begin{proof}
%Let $H = \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx$.
Let $a_L$ be an $o(1)$ sequence such that $\mu(R^c) \leq a_L H $. We divide the set of bins into three subsets:
\begin{align*}
L_1 &= \{ l\,:\, \bin_l \cap R^c = \emptyset \}, \\
L_2 &= \{ l \,:\, \bin_l \cap R^c \neq \emptyset, \, P_l \vee Q_l \geq 2C a_L H \}, \\
L_3 &= \{ l \,:\, \bin_l \cap R^c \neq \emptyset, \, P_l \vee Q_l \leq 2C a_L H \}.
\end{align*}
For each bin $l$, define $P'_l = \int_{\bin_l \cap R} p(x) dx$ and $P''_l = \int_{\bin_l \cap R^c} p(x) dx$, and likewise define $Q'_l$ and $Q''_l$. We now proceed in two steps:

\textbf{Step 1:} We first claim that for all $l \in L_2$, 
\[
\left| (\sqrt{P_l} - \sqrt{Q_l})^2 - (\sqrt{P'_l} - \sqrt{Q'_l})^2 \right| \leq a_L H.
\]
Since $\mu(R^c) \leq a_L H$, we have $P''_l = \int_{\bin_l \cap R^c} p(x) dx \leq C a_L H$, and likewise for $Q''_l$. Then
\begin{align*}
(\sqrt{P_l} - \sqrt{Q_l})^2 - (\sqrt{P'_l} - \sqrt{Q'_l})^2 &=
   P_l + Q_l - P'_l - Q'_l - 2 \sqrt{P_l Q_l} + 2 \sqrt{P'_l Q'_l} \\
  &\stackrel{(a)}\leq P''_l + Q''_l - 2 \sqrt{P''_l Q''_l} \\
  &\leq P''_l + Q''_l \\
  &\leq 2 C a_L H.
\end{align*}
Here, inequality $(a)$ holds by the following reasoning: By the AM-GM inequality, we have $2 \sqrt{P'_l Q'_l P''_l Q''_l} \leq P'_l Q''_l + P''_l Q'_l$. Thus,
\begin{align*}
P'_l Q'_l + P''_l Q''_l + 2 \sqrt{P'_l Q'_l P''_l Q''_l } \leq (P'_l + P''_l)(Q'_l + Q''_l) = P_l Q_l.
\end{align*}
Taking square roots, we conclude that $\sqrt{P'_l Q'_l} + \sqrt{P''_l Q''_l}   \leq \sqrt{ P_lQ_l}$, 
yielding $(a)$.

On the other hand, we have
\begin{align*}
\sqrt{P_l Q_l} - \sqrt{P'_l Q'_l} 
  &= \frac{ P_l Q_l  - P'_l Q'_l }
          {  \sqrt{P_l Q_l} + \sqrt{P'_l Q'_l} }\\
  &= \frac{ P'_l Q''_l + P''_l Q'_l + P''_l Q''_l} 
          {  \sqrt{ P_l Q_l } + \sqrt{P'_l Q'_l} } \\
  &\leq  \frac{ P'_l Q''_l + P''_l Q'_l + P''_l Q''_l} 
          {  2 \sqrt{P'_l Q'_l} } \\
  &\leq Q''_l \frac{P'_l}{2 \sqrt{P'_l Q'_l}} + P''_l \frac{Q'_l}{2\sqrt{P'_l Q'_l}}
        + Q''_l \frac{P''_l}{2 \sqrt{P'_l Q'_l}}. 
\end{align*}
Note that because $P'_l$ and $Q'_l$ are defined on $R$, we have
\begin{align*}
\left| \frac{P'_l}{Q'_l} \right| =
\left| \int_{\bin_l \cap R} \frac{ p(x)}{Q'_l} dx \right| 
 \leq \int_{\bin_l \cap R} \left| \frac{p(x)}{q(x)} \right| \frac{q(x)}{Q'_l} dx 
 \leq \rho. 
\end{align*}
Thus, $\sqrt{ \frac{P'_l}{Q'_l} } \vee \sqrt{ \frac{Q'_l}{P'_l}} \leq \sqrt{\rho}$. This bounds the terms $ Q''_l \frac{P'_l}{2 \sqrt{P'_l Q'_l}} + P''_l \frac{Q'_l}{2\sqrt{P'_l Q'_l}} \leq \sqrt{\rho} (Q''_l + P''_l)$. 

We still need to bound the last term $\frac{Q''_l P''_l}{2 \sqrt{P'_l Q'_l}}$. Since $l \in L_2$, either $P_l \geq 2 C a_L H$ or $Q_l \geq 2 C a_L H$. Suppose the former inequality holds; the latter case may be handled in an identical manner. Since $P''_l \leq C a_L H$ and $P_l \geq 2C a_L H$, we have $P''_l \leq P'_l$, so
$$\frac{Q''_l P''_l}{2 \sqrt{P'_l Q'_l}} \leq Q''_l \frac{P'_l}{2 \sqrt{P'_l Q'_l}} \leq \sqrt{\rho} Q''_l.$$
Putting everything together, we have
\[
\sqrt{P_l Q_l} - \sqrt{P'_l Q'_l} \leq 2\sqrt{\rho} (Q''_l + P''_l).
\]
Thus, 
\begin{align*}
(\sqrt{P_l} - \sqrt{Q_l})^2 - (\sqrt{P'_l} - \sqrt{Q'_l})^2 &=
   P_l + Q_l - P'_l - Q'_l - 2 \sqrt{P_l Q_l} + 2 \sqrt{P'_l Q'_l} \\
  &\geq P''_l + Q''_l - 4\sqrt{\rho}( Q''_l + P''_l) \\
  &\geq -(4\sqrt{\rho}-1) (P''_l + Q''_l) \\
  &\geq - (4\sqrt{\rho}-1)\cdot 2 C a_L H.
\end{align*}
Combining these two bounds yields
\[
\left| (\sqrt{P_l} - \sqrt{Q_l})^2 - (\sqrt{P'_l} - \sqrt{Q'_l})^2 \right| \leq C_{C, \rho} a_L H,
\]
for an appropriate constant $C_{C, \rho}$. This completes step 1.\\

\textbf{Step 2:} In step 2, we verify that $\{ \bin_l \}_{l \in L_1} \cup \{ \bin_l \cap R \}_{l \in L_2} \cup \{ \bin_l \cap R \}_{l \in L_3}$ constitutes a valid approximately-uniform binning of $R$.  First, since $R$ is an interval, it is easy to see that $\bin_l \cap R$ is also an interval. Second, we have $| \bin_l \cap R^c | \leq \mu\{ R^c \} \leq a_L H$. Since $\frac{1}{H} \leq L$ by assumption, we have $\mu\{ R^c \} \leq \frac{a_L}{L}$, so there exists a constant $C_\bin$ such that $\frac{C_\bin}{L} \leq | \bin_l \cap R| \leq \frac{1}{L}$.\\ 

\textbf{Step 3:}
We now turn to main step of the proof. We may bound $|H-H_L|$ as
\begin{align*}
& \left| \sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2 
        - \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| \\
&= \left| 
   \sum_{l \in L_1}  (\sqrt{P_l} - \sqrt{Q_l})^2 
    + \sum_{l \in L_2}  (\sqrt{P_l} - \sqrt{Q_l})^2  
      + \sum_{l \in L_3}  (\sqrt{P_l} - \sqrt{Q_l})^2 
     - \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| \\
&\stackrel{(a)}\leq \left| 
   \sum_{l \in L_1}  (\sqrt{P_l} - \sqrt{Q_l})^2 
    + \sum_{l \in L_2}  (\sqrt{P_l} - \sqrt{Q_l})^2  
     - \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| + 8 C a_L H\\
&\stackrel{(b)}\leq  \left| 
   \sum_{l \in L_1}  (\sqrt{P_l} - \sqrt{Q_l})^2 
    + \sum_{l \in L_2}  (\sqrt{P'_l} - \sqrt{Q'_l})^2  
     - \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| + C_{C, \rho} a_L H\\
& \stackrel{(c)}\leq  \left| 
   \sum_{l \in L_1}  (\sqrt{P_l} - \sqrt{Q_l})^2 
    + \sum_{l \in L_2}  (\sqrt{P'_l} - \sqrt{Q'_l})^2  
    + \sum_{l \in L_3} (\sqrt{P'_l} - \sqrt{Q'_l})^2 
     - \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| + C_{C, \rho} a_L H \\
&\stackrel{(d)} \leq  \left| 
   \sum_{l \in L_1}  (\sqrt{P_l} - \sqrt{Q_l})^2 
    + \sum_{l \in L_2}  (\sqrt{P'_l} - \sqrt{Q'_l})^2
    + \sum_{l \in L_3} (\sqrt{P'_l} - \sqrt{Q'_l})^2 
     - \int_{R} (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right| 
    + C_{C, \rho} a_L H \\
&\stackrel{(e)} \leq C_{C, \rho} a_L H,
\end{align*}
where $(a)$ follows because $P_l \vee Q_l \leq 2 C a_L H$ for all $l \in L_3$, and $|L_3| \leq 2$; $(b)$ follows from step 1 and the fact that $|L_2| \leq 2$; $(c)$ follows because 
$P'_l \leq P_l$, so $\sum_{l \in L_3} (\sqrt{P'_l} - \sqrt{Q'_l})^2 \leq 2 C a_L H$; $(d)$ follows because 
$\int_{R^c} (\sqrt{p(x)} - \sqrt{q(x)})^2 \leq C \mu\{ R^c \} = C a_L H$; and $(e)$ follows by Lemma~\ref{prop:H_HL_convergence_R}. Since $a_L \rightarrow 0$, the conclusion follows. 

%\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Lemmas for Proposition~\ref{prop:discretization1}}

%We define an \emph{approximately uniform} binning of an interval $R$ to be a partition of $R$ into $L$ bins $[a_l, b_l]$, such that the length of each bin is bounded as $\frac{C_\bin}{L} \leq b_l - a_l \leq \frac{C_\bin}{L}$, for some constants $C_\bin$ and $C_\bin$. We also use the notation $\bin_l$ to denote $[a_l, b_l]$, and let $B_l = b_l - a_l$. 

\begin{lemma}
\label{prop:d_dL_convergence}
Let $d_L = \sum_l Q_l' \left( \frac{\gamma_l'}{Q_l} \right)^2$. Suppose Assumptions C1'--C5' hold. Then $\lim_{L \to \infty} d_L = \frac{1}{4}$.
\end{lemma}

\begin{proof}
Let $h(x)$ be as defined in Assumption C3'; in particular, $|h(x)| \geq 
\left| \frac{\gamma'(x)}{q(x)} \right| \vee \left| \frac{q'(x)}{q(x)} \right|$.  For a parameter $0 < \tau < 1$ to be chosen later, we call $\bin_l$ \emph{good} if
$$
\sup_{x \in \bin_l} |h(x)| \leq L^\tau.
$$
We first argue that the proportion of bad bins converges to 0 as $L \rightarrow \infty$. Since $h(x)$ is $(c'_{s1}, c'_{s2}, C'_s)$-bowl-shaped, the set $\left \{x \,:\, |h(x)|  \geq L^\tau \right \}$ is a union of at most two intervals, for all $L \geq C_s^{\prime 1/\tau}$. Using the notation $B_l = b_l - a_l$, we have
\begin{align*}
\sum_{l \in \left \{ l \,:\, |h(x)| 
           \geq L^\tau \right \}}  B_l \leq 
   \mu \left( \left\{x \,:\, |h(x)|
         \geq L^\tau \right\} \right) + \frac{4 C_\bin}{L} \stackrel{(a)}\leq \frac{C}{L^{\tau t}}  + \frac{4 C_\bin}{L} \stackrel{(b)}\leq \frac{C}{L^{\tau t}},
\end{align*}
where $(a)$ follows because $\int_R |h(x)|^t dx < \infty$ by Assumption C4'; and $(b)$ follows because $t \leq 1$ by Assumption C4', and the fact that $\tau t < 1$ by choice. In particular, the number of bad bins may be bounded as follows:
\begin{align*}
\# \{ l \,:\, |h(x)| \geq L^\tau \} \leq \frac{C L^{- \tau t} L}{C_\bin}  \leq C L^{1 - \tau t},
\end{align*}
where we redefine the constant $C$ suitably. For a bad bin $l$, we may bound $Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2$ as follows:
\begin{align*}
Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 &= Q_l' \left( \frac{1}{Q_l'} \int_{\bin_l} \gamma(x) dx \right)^2 \\
   &= Q_l' \left(  \int_{\bin_l} \frac{\gamma(x)}{q(x)} \frac{q(x)}{Q_l'} dx \right)^2 \\
   &\stackrel{(a)} \leq Q_l' \int_{\bin_l} \frac{q(x)}{Q_l'} \left( \frac{\gamma(x)}{q(x)} \right)^2 dx\\
   &\leq \int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx \\
   &\stackrel{(b)}\leq \left(\int_{\bin_l} q(x) \left| \frac{\gamma(x)}{q(x)} \right|^r dx \right)^{2/r}
         \left(\int_{\bin_l} q(x) dx \right)^{(r-2)/r} \\
   &\stackrel{(c)} \leq C (C_\bin)^{(r-2)/r} L^{-(r-2)/r} = C L^{-(r-2)/r}.\\
\end{align*}
Here, $(a)$ follows from Jensen's inequality, $(b)$ follows from H\"{o}lder's inequality, and $(c)$ follows because $\int_R q(x) \left| \frac{\gamma(x)}{q(x)} \right|^r dx < \infty$ by Assumption C3' and the fact that $\int_{\bin_l} q(x) dx \leq \frac{C C_{bin}}{L}$.  

We now have
\begin{align*}
d_L &= \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 = \sum_{l \, good} Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 + 
      \sum_{l \, bad} Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 \\
   &\leq   \sum_{l \, good} Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 + 
       C L^{-(r-2)/r} | \{ l \,:\, l \trm{ bad} \}| \\
   &\leq  \sum_{l \, good} Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 + 
          C L^{1 - \tau t - \frac{(r-2)}{r}} =  \sum_{l \, good} Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 + 
        C L^{\frac{2}{r} - \tau t}. 
\end{align*}
For each good bin $l$, define $x_l = \argmax_{x \in \bin_l} | q(x) |$. The maximum is attainable since $q$ is continuous and bounded. Furthermore,
\begin{align*}
Q_l' &= \int_{\bin_l} q(x) dx = \int_{a_l}^{b_l} q(x) dx \\
 &= \int_{a_l}^{b_l} q(x_l) + q'(c_x) (x - x_l) dx \quad \trm{(for some $c_x \in [a_l, b_l]$)}\\
 &= B_l q(x_l) + \int_{a_l}^{b_l} q'(c_x)(x-x_l) dx = B_l q(x_l) + B_l^2 \xi_l,
\end{align*}
where we define $\xi_l := \frac{1}{B_l^2} \int_{a_l}^{b_l} q'(c_x) (x-x_l) dx$. We also have
\begin{align*}
B_l \left| \frac{\xi_l}{q(x_l)} \right| 
    & \leq \frac{1}{B_l} \int_{a_l}^{b_l} \left|\frac{q'(c_x)}{q(x_l)} \right| |x - x_l| dx 
   \stackrel{(a)}{\leq} \frac{1}{B_l} \int_{a_l}^{b_l} \left|\frac{q'(c_x)}{q(c_x)} \right| |x - x_l| dx \\
%
& \stackrel{(b)}{\leq} \frac{1}{B_l} \int_{a_l}^{b_l} L^{\tau} |x - x_l| dx \leq 
 C_\bin L^{\tau - 1}, 
\end{align*} 
where $(a)$ follows because $q(c_x) \leq q(x_l)$, and $(b)$ follows because $l$ is a good bin, so
$\left| \frac{q'(c_x)}{q(c_x)} \right| \leq L^\tau$. The last inequality follows because $B_l \leq \frac{C_{\bin_l}}{L}$. We may perform a similar analysis on $\gamma$:
\begin{align*}
\gamma_l' &= \int_{\bin_l} \gamma(x) dx =  \int_{a_l}^{b_l} \gamma(x_l) + \gamma'(c_x) (x - x_l) dx = B_l \gamma(x_l) + B_l^2 \xi'_l,
\end{align*}  
where $\xi'_l := \frac{1}{B_l^2} \int_{a_l}^{b_l} \gamma'(c_x)(x - x_l) dx$. It is straightforward to verify that $ B_l \left| \frac{\xi'_l}{q(x_l)}\right| \leq \frac{1}{2} C_\bin L^{\tau - 1}$. For any bin $l$, we also have
\begin{align*}
Q_l' = \int_{\bin_l} q(x) dx \leq C B_l,
\end{align*}
where $C$ is the bound on $p(x) \vee q(x)$. Now we look at a single $Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2$ term for a good bin $l$:
\begin{align*}
Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 &= \frac{\gamma_l'^2}{Q_l'} = \frac{ (B_l \gamma(x_l) + B_l^2 \xi'_l)^2}{B_l q(x_l) + B_l^2 \xi_l} \\
   &= B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} + B_l \frac{\xi'_l}{q(x_l)} \right)^2 
         \left( \frac{1}{1 + B_l \frac{\xi_l}{q(x_l)} } \right) \\
   &=  B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} + B_l \frac{\xi'_l}{q(x_l)} \right)^2 
         \left( 1 - B_l \frac{\xi_l}{q(x_l)} + \eta_l ( B_l \frac{\xi_l}{q(x_l)} )^2 \right).
\end{align*}
To arrive at the last equality, we assume that $L \geq  C_\bin^{1/(1-\tau)}$. Then $\left| B_l \frac{\xi'_l}{q(x_l)} \right| \leq \frac{1}{2}$, so we may take a Taylor approximation. Here, $\eta_l$ is a constant satisfying $|\eta_l| \leq 16$.
Expanding the right-hand side, we have
\begin{multline*}
Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 = \left( B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 + 
          2B_l q(x_l) \frac{\gamma(x_l)}{q(x_l)} B_l \frac{\xi'_l}{q(x_l)} +
          B_l q(x_l) \left( B_l \frac{\xi'_l}{q(x_l)} \right)^2 \right) \\
%
\cdot \left( 1 - B_l \frac{\xi_l}{q(x_l)} + \eta_l ( B_l \frac{\xi_l}{q(x_l)} )^2 \right).
\end{multline*}
Again, note that $\left| B_l \frac{\xi'_l}{q(x_l)} \right| \leq \frac{C_\bin}{2} L^{\tau-1}$ and 
$\left| B_l \frac{\xi_l}{q(x_l)} \right| \leq \frac{C_\bin}{2} L^{\tau-1}$. Suppose $L \geq (2C_\bin)^{1/(1-\tau)}$, so $\frac{C_\bin}{2} L^{\tau - 1} \leq \frac{1}{4}$. Then
\begin{align*}
 \left| B_l \frac{\xi_l}{q(x_l)} \right|  + \left| \eta_l ( B_l \frac{\xi_l}{q(x_l)} )^2 \right| &\leq C_\bin L^{\tau - 1}, \quad \text{and} \quad \left| 1 - B_l \frac{\xi_l}{q(x_l)} + \eta_l ( B_l \frac{\xi_l}{q(x_l)} )^2 \right| \leq 2.
\end{align*}
We now bound
\begin{multline*}
\left| Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 
         - B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| \\
%
\leq B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 C_\bin L^{\tau - 1} +
       2 B_l q(x_l) \frac{\gamma(x_l)}{q(x_l)}C_\bin  L^{\tau - 1} +
        B_l q(x_l) C_\bin^2 L^{2(\tau-1)}. 
\end{multline*}
The third term is bounded by $C_1 L^{2\tau - 3}$, for a suitable constant $C_1$. To bound the second term, we split into two cases:

\textbf{Case 1}: $\left|\frac{\gamma(x_l)}{q(x_l)}\right| \geq 1$. Then 
$q(x) \left| \frac{\gamma(x_l)}{q(x_l)} \right| 
  \leq q(x) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 $.

\textbf{Case 2:} $\left| \frac{\gamma(x_l)}{q(x_l)} \right| \leq 1$. The second term is bounded by $2 B_l CC_\bin L^{\tau - 1 } \leq C_2 L^{\tau - 2}$, for some constant $C_2$.

In either case, we have
\begin{align*}
& \left| Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 
         - B q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| 
  \leq C_3 B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2  L^{\tau - 1} +
        C_4 L^{\tau - 2}.
\end{align*}
Define $d_R = \sum_{l \, good} B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2$. Then
\begin{align*}
  |d_L - d_R| &= \left| \sum_l Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 - \sum_{l \, good} B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| \\
  &\leq \sum_{l \, good} \left| Q_l' \left( \frac{\gamma_l'}{Q_l'} \right)^2 - B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| + C_{M, M', K, C} L^{\frac{2}{r} - \tau t} \\
  &\leq C_3 d_R L^{\tau - 1}
       + L \cdot C_4 L^{\tau - 2}  + C_5 L^{\frac{2}{r} - \tau t} \\
  &\leq  C_3 d_R L^{\tau - 1} 
       +   C_4 L^{\tau - 1}  + C_{5} L^{\frac{2}{r} - \tau t} \\
 &\leq C_3 d_R L^{\frac{2 - rt}{r(1+t)}} 
       +   C_{6} L^{\frac{2 - rt}{r(1+t)} },
\end{align*}
where we have made the choice $\tau= \frac{2 + r}{r(1+t)}$ in the last inequality to balance $L^{\tau - 1}$ and $L^{2/r - \tau t}$. Notice that $0 < \tau < 1$ by Assumption C4', since $rt > 2$. Furthermore, $|d_L - d_R| = o(d_R) + o(1)$.

In a similar manner, we bound $|d_R - d|$. We use the same definition of good and bad bins as before, and obtain 
\begin{align*}
d &= \int_R q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx = \sum_{l=1}^L \int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx \\
  &= \sum_{l \, good} \int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx +
     \sum_{l \, bad} \int_{\bin_l}  q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx \\
  &\leq \sum_{l\, good} \int_{\bin_l}  q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx +
        |\{ l \,:\, l \, \trm{bad}\}| C L^{-\frac{2}{r}} \\
 & \leq \sum_{l\, good} \int_{\bin_l}  q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx +
        C L^{\frac{2}{r} - \tau t}. 
\end{align*}
The bound on the second term follows from the previous analysis. For the first term, note that for all $x \in \bin_l$, we have
\begin{align*}
q(x) = q(x_l) + q'(c_l)(x-x_l), \quad \text{and} \quad
\gamma(x) = \gamma(x_l) + \gamma'(c'_l)(x - x_l),
\end{align*}
where $c_l, c'_l \in \bin_l$ depend implicitly on $x$. For $\bin_l$, we have
\begin{align*}
\int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 &=
  \int_{\bin_l} 
      \frac{ (\gamma(x_l) + \gamma'(c'_l)(x - x_l))^2  }{ q(x_l) + q'(c_l)(x-x_l)} dx
  \\
&= \int_{\bin_l} q(x_l)
   \left( \frac{\gamma(x_l)}{q(x_l)} + \frac{\gamma'(c'_l)}{q(x_l)} (x - x_l) \right)^2
  \left( \frac{1}{1 + \frac{q'(c_l)}{q(x_l)} (x - x_l)} \right) dx.
\end{align*}
Denote
\[
T_1 = \frac{q'(c_l)}{q(x_l)} ( x - x_l), \quad \text{and} \quad T_2 = \frac{\gamma'(c'_l)}{q(x_l)} ( x - x_l).
\]
Observe that $|x -x_l| \leq B_l$ and
\[
\left| \frac{\gamma'(c'_l)}{q(x_l)} \right| 
    \leq \left| \frac{\gamma'(c'_l)}{q(c'_l)} \right| \leq L^\tau.
\]
Similarly, $\left| \frac{q'(c_l)}{q(x_l)} \right| \leq L^\tau$. Hence, $|T_1|, |T_2| \leq C_\bin L^{\tau -1}$. Now suppose $C_\bin L^{\tau -1} \leq \frac{1}{2} $, which is satisfied if $L \geq (2 C_\bin)^{\frac{1}{1-\tau}}$. We obtain
\begin{align*}
\int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx &= \int_{\bin_l} q(x_l) 
       \left( \frac{\gamma(x_l)}{q(x_l)} + T_2 \right)^2 
        \left( \frac{1}{ 1 + T_1 } \right) dx  \\
 =& \int_{\bin_l} \left( q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 + 
                q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right) T_2 + q(x_l) T_2^2 \right) 
    (1 - T_1 + \eta T_1^2) dx,
\end{align*}
where $\eta$ is some function of $x$ satisfying $|\eta| \leq 16$. Thus,
\begin{multline*}
\left| \int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx - 
      B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| \\
%
\leq   B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 C_\bin L^{\tau-1} +
      B_l q(x_l) \frac{\gamma(x_l)}{q(x_l)} C_\bin L^{\tau-1} + B_l q(x_l) C_\bin^2 L^{2(\tau-1)}. 
\end{multline*}
The same analysis used to bound $|d_L-d_R|$ implies that
$|d - d_R| = o(d_R) + o(1)$.
Since $d = \frac{1}{4}$, we have $d_R \to \frac{1}{4}$, which in turn implies $d_L \to \frac{1}{4}$. This completes the proof.
\end{proof}
%Since $q(x_l) \leq C$, the $q(x_l) L^{2(\tau-1)}$ term is bounded by $C L^{2(\tau - 1)}$. To bound the $q(x_l) \frac{\gamma(x_l)}{q(x_l)} L^{\tau-1} $ term, we perform case analysis, exactly as before:

%Case 1: if $\frac{\gamma(x_l)}{q(x_l)} \geq 1$. Then $q(x_l) \frac{\gamma(x_l)}{q(x_l)} L^{\tau-1} \leq q(x_l) \left(\frac{\gamma(x_l)}{q(x_l)} \right)^2 L^{\tau-1}$. 

%Case 2: if $\frac{\gamma(x_l)}{q(x_l)} \leq 1$. Then $q(x_l) \frac{\gamma(x_l)}{q(x_l)} L^{\tau-1} \leq C L^{\tau-1}$.\\ 

%Thus, in any case, we have that, for a single good bin:
%\begin{align*}
%& \left| \int_{\bin_l} q(x) \left( \frac{\gamma(x)}{q(x)} \right)^2 dx - 
%      B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 \right| \\
%& \leq   B_l q(x_l) \left( \frac{\gamma(x_l)}{q(x_l)} \right)^2 L^{\tau-1} +
%      B_l C L^{\tau-1} 
%\end{align*}

%\[
%|d - d_R | \leq d_R L^{\tau-1} + C L^{\tau-1} + C_{M, M', C, K} L^{\frac{2}{r} - \frac{\tau}{t}}
%\]
%%%%%%%%%%%%%%%%%%

\begin{lemma}
\label{prop:continuous_hellinger_chi_square}
Let
\begin{align*}
H^R & = \int_R (\sqrt{p(x)} - \sqrt{q(x)})^2 dx, \\
%
\delta(x) & = q(x) - p(x), \\
%
\alpha^2 & = \int_R q(x) \left( \frac{\delta(x)}{q(x)} \right)^2 dx, \\
%
\gamma(x) & = \frac{\delta(x)}{\alpha}.
\end{align*}
Suppose Assumptions C1'--C5' hold. Then $H^R = \frac{\alpha^2}{4}(1 + \eta)$,
where $|\eta| \leq C(\alpha + \alpha^2)$ for some constant $C$. In particular, if $H^R \rightarrow 0$, then $\alpha \rightarrow 0$ and $\eta \rightarrow 0$. 
\end{lemma}

\begin{proof}
We write
\begin{align*}
H^R = &\int_R (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = \int_R ( \sqrt{q(x)} - \sqrt{q(x) - \delta(x)} )^2 dx = \int_R q(x) \left( 1 - \sqrt{ 1 - \frac{\delta(x)}{q(x)}} \right)^2 dx. 
\end{align*}
By convention, let $\frac{\delta(x)}{q(x)} = 0$ whenever $q(x) = p(x) = 0$. Thus, we may define $\xi(x) = 1- \frac{1}{2} \frac{\delta(x)}{q(x)} - \sqrt{ 1 - \frac{\delta(x)}{q(x)}}$ for $x \in [0,1]$ and rewrite
\begin{align*}
H^R =& \int_R q(x) \left( 1 - (1 - \frac{1}{2} \frac{\delta(x)}{q(x)} + \xi(x) ) \right)^2 dx \\
=& \int_R q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} + \xi(x) \right)^2 dx\\ 
=& \int_R q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} \right)^2 \left( 1 + \xi_2(x) \right)^2 dx, 
\end{align*}
where $\xi_2(x) = \frac{2\xi(x)}{\delta(x)/q(x)}$ if $\delta(x) \neq 0$, and $\xi_2(x) =0$ if $\delta(x) = 0$. Thus,
\begin{align*}
\int_R \left( \sqrt{p(x)} - \sqrt{q(x)} \right)^2 dx &= (1 + \eta)\frac{\alpha^2}{4},
\end{align*}
where 
\[
\eta = \frac{\int_R q(x) \left( \frac{1}{2} \frac{\delta(x)}{q(x)} \right)^2 (\xi_2(x)^2 + 2\xi_2(x)) dx }
           { \alpha^2/4} =  \int_R q(x) \left(  \frac{\gamma(x)}{q(x)} \right)^2 (\xi_2(x)^2 + 2\xi_2(x)) dx.
\]
By Lemma~\ref{lem:sqrt_linearize}, we have $\xi_2(x) \leq 2 \left| \frac{ \delta(x)}{q(x)} \right|$, implying that
\begin{align*}
| \eta | &\leq  \int_R q(x) \left(  \frac{\gamma(x)}{q(x)} \right)^2   \left( 4\left|\frac{\delta(x)}{q(x)} \right|^2 + 4\left| \frac{\delta(x)}{q(x)} \right| \right) dx\\
%
&= 4\alpha^2 \int_R q(x) \left|  \frac{\gamma(x)}{q(x)} \right |^4 dx + 4\alpha \int_R q(x) \left |  \frac{\gamma(x)}{q(x)} \right|^3 dx\\
%
    &\leq C (\alpha^2 + \alpha),
\end{align*}
using the finiteness of integrals in Assumption C3'.
\end{proof}


%%%%%%%%%%%%%%%%%


\begin{lemma}
\label{prop:discrete_hellinger_chi_square}
Let
\begin{align*}
H^R_L & = \sum_{l=1}^L \left( \sqrt{P_l'} - \sqrt{Q_l'} \right)^2, \\
%
\delta(x) & = q(x) - p(x), \\
%
\alpha^2 & =  \int_R q(x) \left( \frac{\delta(x)}{q(x)} \right)^2 dx, \\
%
\gamma(x) & = \frac{\delta(x)}{\alpha} dx.
\end{align*}
Suppose that Assumptions C1'--C5' hold. Then $H^R_L = d_L ( 1 + \eta_L )$,
where $d_L = \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 dx$, $\gamma_l' = \frac{Q_l'-P_l'}{\alpha}$ and $\sup_L |\eta_L | \leq C(\alpha + \alpha^2)$, for some constant $C$.%In particular, we have that if $\alpha \rightarrow 0$, then $\eta_L \rightarrow 0$. 
\end{lemma}

\begin{proof}
Let $\delta_l = Q_l'-P_l'$. We have
\begin{align*}
H^R_L &= \sum_{l=1}^L (\sqrt{P_l'} - \sqrt{Q_l'})^2 = \sum_{l=1}^L Q_l' \left( 1 - \sqrt{\frac{P_l'}{Q_l'}} \right)^2 \\
 &= \sum_{l=1}^L Q_l' \left( 1 - \sqrt{1 - \frac{\delta_l}{Q_l'}} \right)^2 = \sum_{l=1}^L Q_l' \left( 1 - \left( 1 - \frac{1}{2} \frac{\delta_l}{Q_l'} - \xi_l \right) \right)^2, 
\end{align*}
where by convention, we define $\frac{\delta_l}{Q_l'} = 0$ when $Q_l', P_l' = 0$, and we use the shorthand $\xi_l = 1 - \frac{1}{2} \frac{\delta_l}{Q_l'} - \sqrt{ 1 - \frac{\delta_l}{Q_l'} }$. Hence,
\begin{align*}
H^R_L &=  \sum_{l=1}^L Q_l' \left(  \frac{1}{2} \frac{\delta_l}{Q_l'} + \xi_l \right)^2 =  \sum_{l=1}^L Q_l' \left(  \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2 \left( 1 + \xi_{2l} \right)^2, 
\end{align*}
where $\xi_{2l} = 0$ if $\frac{\delta_l}{Q_l'} = 0$, and $\xi_{2l} = 2 \xi_l \frac{Q_l'}{\delta_l}$ otherwise. Then
\begin{align*}
H^R_L &= (1 + \eta_L) \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2,
\end{align*}
where $\eta_L = \frac{ \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2 (2 \xi_{2l} + \xi_{2l}^2) }{ \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2 }$. By Lemma~\ref{lem:sqrt_linearize}, we have $|\xi_{2l}| \leq 2 \left| \frac{\delta_l}{Q_l'} \right|$. Therefore,
\begin{align*}
|\eta_L| &= \left| \frac{ \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2 (2 \xi_{2l} - \xi_{2l}^2) }
          { \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\delta_l}{Q_l'} \right)^2 } \right| \leq  \frac{ \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 (2 |\xi_{2l}| + \xi_{2l}^2) }
          { \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 } \\
  &\leq   4 \frac{ \alpha \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^3 +  
                   \alpha^2 \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^4 }
          { \sum_{l=1}^L Q_l' \left( \frac{1}{2} \frac{\gamma_l'}{Q_l'} \right)^2 }.
\end{align*}
The denominator tends to $\frac{1}{4}$ by Lemma~\ref{prop:d_dL_convergence} and may be bounded by $1/(2C')$ for large enough $L$. To bound the numerator, note that for a single $l$, we have
\[
\int_{a_l}^{b_l} \frac{q(x)}{Q_l'} \left| \frac{\gamma(x)}{q(x)} \right|^3 dx \geq
 \left| \int_{\bin_l} \frac{q(x)}{Q_l'} \frac{\gamma(x)}{q(x)} dx \right|^3 =
 \left| \frac{\gamma_l'}{Q_l'} \right|^3.
\]
Therefore,
\begin{align*}
\sum_{l=1}^L Q_l' \left| \frac{\gamma_l'}{Q_l'} \right|^3 &\leq \int_R q(x) \left| \frac{\gamma(x)}{q(x)} \right|^3 \leq M,  \text{ and }\\
\sum_{l=1}^L Q_l' \left| \frac{\gamma_l'}{Q_l'} \right|^4 &\leq \int_R q(x) \left| \frac{\gamma(x)}{q(x)} \right|^4 \leq M,
\end{align*}
implying that $|\eta_L| \leq (2 \alpha + \alpha^2) 2 C' M$.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%



\begin{lemma}
\label{prop:H_HL_convergence_R}
Suppose Assumptions A1'--A4' hold. For any sequences $L_n, \alpha_n \rightarrow \infty$, we have $H^R_L = H^R(1+o(1))$; i.e.,
\[
\left| \frac{\sum_l (\sqrt{P_l'} - \sqrt{Q_l})^2}{\int_R (\sqrt{p(x)} - \sqrt{q(x)})^2 dx} 
        - 1 \right| \rightarrow 0.
\]

\end{lemma}

\begin{proof}
By Lemmas~\ref{prop:continuous_hellinger_chi_square} and~\ref{prop:discrete_hellinger_chi_square}, we have
\begin{align*}
| H^R_L - H^R | & = \left| d_L \alpha^2 ( 1 + \eta_L) -  \frac{\alpha^2}{4} ( 1 + \eta) \right|,
 \end{align*}
implying that
\begin{align*}
\left| \frac{H^R_L}{H^R} - 1 \right| &= \left |4d_L \frac{(1+\eta_L)}{1 + \eta} - 1 \right|,
\end{align*}
where $|\eta|, |\eta_L| \leq C_1(\alpha + \alpha^2)$ for all $L$. Thus,
\[
\lim_{\alpha_n \rightarrow 0} \sup_L \left| \frac{1+\eta_L}{1+\eta} - 1 \right| = 0.
\]
Furthermore, by Lemma~\ref{prop:d_dL_convergence}, we have $|4d_L - 1| \rightarrow 0$,
uniformly for all $\alpha$. Thus, $\left| \frac{H^R_L}{H^R} - 1 \right| \to 0$, completing the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%










\section{Proof of Proposition~\ref{prop:discretization2}}

%\begin{repproposition}{prop:discretization2}
%Suppose $p(x), q(x)$ are supported on $[0,1]$.

%\begin{enumerate}
%\item[C1']  $p(z), q(z)$ are continuous densities on $[0,1]$ and that $0 < p(z), q(z) \leq C$ on $(0, 1)$.
%\item[C2'] For some $r > 2$, $sup_n \int \left| \log \frac{p(z)}{q(z)} \right| dz < \infty$.
%\item[C3']  There is some $h_{n}(z)$ such that (1) $h_{n}(z) \geq \max \left\{  \left|\frac{p'(z)}{p(z)} \right|, 
% \left|\frac{q'(z)}{q(z)}\right|  \right\} $, (2) $h_{n}(z)$ is $(c_{s1}', c_{s2}', C_s')$-bowl-shaped, and (3) $\sup_n \int |h_{n}(z)|^t dz < \infty$ for some constant $t$ such that  $1 \geq t \geq 2/r$. 
%\item[C4']  $p'(z), q'(z) \geq 0$ and for all $z < c'_{s1}$ and $p'(z), q'(z) \leq 0$ for all $z > c'_{s2}$.
%\end{enumerate}

%Let $L$ be a sequence such that $L \rightarrow \infty$. Suppose $\frac{1}{c_0} \leq \frac{1 - P_0}{1-Q_0} \leq c_0$. Let $\bin_l = [a_l, b_l]$ for $l=1,...,L$ be a uniformly spaced binning of the interval $[0,1]$ and let $\tilde P_l = (1- P_0) \int_{a_l}^{b_l} p(x) dx$ and $\tilde Q_l = (1-Q_0)\int_{a_l}^{b_l} q(x) dx$. Define $I = -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0) p(x) q(x)} dx \right)$ and $I_L = -2 \log \left( \sqrt{P_0 Q_0} + \sum_{l=1}^L \sqrt{\tilde P_l \tilde Q_l} \right)$. Then, we have that
% $$\left| \frac{I - I_L}{I} \right| = o(1),$$ 
%and that $ \exp(-C L^{1/r}) \leq \frac{\tilde P_l}{\tilde Q_l} \leq \exp(C L^{1/r})$ for all $l$. 
%\end{repproposition}

%\begin{proof}

First, we prove the bounds on $\frac{\tilde P_l}{\tilde Q_l}$. Define 
$$R = \left \{ x \in [0,1] \,:\, \left| \log \frac{p(x)}{q(x)} \right| \leq C (2L)^{1/r} \right \},$$
where $C = \left( \int \left| \log \frac{p(x)}{q(x)} \right|^r dx \right)^{1/r}$ is a constant. Since $\int \left| \log \frac{p(x)}{q(x)} \right|^r dx < \infty$, Markov's Inequality implies $\mu\{ R^c \} \leq \frac{1}{2L}$.

%The rest of the proof proceeds exactly as in Proposition~\ref{prop:discretization2}, so we omit the details here.

The remainder of the proof follows the argument used to prove Proposition~\ref{prop:discretization1}, except for the final step, where we need to show that
$$\left| \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx - \sum_l (\sqrt{P_l} - \sqrt{Q_l})^2 \right| = o(\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx) = o(1).$$
We establish this fact in the following proposition:

%Proposition~\ref{prop:H_HL_convergence2} in Appendix~\ref{appendix: another saturday}.
%\end{proof}


%\subsection{Appendix for Proposition~\ref{prop:discretization2}} \label{appendix: another saturday}
\begin{proposition}
\label{prop:H_HL_convergence2}
Let Assumptions C1 and C3 be satisfied. Let $\bin_l = [a_l, b_l]$ be a uniform binning of $[0,1]$, for $l=1, \dots, L$, and let $P_l = \int_{\bin_l} p(x) dx$ and $Q_l = \int_{\bin_l} q(x) dx$. Then
\[
\left| \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx - \sum_l (\sqrt{P_l} - \sqrt{Q_l})^2 \right| \rightarrow 0.
\]
\end{proposition}

\begin{proof}
We use a similar argument to the proof of Proposition~\ref{prop:d_dL_convergence}. First observe that
\begin{equation*}
\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = \int p(x) dx + \int q(x) dx - 2 \int \sqrt{p(x)q(x)} dx = 2 - 2 \int \sqrt{p(x)q(x)}
\end{equation*}
and
\begin{equation*}
\sum_l (\sqrt{P_l} - \sqrt{Q_l})^2 = \sum_l P_l + \sum_l Q_l - 2 \sum_l \sqrt{P_l Q_l}.
\end{equation*}
Thus, we only need to show that 
\[
\left| \int \sqrt{p(x)q(x)} dx - \sum_l \sqrt{P_l Q_l} \right| \rightarrow 0.
\]
We have $|h(x)| \geq \left| \frac{p'(x)}{p(x)} \right| \vee \left| \frac{q'(x)}{q(x)} \right|$. Let $0 < \tau < 1$. We call $\bin_l$ \emph{good} if
$$
\sup_{x \in \bin_l} |h(x)| \leq L^\tau.
$$
We now argue that the proportion of bad bins converges to 0 as $L \rightarrow \infty$: Since $h(x)$ is $(c'_{s1}, c'_{s2}, C'_s)$-bowl-shaped, the set $\left \{x \,:\, |h_n(x)|  \geq L^\tau \right \}$ is a union of at most two intervals, for all $L \geq C_s^{\prime 1/\tau}$. Hence,
\begin{align*}
\sum_{l \in \left \{ l \,:\, \sup_{x \in \bin_l} |h(x)| 
           \geq L^\tau \right \}}  B_l &\leq 
   \mu \left( \left\{x \,:\, \sup_{x \in \bin_l} |h(x)|
         \geq L^\tau \right\} \right) + 4 C_\bin L^{-1} \\
  &\stackrel{(a)} \leq C L^{-\tau t}  + 4 C_\bin L^{-1} \stackrel{(b)}{\leq} C L^{ - \tau t},
\end{align*}
where $(a)$ follows because $\int_R |h(x)|^t dx < \infty$ by Assumption C3; and $(b)$ follows because $t \leq 1$, so $\tau t < 1$ and the first term dominates. We now bound the number of bad bins: 
\begin{align*}
\# \{ l \,:\, |h(x)| \geq L^\tau \} \leq \frac{C L^{- \tau t} L}{C_\bin}  \leq C L^{1 - \tau t}.
\end{align*}
For a bad bin, we have $P_l, Q_l \leq \frac{C C_\bin}{L}$ and $\int_{\bin_l} (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \leq \frac{2CC_\bin}{L}$.

We now consider a good bin $l$. Let $x_l$ be $\argmax_{x \in \bin_l} p(x)$. The argmax is attainable since $p$ is continuous and bounded. We have
\begin{align*}
P_l &= \int_{a_l}^{b_l} p(x) dx = \int_{a_l}^{b_l} p(x_l) + p'(c_x)(x-x_l) dx = B_l p(x_l) + B_l^2 \xi_l,
\end{align*}
where $\xi_l = \frac{1}{B_l^2} \int_{a_l}^{b_l} p'(c_x)(x - x_l)dx$. Furthermore,
\begin{align*}
B_l \left| \frac{\xi_l}{p(x_l)} \right| 
   & \leq \frac{1}{B_l} \int_{a_l}^{b_l} \left|\frac{p'(c_x)}{p(x_l)} \right| |x - x_l| dx 
   \leq \frac{1}{B_l} \int_{a_l}^{b_l} \left|\frac{p'(c_x)}{p(c_x)} \right| |x - x_l| dx \\
   %
   & \leq \frac{1}{B_l} \int_{a_l}^{b_l} L^{\tau} |x - x_l| dx \leq 
  C_\bin L^{\tau - 1}. 
\end{align*}
Likewise, define $x'_l = \argmax_{x \in \bin_l} q(x)$. We have $Q_l = B_l q(x'_l) + B_l^2 \xi'_l$,
where
\begin{equation*}
\xi'_l := \frac{1}{B_l} \int_{a_l}^{b_l} q'(c_x) (x - x'_l) dx.
\end{equation*}
We can also bound $B_l \left| \frac{\xi'_l}{q(x'_l)} \right| \leq C_{\bin} L^{\tau - 1}$.
Thus,
\begin{align*}
 \sqrt{P_l Q_l} &= \sqrt{ (B_l p(x_l) + B_l^2 \xi_l) 
                           (B_l q(x'_l) + B_l^2 \xi'_l) } \\
    &= \sqrt{p(x_l) q(x'_l)} \sqrt{ (B_l + B_l^2 \frac{\xi_l}{p(x_l)} ) 
                                           (B_l + B_l^2 \frac{\xi'_l}{q(x'_l)} ) } \\
   &=  \sqrt{p(x_l) q(x'_l)} B_l \sqrt{ (1 + B_l \frac{\xi_l}{p(x_l)} ) 
                                           (1 + B_l \frac{\xi'_l}{q(x'_l)} ) }.
\end{align*}
By our bounds on $ B_l \frac{\xi_l}{p(x_l)} $ and $B_l  \frac{\xi'_l}{q(x'_l)}$, we can bound the nuisance term as
\begin{align*}
 \sqrt{ (1 + B_l \frac{\xi_l}{p(x_l)} ) 
         (1 + B_l \frac{\xi'_l}{q(x'_l)} ) } &\leq \sqrt{ 1 + C_\bin L^{\tau - 1} (1+o(1))} \leq 1 + \frac{1}{2}  L^{\tau - 1} (1 + o(1)).
\end{align*}
It is clear that $B_l \sqrt{p(x_l)q(x'_l)} \leq B_l C$. Therefore,
\begin{align}
\label{eqn:discrete_riemann_bound2}
\left| \sqrt{ P_l Q_l} -  \sqrt{ p(x_l) q(x'_l)} B_l \right| \leq 
     B_l C  L^{\tau - 1}(1 + o(1)),
\end{align}
and likewise,
\begin{align*}
& \int_{a_l}^{b_l} \sqrt{p(x) q(x)}dx = \int_{a_l}^{b_l} \sqrt{p(x) q(x)} dx \\
                      & \qquad = \int_{a_l}^{b_l} \sqrt{ (p(x_l) + p'(c_x)(x - x_l))
                                      (q(x'_l) + q'(c'_x)(x - x'_l)) } dx \\
    & \qquad =\int_{a_l}^{b_l} \sqrt{ p(x_l) q(x'_l)} \left(
              \sqrt{ 1+ (x - x_l) \frac{p'(c_x)}{p(x_l)} + (x - x'_l) \frac{q'(c'_x)}{q(x'_l)} 
                     + (x - x_l)(x - x'_l) \frac{p'(c_x)}{p(x_l)} \frac{q'(c'_x)}{q(x'_l)} } \right) dx. 
\end{align*}
Since
\begin{align*}
\left| (x - x_l) \frac{p'(c_x)}{p(x_l)} \right| & \leq B_l \left| \frac{ p'(c_x)}{p(c_x)} \right| \leq L^{\tau - 1}, \\
%
\left| (x - x_l) \frac{q'(c'_x)}{q(x_l)} \right| & \leq B_l \left| \frac{ q'(c'_x)}{q(c'_x)} \right| \leq L^{\tau - 1},
\end{align*}
we may bound the nuisance term as follows:
\begin{align*}
 \sqrt{ 1+ (x - x_l) \frac{p'(c_x)}{p(x_l)} + (x - x'_l) \frac{q'(c'_x)}{q(x'_l)} 
                     + (x - x_l)(x - x'_l) \frac{p'(c_x)}{p(x_l)} \frac{q'(c'_x)}{q(x'_l)} } 
    &\leq \sqrt{ 1 + C_\bin L^{\tau - 1} (1 + o(1)) } \\
    &\leq 1 + \frac{1}{2} C_\bin L^{\tau - 1} (1 + o(1)). 
\end{align*}
The term $B_l \sqrt{p(x_l)q(x'_l)}$ is bounded by $B_l C$. Hence,
\begin{align}
\label{eqn:continuous_riemann_bound2}
\left| \int_{a_l}^{b_l} \sqrt{p(x) q(x)}dx - B_l \sqrt{p(x_l) q(x'_l)} \right| &\leq B_l C C_\bin L^{\tau - 1}
\end{align}
By combining inequalities~\eqref{eqn:discrete_riemann_bound2} and \eqref{eqn:continuous_riemann_bound2}, we have
\[
\left| \sqrt{P_l Q_l} - \int_{a_l}^{b_l} \sqrt{p(x) q(x)} dx \right| \leq B_l C C_\bin L^{\tau - 1}.
\]
Hence,
\begin{align*}
\left| \sum_l \sqrt{P_l Q_l} - \int \sqrt{p(x) q(x)} dx \right| &\leq
   \sum_{l \,:\, \trm{ $l$ bad}} B_l C + 
   \sum_{l \,:\, \trm{ $l$ good}} \left| \sqrt{P_l Q_l} - \int_{a_l}^{b_l} \sqrt{ p(x) q(x)} dx \right| \\
  & \leq C L^{ - \tau t} + \sum_{l \,:\, \trm{ $l$ good}} B_l C C_\bin L^{\tau - 1} \\
  & \leq  C L^{ - \tau t} + C C_\bin L^{\tau - 1}.
\end{align*}
Setting $\tau = \frac{1}{1+t}$, we obtain
\[
\left| \sum_l \sqrt{P_l Q_l} - \int \sqrt{p(x) q(x)} dx \right| \rightarrow 0,
\]
completing the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proofs of Theorems~\ref{thm:weighted_sbm_rate1} and Theorem~\ref{thm:weighted_sbm_rate2}}
\label{sec:transformation_proof}

We now outline the proofs of Theorems~\ref{thm:weighted_sbm_rate1} and~\ref{thm:weighted_sbm_rate2}, with proofs of supporting propositions in the succeeding subsections.

\subsection{Main argument: Proof of Theorem~\ref{thm:weighted_sbm_rate1}}
\label{AppThmRate1}

By the argument outlined in Section~\ref{sec:transformation_analysis}, the divergences $I$ and $H$ do not change after transforming the densities $p(x)$ and $q(x)$ according to $\Phi$. 
%First, we claim that the divergence $I$ and $H$ between $p(x), q(x)$ does not change after we apply the transformation $\Phi$. To see this, note that the transformed density $p_{\Phi}(z)$ and $q_{\Phi}(z)$, now supported over $[0,1]$, have the following form:
%\[
%p_{\Phi}(z) = \frac{p(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))} \quad \text{and} \quad
%q_{\Phi}(z) = \frac{q(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}.
%\]
%Therefore, we have, by a change of variable $z = \Phi^{-1}(x)$, that
%\begin{align*}
%\int_\R \sqrt{p(x) q(x)} dx &= \int_0^1 \sqrt{p_{\Phi}(z) q_{\Phi}(z)} dz, \quad \text{ and} \\
%\int_\R (\sqrt{p(x)} - \sqrt{q(x)})^2 dx &= \int_0^1 (\sqrt{p_{\Phi}(z)} - \sqrt{ q_{\Phi}(z)})^2 dz. 
%\end{align*}
Proposition~\ref{prop:transformation1} shows that under Assumptions A1'--A5', Assumptions C1'--C5' are also satisfied.

Furthermore, our assumption that $L = o(\frac{1}{H})$ implies $L \leq \frac{2}{H}$ for sufficiently large $L$. Hence, Proposition~\ref{prop:discretization1} applies, and we may conclude that after transformation and discretization, the label probabilities satisfy $\frac{1}{2c_0 \rho} \leq \frac{P_l}{Q_l} \leq 2c_0 \rho$, for all $l$. Using the assumption $L = o(nI)$ and the fact that $I_L = I (1 + o(1))$ from Proposition~\ref{prop:discretization1}, we also have $L = o(n I_L)$, so we may use Proposition~\ref{prop:labeled_sbm_rate} (with $\rho_L = 2 c_0 \rho$) to obtain
\[
\lim_{n \rightarrow \infty} P \left( l(\hat{\sigma}, \sigma_0) \leq \exp \left( - \frac{ n I_L}{ \beta K} (1 + o(1)) \right) \right) \rightarrow 1.
\]
The theorem then follows from the fact that $I_L = I(1+o(1))$. 

\subsection{Main argument: Proof of Theorem~\ref{thm:weighted_sbm_rate2}}
\label{AppThmRate2}

The proof parallels the argument for Theorem~\ref{thm:weighted_sbm_rate1} outlined above. Proposition~\ref{prop:transformation2} establishes that Assumptions A1--A4 imply Assumptions C1--C4. Hence, Proposition~\ref{prop:discretization2} applies, and we may conclude that after transformation and discretization, the label probabilities satisfy 
\[
\frac{1}{2c_0 \exp(L^{1/r})} \leq \frac{P_l}{Q_l} \leq 2c_0 \exp(L^{1/r}),
\]
for all $l$, and $I_L = I(1+o(1))$. Therefore, we may again apply Proposition~\ref{prop:labeled_sbm_rate} (with $\rho_L = 2 c_0 \exp(L^{1/r})$) to obtain
\[
\lim_{n \rightarrow \infty} P \left( l(\hat{\sigma}, \sigma_0) \leq \exp \left( - \frac{ n I_L}{ \beta K} (1 + o(1)) \right) \right) \rightarrow 1.
\]
The theorem follows from the fact that $I_L = I(1+o(1))$. 

\subsection{Transformation Analysis}

\begin{proposition}
\label{prop:transformation1}
Let $p(x)$ and $q(x)$ be densities over $S$, where $S = \R$ or $S = \R^+$, and let $\Phi \,:\, S \rightarrow [0,1]$ be a CDF such that $\phi = \Phi'$ is positive and continuous. Suppose Assumptions A1'--A5' hold.

%Suppose $p(x), q(x), \Phi$ satisfy the following conditions:

%\begin{enumerate}
%\item[A1] $p(x), q(x) > 0$ on the interior of $S$, $\sup_n \sup_x p(x) \vee q(x) < \infty$, and $\sup_n \inf_{x \in S} \frac{p(x) \vee q(x)}{\phi(x)} < \infty$. 
%\item[A2] There exists $R$ a subinterval of $S$ such that: (a) for $x \in \R$, $\frac{1}{\rho} \leq \frac{p(x)}{q(x)} \leq \rho $ where $\rho$ is an absolute constant and (b) $\Phi\{R^c\} = o(H)$ 

%\item[A3] Let $\alpha^2 = \int_R q(x) \left( \frac{p(x) - q(x)}{q(x)} \right)^2 dx$ and $\gamma(x) = \frac{q(x) - p(x)}{\alpha}$, then 
%$$\sup_n \int_R q(x) \left| \frac{\gamma(x)}{q(x)} \right|^r dx  < \infty.$$
%for an absolute constant $r > 4$. 
%\item[A4] There is some $h_n(x)$ such that 
%   (1) $h_n(x) \geq \max \left\{  \left|\frac{\gamma'(x)}{q(x)} \right|,  \left|\frac{q'(x)}{q(x)}\right|, \left | \frac{\gamma(x)}{q(x)} \right|  \right\} $, 
%   (2) $h_n(x)$ is $(c_{s1}, c_{s2}, C_s)$-bowl-shaped for absolute constants $c_{s1}, c_{s2}, C_s$, and
%   (3) $$\sup_n \int_R |h_n(x)|^{2t} \phi(x) dx < \infty$$ for an absolute constant $t$ such that %$1 > 2t > \frac{4}{r}$.

%\item[A5]  $(\log p)'(x), (\log q)'(x) \geq (\log \phi)'(x)$ for all $x \leq c_{s1}$ and $ (\log p)'(x), (\log q)'(x) \leq (\log \phi)'(x)$ for all $x \geq c_{s2}$.
%\end{enumerate}

%Let $p_\Phi(z), q_\Phi(z)$ be the $\Phi$-transformed densities over $[0,1]$.
The following conditions are satisfied for $p_\Phi(z) = \frac{p(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$ and $q_\Phi(z) = \frac{q(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$:

\begin{enumerate}
\item[C1'] $p_{\Phi}(z), q_{\Phi}(z) > 0$ on $(0, 1)$, and $\sup_z \left\{p_{\Phi}(z) \vee q_{\Phi}(z)\right\} < \infty$. 
\item[C2'] There exists a subinterval $R \subseteq [0,1]$ such that
\begin{itemize}
\item[(a)] for all $z \in R$, $\frac{1}{\rho} \leq \left| \frac{p_{\Phi}(z)}{q_{\Phi}(z)} \right| \leq \rho$, where $\rho$ is an absolute constant, and
\item[(b)] $\mu\{R^c\} = o(H)$, where $\mu$ is the Lebesgue measure.
\end{itemize}
\item[C3'] Let $\alpha^2 = \int_R \frac{(p_{\Phi}(z) - q_{\Phi}(z))^2}{q_{\Phi}(z)} dz$ and $\gamma_{\Phi} (z) = \frac{q_{\Phi}(z) - p_{\Phi}(z)}{\alpha}$. Then $\int_R q_{\Phi}(z) \left| \frac{\gamma(z)}{q_{\Phi}(z)} \right|^r dz  < \infty$, for an absolute constant $r \geq 4$.
\item[C4'] There exists $h_{\Phi}(z)$ such that
\begin{itemize}
\item[(a)] $h_{\Phi}(z) \geq \max \left\{  \left|\frac{\gamma'_{\Phi}(z)}{q_{\Phi}(z)} \right|, 
 \left|\frac{q_{\Phi}'(z)}{q_{\Phi}(z)}\right|  \right\} $,
 \item[(b)] $h_{\Phi}(z)$ is $(c_{s1}', c_{s2}', C_s')$-bowl-shaped, for absolute constants $c_{s1}', c_{s2}'$, and $C_s'$, and
\item[(c)] $\int_R |h_{\Phi}(z)|^t dz < \infty$ for an absolute constant $\frac{2}{r} < t < 1$.
\end{itemize}
\item[C5'] $p_{\Phi}'(z), q_{\Phi}'(z) \geq 0$ and for all $z < c'_{s1}$, and $p_{\Phi}'(z), q_{\Phi}'(z) \leq 0$ for all $z > c'_{s2}$.
\end{enumerate}
\end{proposition}

\begin{proof}

\textbf{C1'} follows from A1' and the condition that $\phi$ is positive and continuous.

To prove \textbf{C2'}, assume that A2' is true, and let $R$ be a subinterval of $\R$ such that $\frac{1}{\rho} \leq \frac{p(x)}{q(x)} \leq \rho$. Define $R_{\Phi} = \left\{ z \in [0,1] \,:\, \Phi^{-1}(z) \in R \right\}$, so $\mathbf{1}_{R_\Phi}(z) = \mathbf{1}_R(\Phi^{-1}(z))$. Then $R_{\Phi}$ is clearly an interval, and $\Phi\{ R^c \} = \mu\{ R_{\Phi}^c \}$.
 
\textbf{C3'} follows from A3' via a change of variables. 

It remains to prove \textbf{C4'} and \textbf{C5'}. We first prove \textbf{C5'}. Note that 
\begin{align*}
p'_\Phi(z) = \frac{ p'(\Phi^{-1}(z))  - 
                     p(\Phi^{-1}(z)) \frac{\phi'(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))} }
           { \phi(\Phi^{-1}(z))^2}.
\end{align*}
Therefore, $p'_\Phi(z) \geq 0$ if and only if $p'(x) \geq p(x) \frac{\phi'(x)}{\phi(x)}$, and likewise for $q'_\Phi(z)$.

Moving onto \textbf{C4'}, we first construct $h(z)$. For ease of presentation, let $x = \Phi^{-1}(z)$. We then have
\begin{align*}
\frac{q'_\Phi(z)}{q_\Phi(z)} &= \frac{ q'(x) }{q(x)} \frac{1}{ \phi(x)} - 
                     \frac{ \phi'(x) }{\phi(x)} \frac{1}{ \phi(x)},
\end{align*}
implying that
\begin{align*}
\left| \frac{q'_\Phi(z)}{q_\Phi(z)} \right| &\leq 
      \left| \frac{ q'(x) }{q(x)} \frac{1}{ \phi(x)} \right| +  
     \left| \frac{ \phi'(x) }{\phi(x)} \frac{1}{ \phi(x)} \right| \lesssim (h(x) + 1) \frac{1}{\phi(x)} ,
\end{align*}
where the last inequality follows because $\left| \frac{\phi'(x)}{\phi(x)} \right|$ is bounded. Furthermore,
\begin{align*}
 \frac{\gamma'_\Phi(z)}{q_\Phi(z)} &=   
       \frac{1}{\alpha} 
    \frac{p'(x) - p(x) \frac{\phi'(x)}{\phi(x)} - q'(x) + q(x) \frac{\phi'(x)}{\phi(x)} }
                                       {q(x) \phi(x) } \nonumber \\
  &=  \left( \frac{1}{\alpha} \frac{p'(x) - q'(x)}{q(x)} 
          - \frac{1}{\alpha} \frac{p(x) - q(x)}{q(x)} \frac{\phi'(x)}{\phi(x)}\right)
         \frac{1}{\phi(x)},
\end{align*}
so
\begin{align*}   
\left|  \frac{\gamma'_\Phi(z)}{q_\Phi(z)} \right| &\leq
      \left| \frac{1}{\alpha} \frac{p'(x) - q'(x)}{q(x)} \right| 
            \frac{1}{\phi(x)} 
          + \left| \frac{1}{\alpha} \frac{p(x) - q(x)}{q(x)} \right| 
        \left| \frac{\phi'(x)}{\phi(x)}\right|   
         \frac{1}{\phi(x)} \\
  & = \left| \frac{\gamma'(x)}{q(x)} \right| \frac{1}{\phi(x)} + 
       \left| \frac{\gamma(x)}{q(x)} \right| \left| \frac{\phi'(x)}{\phi(x)} \right| \frac{1}{\phi(x)}\\
  &\stackrel{(a)} \lesssim h(x) \frac{1}{ \phi(x)},
\end{align*}
where $(a)$ follows because $\left| \frac{\phi'(x)}{\phi(x)} \right|$ is bounded. We want to take $h_{\Phi}(z) \simeq (h(x) + 1) \frac{1}{\phi(x)}$, but we use a modified upper bound to ensure that $h_{\Phi}(z)$ is bowl-shaped. Let $\psi(x) = \max \left\{ \frac{1}{\phi(c_{s1})}, \frac{1}{\phi(c_{s2})}, \frac{1}{\phi(x)} \right\} $. We then take 
\begin{align*}
h_{\Phi}(z) &\simeq h(x) \psi(x) = h( \Phi^{-1}(z)) \psi(\Phi^{-1}(z)).
\end{align*}

%$h_{\Phi,n}(z)$ is an upper bound by construction.

Note that $(h(x) + 1)$ is $(c_{s1}, c_{s2}, C_s + 1)$-bowl-shaped, and $\phi$ is unimodal, so$\frac{1}{\phi(x)}$ is quasi-convex. Hence, $\psi(x)$ is quasi-convex and has a mode lying in $[c_{s1}, c_{s2}]$. Therefore, $(h(x) + 1) \psi(x)$ is $(c_{s1}, c_{s2}, C'_s)$-bowl-shaped, where 
$C'_s \simeq (C_s + 1) \left( \frac{1}{\phi(c_{s1})} \vee \frac{1}{\phi(c_{s2})} \right)$. This shows that $h_{\Phi}(z)$ is $(c'_{s1}, c'_{s2}, C'_s)$-bowl-shaped for $c'_{s1} = \Phi(c_{s1})$ and $c'_{s2} = \Phi(c_{s2})$. 

Finally, we need to verify the integrability conditions:
\begin{align*}
\int |h_{\Phi}(z)|^t dz &\simeq 
        \int  (h(\Phi^{-1}(z)) + 1)^t \psi( \Phi^{-1}(z))^t  dz \\
     &\stackrel{(a)} =  \int_S (h(x) + 1)^t \psi(x)^t \phi(x) dx \\
   &\leq \left\{ \int_S (h(x) + 1)^{2t} \phi(x) dx  \right\}^{1/2} 
         \left\{ \int_S \psi(x)^{2t}\phi(x) dx \right\}^{1/2},
\end{align*}
where $(a)$ follows from a change of variables. To bound the first term, note that
\begin{align*}
 \int_S (h(x) + 1)^{2t} \phi(x) dx &\leq \int_S h(x)^{2t} \phi(x) dx + \int_S \phi(x) dx \\
    &\leq \int_S h(x)^{2t} \phi(x) dx + 1.
\end{align*}
The first inequality follows since $2t < 1$ by assumption. Note that $\int_S h(x)^{2t} \phi(x) dx < \infty$.

We now bound the second term:
\begin{align*}
\int_S \psi(x)^{2t} \phi(x) dx &\leq  \int_S \phi(c_{s1})^{-2t} \phi(x) dx +
           \int_S \phi(c_{s2})^{-2t} \phi(x) dx +  
           \int_S \phi(x)^{-2t} \phi(x) dx.
\end{align*}
The first two terms are constants. The last term is $\int_S \phi(x)^{1-2t} dx$, which is finite because $1 - 2t > 0$ and $\phi$ is a valid transformation function. 
\end{proof}

%%%%%

\begin{proposition}
\label{prop:transformation2}
Suppose Assumptions A1--A4 hold.
%\begin{enumerate}
%\item[A1'] $p(x), q(x) > 0$ on the interior of $S$, $\sup_n \sup_x p(x) \vee q(x) < \infty$, and $\sup_n \inf_{x \in S} \frac{p(x) \vee q(x)}{\phi(x)} < \infty$. 
%\item[A2'] For an absolute constant $r \geq 8$, $\sup_n \int \left| \log \frac{p(x)}{q(x)} \right|^r \phi(x) dx < \infty$.
%\item[A3'] There is some $h_n(x)$ such that 
%   (1) $h_n(x) \geq \max \left\{  \left|\frac{q'(x)}{q(x)} \right|,  \left|\frac{p'(x)}{p(x)}\right| \right\} $, 
%   (2) $h_n(x)$ is $(c_{s1}, c_{s2}, C_s)$-bowl-shaped for some absolute constants $c_{s1}, c_{s2}, C_s$, and
%   (3) $$ \sup_n \int_R |h_n(x)|^{2t} \phi(x) dx < \infty$$ for some constant $t$ such that  $1 > 2t > \frac{4}{r}$.
%\item[A4']  $(\log p)'(x), (\log q)'(x) \geq (\log \phi)'(x)$ for all $x < c_{s1}$ and $ (\log p)'(x), (\log q)'(x) \leq (\log \phi)'(x)$ for all $x > c_{s2}$.
%\end{enumerate}
%Now we let $p_\Phi(z), q_\Phi(z)$ be the $\Phi$-transformed densities over $[0,1]$.
The following conditions are satisfied for $p_\Phi(z) = \frac{p(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$ and $q_\Phi(z) = \frac{q(\Phi^{-1}(z))}{\phi(\Phi^{-1}(z))}$:
\begin{enumerate}
\item[C1] $p_{\Phi}(z), q_{\Phi}(z) > 0$ on $(0, 1)$, and $\sup_z \left\{p_{\Phi}(z) \vee q_{\Phi}(z)\right\} < \infty$. 
\item[C2] For some $r > 2$, $\int \left| \log \frac{p_\Phi(z)}{q_\Phi(z)} \right|^r dz < \infty$.
\item[C3]  There exists $h_{\Phi}(z)$ such that
\begin{itemize}
\item[(a)] $h_{\Phi}(z) \geq \max \left\{  \left|\frac{p'_\Phi(z)}{p_\Phi(z)} \right|, 
 \left|\frac{q'_\Phi(z)}{q_\Phi(z)}\right|  \right\}$,
\item[(b)] $h_{\Phi}(z)$ is $(c_{s1}', c_{s2}', C_s')$-bowl-shaped, and
\item[(c)] $\int_R |h_{\Phi,n}(z)|^t dz < \infty$, for some constant $t$ such that $\frac{2}{r} \le t \le 1$.
\end{itemize} 
\item[C4]  We have that $p'_\Phi(z), q'_\Phi(z) \geq 0$ for all $z < c'_{s1}$, and $p'_\Phi(z), q'_\Phi(z) \leq 0$ for all $z > c'_{s2}$. 
\end{enumerate}

\end{proposition}

\begin{proof}
The proof is identical to that of Proposition~\ref{prop:transformation1}, so we omit the details.
\end{proof}

%%%%%%

\section{Proof of Proposition~\ref{prop:theta_rate}}
\label{sec:theta_rate_proof}


First suppose $\| \theta_1 - \theta_0 \| \rightarrow 0$. In Lemma~\ref{lem:hellinger_theta_equivalence}, we show that $\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \rightarrow 0$. Assumptions \textbf{A1'} and \textbf{A5'} follow directly from Assumptions B1 and B5, respectively. 

We now prove \textbf{A2'}. Let $\rho$ be a constant and define 
\begin{align}
R = \left\{ x \,:\, g_1(x) 
      \leq \frac{\log \rho}{\| \theta_1 - \theta_0 \|} \right\}, \label{eqn:parametric_R_defn}
\end{align}
where $g_1(x)$ is the upper bound on $\sup_{\theta \in \Theta} \| \nabla_\theta f_\theta(x) \|$ defined in Assumption B3. Since $g_1(x)$ is bowl-shaped, we conclude that $R$ is an interval if $\log \rho \geq C_s \trm{diam}(\Theta)$. Note that
\begin{align*}
\log \frac{p(x)}{q(x)} &= f_{\theta_1}(x) - f_{\theta_0}(x) = (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}} (x).
\end{align*}
This implies 
\begin{align*}
\left | \log \frac{p(x)}{q(x)} \right | &\leq \| \theta_1 - \theta_0 \| \| \nabla_{\theta} f_{\bar{\theta}} (x) \| \leq  \| \theta_1 - \theta_0 \| \sup_\theta \| \nabla_{\theta} f_{\theta} (x) \| \le \|\theta_1 - \theta_0\| \cdot g_1(x),
\end{align*}
where $\bar{\theta}$ is a convex combination of $\theta_0, \theta_1$. Therefore, for all $ x \in R$, we have $\frac{1}{\rho} \leq \frac{p(x)}{q(x)} \leq \rho$.

Since we know from Assumption B3 that $\int |g_1(x)|^r \phi(x) dx < \infty$, Markov's inequality gives
\begin{align*}
\Phi(R^c) &= \Phi \left\{ x \,:\, g_1(x) 
    > \frac{\log \rho}{\| \theta_1 - \theta_0 \|} \right\} 
    \leq C \frac{\| \theta_1 - \theta_0 \|^{r}}{(\log \rho)^{r}} 
    \stackrel{(a)}= \Theta( H^{r/2} ) = o(H),
\end{align*}
where $(a)$ follows from Lemma~\ref{lem:hellinger_theta_equivalence}. The last equality follows from the assumption that $r > 2$. This proves A2'.

Now we move on to \textbf{A3'}. By Lemma~\ref{lem:chi_square_theta_equivalence}, we have
\begin{align*}
\frac{1}{\alpha} \left| \frac{p(x) - q(x)}{q(x)} \right| &\lesssim 
    \frac{1}{\| \theta_1 - \theta_0\|}  \left| \frac{p(x)}{q(x)} - 1 \right| \\
%
 &=  \frac{1}{\| \theta_1 - \theta_0\|}  
        \left| \exp( f_{\theta_1}(x) - f_{\theta_0}(x) ) - 1 \right| \\
%
   & \stackrel{(a)} =  \frac{1}{\| \theta_1 - \theta_0\|} \Big|(\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x)  \Big|
      \exp( f_{\bar{\theta}}(x) - f_{\theta_0}(x)) \\
  &\leq \| \nabla_{\theta} f_{\bar{\theta}}(x) \|   \exp( f_{\bar{\theta}}(x) - f_{\theta_0}(x)) \\
%
  &\stackrel{(b)} = \|  \nabla_{\theta} f_{\bar{\theta}}(x) \| 
       \exp\Big( (\bar{\theta} - \theta_1)^\tran \nabla_{\theta} f_{\tilde{\theta}}(x) \Big) \\
  &\leq  \|  \nabla_{\theta} f_{\bar{\theta}}(x) \| 
        \exp\Big( \| \theta_1 - \theta_0\| \| \nabla_{\theta} f_{\tilde{\theta}}(x) \| \Big),
\end{align*}
where in $(a)$, $\bar{\theta}$ is a convex combination of $\theta_1$ and $\theta_0$, and in $(b)$, $\tilde{\theta}$ is a convex combination of $\bar{\theta}$ and $\theta_0$. Assumption B3 implies that both $\| \nabla_\theta f_{\bar{\theta}}(x) \|$ and 
$\| \nabla_\theta f_{\tilde{\theta}}(x) \|$ are upper-bounded by $g_1(x)$, so
\begin{align}
\frac{1}{\alpha} \left| \frac{p(x) - q(x)}{q(x)} \right| &\lesssim
         g_1(x) \exp\big( \| \theta_0 - \theta_1 \| g_1(x) \big). \label{eqn:gamma_bound_function}
\end{align}

Therefore,
\begin{align*}
\int_R \left( \frac{1}{\alpha} \left| \frac{p(x)}{q(x)} - 1 \right| \right)^r q(x) dx &\lesssim 
    \int_R  g_1(x)^r \exp\big(r \| \theta_0 - \theta_1 \| g_1(x) \big)  q(x) dx \\
  &\stackrel{(a)} \leq \int_R g_1(x)^r \rho^r q(x) dx \\
  &\stackrel{(b)} \lesssim \int_R g_1(x)^r \phi(x) dx,
\end{align*}
where $(a)$ follows from the definition of $R$ and $(b)$ follows because $\frac{q(x)}{\phi(x)}$ is bounded. This proves A3'. 

To prove \textbf{A4'}, we first construct $h(x)$. By equation~\ref{eqn:gamma_bound_function}, we have
\[
\left| \frac{\gamma(x)}{q(x)} \right| \lesssim
     g_1(x) \exp\Big( \|\theta_0 - \theta_1 \|  g_1(x) \Big).
\]
By Assumption B3, we also have $\left| \frac{q'(x)}{q(x)} \right| = |f'_{\theta_0}(x)| \leq g_{2, \theta_0}(x)$, and
\begin{align*}
\left| \frac{\gamma'(x) }{q(x)} \right| &=
    \left| \frac{1}{\alpha} \frac{p'(x) - q'(x)}{q(x)} \right| \\
%
&\lesssim \frac{1}{\| \theta_0 - \theta_1 \|}\left| \frac{p'(x) - q'(x)}{q(x)}\right|\\
%
& =  \frac{1}{\| \theta_0 - \theta_1 \|} \left|
           f'_{\theta_1} \frac{p(x)}{q(x)} - f_{\theta_0}'(x) \right| \\
  &= \frac{1}{\|\theta_0 - \theta_1\|} \left| 
          ( f'_{\theta_1}(x) - f'_{\theta_0}(x) ) \frac{p(x)}{q(x)} 
       + f'_{\theta_0} \left( \frac{p(x)}{q(x)} - 1 \right) \right| \\
  &\leq \| \nabla_{\theta} f'_{\bar{\theta}}(x) \| \frac{p(x)}{q(x)} 
         + \frac{1}{\| \theta_1 - \theta_0\|} 
        \left| \frac{p(x)}{q(x)} - 1 \right|  |f'_{\theta_0}(x)|,
\end{align*}
where $\bar{\theta}$ is a convex combination of $\theta_0$ and $\theta_1$. 

Using Assumption B3 and inequality~\eqref{eqn:gamma_bound_function}, we have
\[
\left| \frac{\gamma'(x)}{q(x)} \right| \lesssim 
      g_{2,\bar{\theta}}(x) \exp\Big( \| \theta_0 - \theta_1\| g_1(x) \Big)
      +  g_1(x) \exp\Big( \| \theta_0 - \theta_1 \| g_1(x) \Big) g_{2, \theta_0}(x).
\]
Hence, we may choose choose 
\begin{align*}
h(x) & \simeq g_{2,\bar{\theta}}(x) \exp\Big( \| \theta_0 - \theta_1\| g_1(x) \Big)
      +  g_1(x) \exp\Big( \| \theta_0 - \theta_1 \| g_1(x) \Big) g_{2, \theta_0}(x) \\
      & \qquad +  g_1(x) \exp\Big( \| \theta_0 - \theta_1 \| g_1(x) \Big).
\end{align*}
Since all the component functions are $(c_{s1}, c_{s2}, \tilde{C}_s)$ bowl-shaped, $h(x)$ is $(c_{s1}, c_{s2}, C_s)$-bowl-shaped, where 
$C_s = 3 \tilde{C}^2_s \exp( \textrm{diam}(\Theta) \tilde{C}_s)$. Furthermore,
\begin{align*}
\int_R h(x)^{2t} \phi(x) dx &\stackrel{(a)} \lesssim \int_R \Big( g_{2, \bar{\theta}}^{2t} \rho^{2t} + 
               g_1(x)^{2t} \rho^{2t} g_{2, \theta_0}^{2t} + g_1(x)^{2t} \rho^{2t}
              \Big) \phi(x) dx  \\
   & \lesssim \int_R  g_{2, \bar{\theta}}(x)^{2t} \phi(x) dx + 
        \int_R  g_1(x)^{2t} g_{2, \theta_0}^{2t} \phi(x) dx  + 
       \int_R g_1(x)^{2t} \phi(x) dx,
\end{align*}
where $(a)$ follows because on $R$, we have $\| \theta_0 - \theta_1 \| g_1(x) \leq \log \rho$. 
By Assumption B3, the first and third terms are finite, uniformly over all $\theta_1, \theta_0 \in \Theta$. It is straightforward to show that the second term is also finite, by an application of the Cauchy-Schwartz inequality.

Now suppose $\| \theta_1 - \theta_0 \| = \Theta(1)$. Lemma~\ref{lem:hellinger_theta_equivalence} implies $\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = \Theta(1)$. Assumptions \textbf{A1} and \textbf{A4} follow directly from Assumptions B1 and B5. 

To prove \textbf{A2}, note that from a previous derivation, we have
\[
\left | \log \frac{p(x)}{q(x)} \right | 
                \leq  \| \theta_1 - \theta_0 \| \sup_\theta \| \nabla_{\theta} f_{\theta} (x) \| \leq 
     \| \theta_1 - \theta_0 \| g_1(x).
\]
Since $\|\theta_1 - \theta_0\| \leq \textrm{diam}(\Theta)$, which is a constant, and $\int g_1(x)^r \phi(x) dx < \infty$ by Assumption B3, we obtain A2.

To prove \textbf{A3}, note that
\begin{align*}
\frac{q'(x)}{q(x)} = f'_{\theta_0}(x), \quad \text{and} \quad \frac{p'(x)}{p(x)} &= f'_{\theta_1}(x).
\end{align*}
Therefore, the choice $h(x) = g_{2, \theta_0}(x) + g_{2, \theta_1}(x)$ upper-bounds $\left| \frac{q'(x)}{q(x)} \right|$ and 
$\left| \frac{p'(x)}{p(x)} \right|$, by Assumption B3. Furthermore, $h(x)$ is $(c_{s1}, c_{s2}, C_s)$-bowl-shaped, where $C_s = 2 \tilde{C}_s$. 

To prove the last integrability condition, note that
\[
 \int h(x)^{2t} \phi(x) dx \leq \int g_{2, \theta_0}(x)^{2t} \phi(x) dx  + \int g_{2, \theta_1}(x)^{2t} \phi(x) dx.
\]
Hence, 
\[
\int h(x)^{2t} \phi(x) dx \leq 2 \sup_{\theta \in \Theta} \int g_{2, \theta}(x)^{2t} \phi(x) dx.
\]



\subsection{Supporting lemmas}

\begin{lemma}
\label{lem:chi_square_theta_equivalence}
Under Assumptions B1--B5, we have
%$\alpha = \int_R q(x) \left( \frac{p(x)}{q(x)} - 1 \right)^2 dx$ satisfies
$\alpha \asymp \| \theta_1 - \theta_0 \|$.
\end{lemma}

\begin{proof}
We write
\begin{align*}
\alpha^2 &= \int_R \left( \frac{p(x)}{q(x)} - 1 \right)^2 q(x) dx \\
 &= \int_{R} \left| \exp\Big( f_{\theta_1}(x) - f_{\theta_0}(x) \Big) - 1 \right| ^2
            q(x )dx \\
 &= \int_{R} \left( (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x) 
                            \exp\Big( f_{\bar{\theta}}(x) - f_{\theta_0}(x) \Big) \right)^2 q(x)dx. 
\end{align*}
First we show an upper bound:
\begin{align*}
\alpha^2 & \leq \int_{R} \| \theta_1 - \theta_0 \|^2 \| \nabla_{\theta} f_{\bar{\theta}}(x)\|^2
                            \exp\Big( f_{\bar{\theta}}(x) - f_{\theta_0}(x) \Big)
              \exp( f_{\bar{\theta}}(x) ) dx\\
& \leq  \int_{R} \| \theta_1 - \theta_0 \|^2 \| \nabla_{\theta} f_{\bar{\theta}}(x)\|^2
                            \exp\Big( \| \theta_1 - \theta_0\| \|\nabla_{\theta} f_{\tilde{\theta}}(x) \| \Big) 
              \exp( f_{\bar{\theta}}(x) ) dx.
\end{align*}
On $R$, we have $\| \theta_1 - \theta_0\| \sup_\theta \| \nabla_\theta f_{\theta}(x) \| \leq \log \rho$. Hence,
\begin{align*}
\alpha^2 & \leq   \| \theta_1 - \theta_0 \|^2 \int_{R} \| \nabla_{\theta} f_{\bar{\theta}}(x)\|^2
                           e^{\log \rho}
              \exp( f_{\bar{\theta}}(x) ) dx\\
& \leq  \| \theta_1 - \theta_0 \|^2 \rho \int_{-\infty}^\infty \| \nabla_{\theta} f_{\bar{\theta}}(x)\|^2
              \exp( f_{\bar{\theta}}(x) ) dx \\
& \stackrel{(a)}\lesssim \| \theta_1 - \theta_0 \|^2,
\end{align*}
where $(a)$ follows from Assumptions B1 and B4. We now establish a lower bound:
\begin{align*}
\alpha^2 &\geq  \int_{R} \left( (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x) \right)^2
                            \exp\Big( - |f_{\bar{\theta}}(x) - f_{\theta_0}(x)| \Big)  \exp(f_{\bar{\theta}}(x))
             dx \\
  &\geq \int_{R} \left( (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x) \right)^2
                            \exp\Big( - \|\theta_1 - \theta_0\| \| \nabla_{\theta} f_{\bar{\theta}}(x) \| \Big)  
          \exp(f_{\bar{\theta}}(x)) dx \\
  &\stackrel{(a)}\geq \frac{1}{\rho}  (\theta_1 - \theta_0)^\tran 
                \left( \int_{R} ( \nabla_{\theta} f_{\bar{\theta}}(x) ) 
                                      ( \nabla_{\theta} f_{\bar{\theta}}(x) )^\tran
                          \exp(f_{\bar{\theta}}(x)) dx \right) (\theta_1 - \theta_0),
\end{align*}
where $(a)$ follows from Assumption B3. Define 
\[
\tilde{G}_{\bar \theta} =  \int_{R} ( \nabla_{\theta} f_{\bar{\theta}}(x) ) 
                                      ( \nabla_{\theta} f_{\bar{\theta}}(x) )^\tran
                          \exp(f_{\bar{\theta}}(x)) dx.
\]
As $\rho$ increases, $R \rightarrow S$. Therefore, there exists an absolute constant such that for all $\rho$ greater than or equal to this constant, we have
$\lambda_{min}(\tilde{G}_{\bar \theta}) > \frac{1}{2} \lambda_{min}(G_{\bar \theta}) > 0$. Hence, $\alpha^2 \gtrsim \| \theta_1 - \theta_0 \|^2$.
\end{proof}




\begin{lemma}
\label{lem:hellinger_theta_equivalence}
The Hellinger distance satisfies the bound
\[
\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx = c \| \theta_0 - \theta_1 \|_2^2,
\]
where $ c_{\min} \leq c \leq \frac{1}{4} c_{\max} d_{\Theta} $.
\end{lemma}

\begin{proof}
Expanding the left-hand side, we have
\begin{align*}
\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx & = \int q(x) \left(
        \sqrt{ \frac{p(x)}{q(x)} } - 1 \right)^2 dx \\
&= \int q(x)
       \left( \exp\left(\frac{f_{\theta_1}(x)}{2} - \frac{f_{\theta_0}(x)}{2} \right) - 1 \right)^2 dx.
\end{align*}
%Consider the exponential term $\exp\left(\frac{f_{\theta_1}(x)}{2} - \frac{f_{\theta_0}(x)}{2}\right) - 1$.
Let $h(\theta) = \exp\left(\frac{f_{\theta}(x)}{2} - \frac{f_{\theta_0}(x)}{2}\right)$. It is clear that $h(\theta_0) = 1$ and that we wish to bound $h(\theta_1) - h(\theta_0)$. We bound this as follows:
\begin{align*}
|h(\theta_1) - h(\theta_0)| &= |(\theta_1 - \theta_0)^\tran \nabla_{\theta} h(\bar{\theta})| \\
   &= \left |\frac{1}{2} (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x) 
             \exp\left(\frac{f_{\bar{\theta}}(x)}{2} - \frac{f_{\theta_0}(x)}{2}\right) \right|\\
&\leq \frac{1}{2} \| \theta_1 - \theta_0 \| 
                  \| \nabla_{\theta} f_{\bar{\theta}}(x) \| \exp\left(\frac{f_{\bar{\theta}}(x)}{2} - \frac{f_{\theta_0}(x)}{2}\right), 
\end{align*}
where $\bar{\theta} \in \Theta$ is some convex combination of $\theta_1, \theta_0$. Thus, we have
\begin{align*}
\int q(x)
       \left(\exp\left(\frac{f_{\theta_1}(x)}{2} - \frac{f_{\theta_0}(x)}{2}\right) - 1\right)^2 dx 
  &\leq 
  \int q(x) \frac{1}{4} \| \theta_1 - \theta_0 \|^2 \| \nabla_{\theta} f_{\bar{\theta}}(x) \|^2 
               \exp( f_{\bar{\theta}}(x) - f_{\theta_0}(x)) dx \\
  &= \frac{1}{4} \| \theta_1 - \theta_0 \|^2 \int  \| \nabla_{\theta} f_{\bar{\theta}}(x) \|^2  
            \exp( f_{\bar{\theta}} (x)) d x \\
  &\leq \frac{1}{4}  \| \theta_1 - \theta_0 \|^2 \tr( G_{\bar{\theta}} ) \\
  &\leq \frac{1}{4}  \| \theta_1 - \theta_0 \|^2 c_{\max} d_{\Theta},
\end{align*}
where $\Theta \subseteq \mathbb R^{d_\Theta}$. Furthermore,
\begin{align*}
\int q(x)
       \left(\left(\frac{f_{\theta_1}(x)}{2} - \frac{f_{\theta_0}(x)}{2} \right) - 1 \right)^2 dx 
   &= \int \Big( (\theta_1 - \theta_0)^\tran \nabla_{\theta} f_{\bar{\theta}}(x) \Big)^2 
               \exp(f_{\bar{\theta}}(x))  dx \\
   &= (\theta_1 - \theta_0)^\tran G_{\bar{\theta}} (\theta_1 - \theta_0)  \\
   &\geq c_{\min} \| \theta_1 - \theta_0 \|^2.
\end{align*}
\end{proof}


\subsection{Proof of examples}
\label{sec:appendix_examples}

\begin{proposition}
\label{prop:scale_location_family}
Let $\exp( f(x))$ be a positive density over $\R$, where
\begin{itemize}
\item[(a)] $|f^{(k)}(x)|$ is bounded for some $k \geq 2$, and
\item[(b)] there exist constants $c$ and $M$ such that $f'(x) > M$ for $x < -c$ and $f'(x) < - M$ for $x > c$. 
\end{itemize}
Let $\theta = (\mu, \sigma)$ and $\Theta = [- C_{\mu}, C_{\mu}] \times [\frac{1}{c_{\sigma}}, c_{\sigma}]$, for some absolute constants $C_{\mu}$ and $c_{\sigma}$, and let
\[
f_\theta (x) = f\left( \frac{x - \mu}{\sigma} \right) - \log \sigma.
\]
Then $\{ f_\theta(x) \}_{\theta \in \Theta}$ satisfies Assumptions B1--B4 with respect to $\phi$ defined in equation~\eqref{eqn:phi_defn2}.
\end{proposition}

\begin{proof}

Before we prove the claims, let us derive some useful properties of $f$. 

First, for any $x > c$, we have
\begin{align*}
f(x) &= \int_0^x f'(t) dt = \int_0^c f'(t) dt + \int_c^x f'(t) dt \lesssim 1 -  \int_c^x M dt \lesssim 1 - x.
\end{align*}
Similarly, for any $x < -c$, we have $f(x) \lesssim 1 + x$. Therefore, $f(x) \lesssim 1 - |x|$. 

Likewise, we have
\[
f\left( \frac{x - \mu}{\sigma} \right) \lesssim 1 - \left| \frac{x - \mu}{\sigma} \right| 
         \lesssim 1 - \left| \frac{x}{\sigma} \right| + \frac{\mu}{\sigma} 
     \stackrel{(a)} \lesssim 1 - |x|,
\]
where $(a)$ follows because $\sigma \geq \frac{1}{c_\sigma}$ and $|\mu| \leq C_\mu$, for some absolute constants $c_\sigma$ and $C_\mu$. Thus, the density $\exp f\left( \frac{x - \mu}{\sigma} \right)$ is sub-exponential. 

Since $f^{(k)}(x)$ is bounded, L'Hopital's rule implies $|f'(x)| \lesssim |x|^{k-1} + 1$ and $|f''(x)| \lesssim |x|^{k-2} + 1$. Furthermore,
\begin{align}
f'\left( \frac{x - \mu}{\sigma} \right) \lesssim \left| \frac{x -\mu}{\sigma} \right|^{k-1} + 1 
           \stackrel{(a)} \lesssim \left| \frac{x}{\sigma} \right|^{k-1} + \left| \frac{\mu}{\sigma} \right|^{k-1} + 1
  \stackrel{(b)}  \lesssim |x|^{k-1} + 1, \label{eqn:fprime_bound}
\end{align}
where (a) follows because $k$ is a constant and (b) follows because $|\mu| \leq C_\mu$ and $\sigma \geq \frac{1}{c_\sigma}$, by assumption. 

Now we prove the first claim \textbf{B1}. We have
\begin{align*}
\log \phi (x) - f_\theta(x) &= \log \frac{e}{8} - \sqrt{|x| + 1} - f\left( \frac{x - \mu}{\sigma} \right) - \log \sigma \\
              & \geq  - \sqrt{|x| + 1} - f\left( \frac{x - \mu}{\sigma} \right) - \log \frac{1}{c_{\sigma}} + \log \frac{e}{8}  \\
        &\geq- \sqrt{|x| +1} - C (1 - |x|) -  \log \frac{1}{c_{\sigma}} + \log \frac{e}{8} > - \infty.
\end{align*}

Moving on to \textbf{B2}, we have
\begin{align*}
\nabla f_\theta(x) = \left[ \begin{array}{c}
           - \frac{1}{\sigma} f'\left( \frac{x - \mu}{\sigma} \right) \\
           -\left( \frac{x - \mu}{\sigma^2} \right) f'\left( \frac{x - \mu}{\sigma} \right) - \frac{1}{\sigma} 
        \end{array} \right]
%
   = - \frac{1}{\sigma} f'\left( \frac{x - \mu}{\sigma} \right)  \left[ \begin{array}{c}
           1  \\
           \frac{x - \mu}{\sigma} + 1  
        \end{array} \right].
\end{align*}
To show that $\lambda_{\max}(G_\theta) < \infty$, it is sufficient to show that 
\begin{align*}
&\int \frac{1}{\sigma} f'\left( \frac{x - \mu}{\sigma} \right)^2 \left( \frac{x -\mu}{\sigma} + 1 \right) 
          \exp f \left( \frac{x - \mu}{\sigma} \right) dx < \infty, \quad \text{and} \\
& \int \frac{1}{\sigma} f'\left( \frac{x - \mu}{\sigma} \right)^2 \left( \frac{x -\mu}{\sigma} + 1 \right)^2 
          \exp f \left( \frac{x - \mu}{\sigma} \right) dx < \infty.
\end{align*}

Since $\left| f'\left( \frac{x - \mu}{\sigma} \right) \right| \lesssim \left| \frac{x - \mu}{\sigma} \right|^{k-1} + 1$ and $\exp f\left( \frac{x - \mu}{\sigma} \right)$ is sub-exponential with all moments finite, we conclude that both integrals converge. 

To show that $\lambda_{\min}(G_\theta) > 0$, we need to show that $\textrm{det}(G_\theta) > 0$. Let $g(x) = \frac{1}{\sigma} f'\left( \frac{x - \mu}{\sigma} \right)^2 \exp f \left( \frac{x - \mu}{\sigma} \right)$, and note that $g$ is positive and integrable. The integral of $g$ is not 0, since $|f'(x)| \geq M$ for all $|x| > c$. Thus, $g$ may be normalized to a density $\bar{g}$.

Showing that $\textrm{det}(G_\theta) > 0$ is equivalent to showing that
\[
\int g(x) dx \int \left( \frac{x - \mu}{\sigma} + 1 \right)^2 g(x) dx >  
     \left( \int \left(\frac{ x - \mu}{\sigma} + 1 \right) g(x) dx \right)^2 ,
\]
which is equivalent to showing that
\begin{align*}
\E_{\bar{g}} \left[ \left( \frac{X - \mu}{\sigma} + 1 \right)^2 \right] - 
   \left( \E_{\bar{g}} \left[ \frac{X - \mu}{\sigma} +1 \right] \right)^2 &> 0,
\end{align*}
or $\textrm{Var}_{\bar{g}} (X)  > 0$. This follows because $g(x) \neq 0$.

To verify \textbf{B3}, note that
\begin{align*}
\| \nabla f_\theta \| & = \left| \frac{1}{\sigma} f'\left( \frac{x - \mu}{\sigma} \right) \right| 
            \sqrt{ 1 + ((x-\mu)/\sigma + 1)^2 } \\
     & \lesssim (1 + |x|^{k-1}) (1 + |(x-\mu)/\sigma| ) \\
     & \lesssim 1 + |x|^k.
\end{align*}
Thus, we set $g_1(x) = C(1 + |x|^k) $ for some absolute constant $C$. Note that $g_1(x)$ is clearly bowl-shaped and $\int g_1(x)^r \phi(x) dx$ is finite, since all moments of $\phi$ are finite.
To construct $g_{2, \theta}$, note that
\begin{align*}
f'_\theta(x) &= \frac{1}{\sigma} f'\left( \frac{x - \mu}{\sigma} \right), \quad \text{and} \quad
\nabla f'_\theta(x) = 
           \left[   \begin{array}{c}
                  -\frac{1}{\sigma^2} f''\left( \frac{x -\mu}{\sigma} \right) \\
            - \frac{1}{\sigma^2} f'\left( \frac{x - \mu}{\sigma} \right) 
        -  \frac{x - \mu}{\sigma^3} f''\left( \frac{x -\mu}{\sigma} \right)  \end{array} \right].
\end{align*}
Therefore, $|f'_\theta(x)| \lesssim 1 + |x|^{k-1}$, and
\begin{align*}
\| \nabla f'_\theta(x) \| &\leq \frac{1}{\sigma^2} \left| f''\left( \frac{x - \mu}{\sigma} \right) \right| + 
                            \frac{1}{\sigma^2} \left| f'\left( \frac{x - \mu}{\sigma} \right) \right| +
                           \frac{1}{\sigma^2} \left| f''\left( \frac{x - \mu}{\sigma} \right) \right| 
     \left| \frac{x - \mu}{\sigma} \right| \\
                     & \stackrel{(a)} \lesssim (1 + |x|^{k-2}) + (1 + |x|^{k-1}) + (1 + |x|^{k-2}) (1 + |x|) \\
                    &  \lesssim 1 + |x|^{k-1},
\end{align*}
where $(a)$ follows because $|f''(x)| \lesssim 1 + |x|^{k-2}$. Thus, we may take $g_{2,\theta}(x) = C( 1 + |x|^{k-1})$. This is clearly bowl-shaped and integrable, as well.

Finally, we prove \textbf{B4}. We have
\begin{equation*}
(\log \phi)'(x) = \begin{cases}
\frac{1}{2} \frac{1}{\sqrt{1 - x }}, & \text{if } x < 0 \\
- \frac{1}{2} \frac{1}{\sqrt{1 + x}}, & \text{if } x > 0.
\end{cases}
\end{equation*}
In particular, $(\log \phi)'(x) \rightarrow 0$ as $|x| \rightarrow \infty$.

We also know that $f'(x) \geq M$ for all $x \leq -c$, and $f'(x) \leq -M$ for all $x \geq c$. If $x \leq - \frac{c}{c_\sigma} - C_\mu$, then $\frac{x - \mu}{\sigma} \leq -c$ and
\[
f'_\theta(x) = \frac{1}{\sigma} f'\left( \frac{x - \mu}{\sigma} \right) \geq \frac{M}{c_\sigma}.
\]
If $x \geq \frac{c}{c_\sigma} + C_\mu$, then $\frac{x - \mu}{\sigma} \geq c$ and
\[
f'_\theta(x) = \frac{1}{\sigma} f'\left( \frac{x - \mu}{\sigma} \right) \leq -\frac{M}{c_\sigma}.
\]
Thus, there exist $c_{s1} < 0$ and $c_{s2} > 0$ such that B4 holds. 

\end{proof}

\begin{proposition}
\label{prop:scale_family}
Let $\exp( f(x))$ be a positive density over $\R^+$, where
\begin{itemize}
\item[(a)] $|f^{(k)}(x)|$ is bounded for some $k \geq 2$, and
\item[(b)] there exist constants $c$ and $M$ such that $f'(x) < - M$ for $x > c$.
\end{itemize}
Let $\theta = \sigma$ and $\Theta = [\frac{1}{c_{\sigma}}, c_{\sigma}]$ for some absolute constant $c_{\sigma}$, and let
\[
f_\theta (x) = f\left( \frac{x}{\sigma} \right) - \log \sigma.
\]
Then $\{ f_\theta(x) \}_{\theta \in \Theta}$ satisfies Assumptions B1--B4 with respect to $\phi$ defined in equation~\eqref{eqn:phi_defn1}.
\end{proposition}

The proof is almost identical to that of proposition~\ref{prop:scale_location_family}. 

\begin{proposition}
\label{prop:gamma_family}

Let $\theta = (\alpha, \beta)$ and $\Theta = [\frac{1}{c}, c]^2$ for some constant $c$, and let
$$
f_{\theta} = (\alpha - 1) \log x - \beta x + \alpha \log \beta - \log \Gamma(\alpha).
$$
Then  $\{ f_{\theta}(x) \}_{\theta \in \Theta}$ satisfies Assumptions B1--B4 with respect to $\phi$ defined in equation~\eqref{eqn:phi_defn1}.
\end{proposition}

\begin{proof}
We first prove \textbf{B1}. We have $\log \phi(x) = \log \frac{e}{4} - \sqrt{x + 1}$, so
\begin{align*}
\log \phi(x) - f_{\theta}(x) &= - \sqrt{x+1} - (\alpha - 1)\log x + \beta x +  
       \log \frac{e}{4} - \alpha \log \beta - \log \Gamma(\alpha) > - \infty.
\end{align*}

To prove \textbf{B2}, note that $G_\theta = \int (H_\theta f_\theta(x) ) \exp f_\theta(x) dx$, where $H_\theta$ is the Hessian operator. Hence,
\begin{align*}
\nabla f_\theta(x) = \left[ \begin{array}{c}
                   \log x + \log \beta - d_{\alpha} \log \Gamma(\alpha) \\
                   -x + \frac{\alpha}{\beta} 
              \end{array} \right],
\end{align*}
impyling that
\begin{align*}
H_\theta f_\theta(x) =  \left[ \begin{array}{cc}
                 d^2_\alpha \log \Gamma(\alpha) & \frac{1}{\beta} \\
                 \frac{1}{\beta} & - \frac{\alpha}{\beta^2}
              \end{array} \right].
\end{align*}
Therefore, $G_\theta = H_\theta f_\theta(x)$ which is clearly full-rank.

To prove \textbf{B3}, we write
\begin{align*}
\| \nabla f_\theta(x) \| &\leq | \log x | + x + |\log \beta| + | d_\alpha \log \Gamma(\alpha) | + \frac{\alpha}{\beta} \\
       &\stackrel{(a)} \leq |\log x | + x + C,
\end{align*}
where $(a)$ follows because $c \geq \beta, \alpha \geq \frac{1}{c}$. Therefore, we take $g_1(x) = |\log x| + x + C$. Then
\begin{align*}
\int g_1(x)^r \phi(x) dx &= \int_0^\infty (|\log x| + x + C)^r \phi(x) dx \\
        &\lesssim \int_0^\infty |\log x|^r \phi(x) + \int_0^\infty x^r \phi(x) dx.
\end{align*}
Observe that both terms are finite for our choice of $\phi$. Furthermore,
\begin{align*}
f'_\theta(x) = \frac{(\alpha - 1)}{x} - \beta, \quad \text{and} \quad \nabla f'_\theta(x) &= \left[ \begin{array}{c}
                  \frac{1}{x} \\
                    -1
           \end{array} \right],
\end{align*}
implying that $|f'_\theta(x)| \lesssim 1 + x^{-1}$ and $\| \nabla f'_\theta(x) \| \lesssim 1 + x^{-1}$. Thus, $g_{2, \theta} = C( 1 + x^{-1})$ satisfies
\[
\int_0^\infty g_{2,\theta}(x)^{4t} \phi(x) dx \lesssim 1 + \int_0^\infty x^{-4t} \phi(x) dx.
\]
Since $4t < 1$ by assumption, the integral converges.

\textbf{B4} readily follows because $\beta \geq \frac{1}{c} > 0$. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Appendix for Theorem~\ref{thm:lower_bound}}
\label{sec:lower_bound_proof}

We begin by defining some notation. Let $\hat{\sigma}$ denote a clustering algorithm and $A$ denote a weighted network such that $\hat{\sigma}(A) \,:\, [n] \rightarrow [K]$ is the clustering obtained by $\hat{\sigma}$ based on the input $A$. Let 
\[
S_K[ \hat{\sigma}(A), \sigma_0] := \argmin_{\rho \in S_K} d_H(\rho \circ \hat{\sigma}(A), \sigma_0), 
\] 
where $d_H(\cdot, \; \cdot)$ denotes the Hamming distance, and define
\begin{align}
\mathcal{E}[ \hat{\sigma}(A), \sigma_0] := \Big\{ v \,:\, (\rho \circ \hat{\sigma}(A))(v) \neq  \sigma_0(v),\, 
          \text{ for some } \rho \in S_K[\hat{\sigma}(A), \sigma_0]   \Big\}. \label{eqn:error_set_defn}
\end{align}
When $S_K[\hat{\sigma}(A), \sigma_0]$ is a singleton, the set $\mathcal{E}[\hat{\sigma}(A), \sigma_0]$ contains all nodes misclustered by $\hat{\sigma}(A)$ in relation to $\sigma_0$.
When $S_K[\hat{\sigma}(A), \sigma_0]$ contains multiple elements,
we continue to call $\mathcal{E}[\hat{\sigma}(A), \sigma_0]$ the set of \emph{misclustered} nodes.

\subsection{Proof of Theorem~\ref{thm:lower_bound}}
\label{sec:actual_lower_bound_proof}
Throughout this proof, let $C$ denote a $\Theta(1)$ sequence whose value may change from instance to instance. Let
\[
\tilde{l}(\hat{\sigma}(A), \sigma_0) = \frac{1}{n} 
     \sum_{v=1}^n \mathbbm{1}\{ v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0]\},
\]
where $\mathcal{E}[\hat{\sigma}(A), \sigma_0]$ is defined in equation~\eqref{eqn:error_set_defn}. In particular, note that if $|S_K[\hat{\sigma}(A), \sigma_0]| = 1$, we have $\tilde{l} = l$. We have the following claims: \\

\noindent{\textbf{Claim 1:}} If $\frac{nI}{K} \rightarrow \infty$, then $\E \tilde{l}(\hat{\sigma}(A), \sigma_0) \geq C\exp \left( - (1+o(1)) \frac{nI}{\beta K} \right)$.

\noindent{\textbf{Claim 2:}} If $\frac{nI}{K} \leq c < \infty$, then $\E \tilde{l}(\hat{\sigma}(A), \sigma_0) \geq c' > 0$, for some constants $c$ and $c'$. \\

\noindent We first prove that the theorem follows from the claims. If $P\left( l(\hat{\sigma}(A), \sigma_0) \geq \frac{1}{2\beta K} \right) \geq \frac{1}{2} \E \tilde{l}(\hat{\sigma}(A), \sigma_0)$, we have
\begin{align*}
\E l(\hat{\sigma}(A), \sigma_0) &\geq \frac{1}{2 \beta K} P\left( l(\hat{\sigma}(A), \sigma_0) \geq \frac{1}{2\beta K} \right) \geq \frac{1}{4 \beta K}  \E \tilde{l}(\hat{\sigma}(A), \sigma_0).
     %&\geq \frac{1}{2 \beta K} \exp \left( - (1+o(1)) \frac{nI}{\beta K} \right) \\
     %&\geq \exp \left( - (1+o(1)) \frac{nI}{\beta K} \right) 
\end{align*}
On the other hand, if $P\left( l(\hat{\sigma}(A), \sigma_0) \geq \frac{1}{2\beta K} \right) < \frac{1}{2} \E \tilde{l}(\hat{\sigma}(A), \sigma_0)$, we have
\begin{align*}
\E l(\hat{\sigma}(A), \sigma_0) &\geq \E\left[ l(\hat{\sigma}(A), \sigma_0) 
       \; \Big | \; l(\hat{\sigma}(A), \sigma_0) < \frac{1}{2 \beta K} \right] 
         P\left(  l(\hat{\sigma}(A), \sigma_0) < \frac{1}{2 \beta K} \right) \\
  & \stackrel{(a)}{=} \E\left[ \tilde{l}(\hat{\sigma}(A), \sigma_0) 
       \; \Big | \; l(\hat{\sigma}(A), \sigma_0) < \frac{1}{2 \beta K}\right]
         P\left(  l(\hat{\sigma}(A), \sigma_0) < \frac{1}{2 \beta K} \right) \\
  &= \E \tilde{l}(\hat{\sigma}(A), \sigma_0) -  
      \E\left[ \tilde{l}(\hat{\sigma}(A), \sigma_0) 
       \; \Big | \; l(\hat{\sigma}(A), \sigma_0) \geq \frac{1}{2 \beta K} \right] 
         P\left(  l(\hat{\sigma}(A), \sigma_0) \geq \frac{1}{2 \beta K} \right) \\
  &\geq \E \tilde{l}(\hat{\sigma}(A), \sigma_0) - \frac{1}{2}  \E \tilde{l}(\hat{\sigma}(A), \sigma_0) \\
  &= \frac{1}{2}  \E \tilde{l}(\hat{\sigma}(A), \sigma_0),
  %&\geq \exp\left( - (1 + o(1)) \frac{nI}{\beta K} \right)
\end{align*}
where $(a)$ holds by invoking Lemma~\ref{lem:consensus_uniqueness}.
Thus, any lower bound on $\E \tilde l(\hat \sigma(A), \sigma_0)$ translates into a lower bound on $\E l(\hat \sigma(A), \sigma_0)$ scaled by a suitable constant, implying the desired result.

We now focus on proving the claims. Without loss of generality, suppose clusters 1 has size $\frac{n}{\beta K} + 1$ and cluster 2 has size $\frac{n}{\beta K}$. Also suppose nodes 1 and 2 are such that $\sigma_0(1)= 1$ and $\sigma_0(2) = 2$. Let $C_i = \{u : \sigma_0(u) = i\}$ denote the $i^\text{th}$ cluster.
%For convenience, let us use $v_1$ to denote node 1.

Let $\sigma_0^1 := \sigma_0$, and let $\sigma_0^2 \,:\, [n] \rightarrow [K]$ be the cluster assignment satisfying $\sigma_0^2(v) = \sigma_0(v)$ for all $v \neq 1$, and $\sigma_0^2(1) = 2$. Let $\sigma^*$ be a random cluster assignment, where 
\[
\sigma^* = \left \{
     \begin{array}{cc}
     \sigma^1_0,  &  \trm{with probability } \frac{1}{2}, \\
     \sigma^2_0, & \trm{with probability } \frac{1}{2}.
    \end{array} \right.
\]
Let $\Phi$ denote the probability measure on $(\sigma^*, A, \hat{\sigma}(A))$, defined by
\[
P_{\Phi}(\sigma^*, A, \hat{\sigma}(A)) = P(\sigma^*) P_{SBM}(A \given \sigma^*) P_{alg}( \hat{\sigma}(A) \given A), 
\]
where $P_{SBM}( A \given \sigma^*)$ is the measure on the weighted graph defined by the weighted SBM treating $\sigma^*$ as the true cluster assignment, and $P_{alg}$ represents any randomness in the clustering algorithm. Let $\Psi$ denote an alternative probability measure defined by
\[
P_{\Psi} (\sigma^*, A, \hat{\sigma}(A)) = P(\sigma^*) P_{\Psi}(A  \given \sigma^*) P_{alg}( \hat{\sigma}(A) \given A),
\]
where $P_{\Psi}( A \given \sigma^*)$ is defined as follows:
\begin{enumerate}
\item If $u,v \neq 1$, then $A_{uv}$ is distributed just as in $P_{SBM}( A \given \sigma^*)$. 
\item If $v = 1$ and $u \notin C_1 \cup C_2$, then $A_{uv}$ is distributed just as in $P_{SBM}( A \given \sigma^*)$.
\item If $v = 1$ and $u \in C_1 \cup C_2$, then $A_{uv}$ is distributed as $Y^*$, where $Y^*$ is the distribution that minimizes $D$ in Lemma~\ref{lem:information_equivalence}; i.e., $Y_0^* \propto (P_0 Q_0)^{1/2}$ and $(1-Y_0^*) y^*(x) \propto \sqrt{(1-P_0)p(x)(1-Q_0) q(x)}$.
\end{enumerate}
Note that $P_{\Psi}(A \given \sigma^*) = P_{\Psi}(A)$ actually does not depend on whether $\sigma^* = \sigma_0^1$ or $\sigma_0^2$.

Furthermore, we have
\begin{align*}
\mathcal{Q} := & \log \frac{d P_{\Psi}}{d P_{\Phi}} = \log \frac{ d P_{SBM}(A \given \sigma^*)}{ d P_{\Psi} (A \given \sigma^*)} \\
      =& \sum_{u \in C_{\sigma^*(1)}} \log \frac{Y(A_{u, 1})}{P(A_{u, 1})} + \sum_{u \in C_1 \cup C_1 \backslash C_{\sigma^*(1)}  } \log \frac{Y(A_{u, 1})}{Q(A_{u, 1})},
\end{align*}
where we use the notation $P(A_{u, 1}) = P_0$ if $A_{u, 1} = 0$ and $P(A_{u, 1}) = (1-P_0)p(A_{u, 1})$ if $A_{u, 1} \neq 0$, and similarly for $Y$. Let
$$E = \Big\{1 \notin\mathcal{E}[\hat{\sigma}(A), \sigma^*] \trm{ and } \tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K} \Big\}.$$ 
%where $\mathcal{E}[\hat{\sigma}(A), \sigma^*]$ is the set of vertices incorrectly clustered by $\hat \sigma(A)$. We have
For an arbitrary function $f(n)$ to be defined later, we may write
\begin{align}
P_{\Psi}( \mathcal{Q} \leq f(n) ) & = P_{\Psi}( \mathcal{Q} \leq f(n),\, \neg E) + P_{\Psi}( \mathcal{Q} \leq f(n),\, E).
%& =
%P_{\Psi}\left( \mathcal{Q} \leq f(n), 1 \in \mathcal{E}[\hat{\sigma}(A), \sigma^*] 
%       \trm{ or } \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K}\right) \nonumber \\
%& \quad +  
%P_{\Psi}\left( \mathcal{Q} \leq f(n), 1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma^*] 
%        \trm{ and } \tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K}\right).
\label{eqn:psi_Q_bound}
\end{align}
We bound the first term as follows:
\begin{align*}
P_{\Psi}( \mathcal{Q} \leq f(n), \neg E) &= \int_{\mathcal{Q} \leq f(n), \neg E} d P_{\Psi} = \int_{\mathcal{Q} \leq f(n), \neg E} \exp(\mathcal{Q}) d P_{\Phi} \\
    &\leq \exp(f(n)) P_{\Phi}( \mathcal{Q} \leq f(n), \neg E) \\
    &\leq \exp(f(n)) P_{\Phi}(\neg E) \\
%    &= \exp(f(n)) P_{\Phi}\Big(v_1 \in \mathcal{E}[\hat{\sigma}(A), \sigma^*] 
%       \trm{ or } \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K} \Big)  \\
    &\leq \exp(f(n)) \left( P_{\Phi}(1 \in \mathcal{E}[\hat{\sigma}(A), \sigma^*]) + 
               P_{\Phi} \left( \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K}  \right) \right).
\end{align*}
%By Lemma~\ref{lem:permutation_equivariance_symmetry}, we have
Furthermore,
\begin{align*}
 \E_{\Phi}  \tilde{l}(\hat{\sigma}(A), \sigma^*) &= 
    \frac{1}{n} \sum_{v=1}^n P_{\Phi}( v \in \mathcal{E}[\hat{\sigma}(A), \sigma^*]) \\
  &\geq \frac{1}{n} \sum_{v \in C_{\sigma^*(1)}} P_{\Phi}( v \in \mathcal{E}[\hat{\sigma}(A), \sigma^*]) \\
  &\stackrel{(a)}= \frac{|C_{\sigma^*(1)}|}{n} P_{\Phi}(1 \in \mathcal{E}[\hat{\sigma}(A), \sigma^*]) \\
  &\geq \frac{1}{ \beta K} P_{\Phi}(1 \in \mathcal{E}[\hat{\sigma}(A), \sigma^*]),
\end{align*}
where $(a)$ follows from Corollary~\ref{cor:permutation_equivariance_symmetry_sbm}, and
\begin{align*}
\E_{\Phi} \tilde{l}(\hat{\sigma}(A), \sigma^*) &\geq  
         \E_{\Phi}\Big[ \tilde{l}(\hat{\sigma}(A), \sigma^*) \,\Big|\, 
        \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K}\Big] 
              P_\Phi\left( \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K} \right) \\
    &\geq \frac{1}{4\beta K} P_\Phi\left( \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{4\beta K} \right).
\end{align*}
so we have the bound
\begin{align*}
P_{\Psi}( \mathcal{Q} \leq f(n), \neg E) &\leq 
     \exp( f(n)) \cdot 5 \beta K \cdot \E_{\Phi} \tilde{l}(\hat{\sigma}(A), \sigma^*).
\end{align*}

We now turn to the second term in equation~\eqref{eqn:psi_Q_bound}. We have
\begin{multline}
\label{EqnSponsor}
P_\Psi(E) = \frac{1}{2} P_{\Psi}\left(1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^1] 
\trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^1)  \leq \frac{1}{4\beta K} \right) \\
%
+ \frac{1}{2} P_{\Psi}\left(1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^2]  
\trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^2)  \leq \frac{1}{4\beta K}  \right).
\end{multline}
%We observe that under $\Psi$, the distribution over $A$ does not depend on $\sigma^*$. Therefore, we have
%\begin{align*}
%&P_{\Psi}\left(1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma^*]   
%    \trm{ and }  \tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K} \right) \\
%& =  \frac{1}{2} P_{\Psi}\left(1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma^*] 
%     \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K} 
%        \Big | \sigma^* = \sigma_0^1 \right) \\
% &\quad + \frac{1}{2} P_{\Psi}\left(1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma^*]
%       \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K} 
%       \Big | \sigma^* = \sigma_0^2 \right) \\
%      &= \frac{1}{2} P_{\Psi}\left(1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^1] 
%                 \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^1)  \leq \frac{1}{4\beta K} \right) \\  &\quad + \frac{1}{2} P_{\Psi}\left(1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^2]  
%          \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^2)  \leq \frac{1}{4\beta K}  \right).
%\end{align*}
If $l(\hat{\sigma}(A), \sigma_0^1) \leq \tilde{l}(\hat{\sigma}(A), \sigma_0^1) \leq \frac{1}{4 \beta K}$, Lemma~\ref{lem:consensus_uniqueness} implies $S_K[\hat{\sigma}(A), \sigma_0^1]$ contains only one element, which we denote by $\rho$. Since $d_H(\sigma_0^1, \sigma_0^2) = 1$, we have $\frac{1}{n} d_H(\rho \circ \hat{\sigma}(A), \sigma_0^2) \leq \frac{1}{4 \beta K} + \frac{1}{n} \leq \frac{1}{2 \beta K}$, so applying Lemma~\ref{lem:consensus_uniqueness} again, we conclude that $\rho \in S_K[\hat{\sigma}(A), \sigma_0^2]$, as well. However, $(\rho \circ \hat{\sigma}(A) )(1) $ cannot be equal to both $\sigma_0^1(1) = 1$ and $\sigma_0^2(1) = 2$. Hence, we cannot simultaneously have $1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^1]$ and $1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^2]$. In particular, the two events in equation~\eqref{EqnSponsor} are disjoint, so
%\[
%\mathbbm{1}\left\{1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^1]  
%   \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^1)  \leq \frac{1}{4\beta K}  \right\} +  
%\mathbbm{1}\left\{1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^2] 
%   \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^2)  \leq \frac{1}{4\beta K}  \right\}
% \leq 1,
%\]
%\[
%P_{\Psi}\left\{1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^1]  
%   \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^1)  \leq \frac{1}{4\beta K}  \right\} +  
%P_{\Psi}\left\{1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma_0^2] 
%   \trm{ and }\tilde{l}(\hat{\sigma}(A), \sigma_0^2)  \leq \frac{1}{4\beta K}  \right\}
% \leq 1.
%\]
%Hence,
%\[
%P_{\Psi}(E) = P_{\Psi}\left(1 \notin \mathcal{E}[\hat{\sigma}(A), \sigma^*]   
%    \trm{ and }  \tilde{l}(\hat{\sigma}(A), \sigma^*) \leq \frac{1}{4\beta K} \right) \leq \frac{1}{2},
%\]
%so
\[
P_{\Psi}(\mathcal{Q} \leq f(n), E) 
             \leq P_{\Psi}(E) \leq \frac{1}{2}.
\]
Plugging back into equation~\eqref{eqn:psi_Q_bound}, we conclude that
\[
P_{\Psi}(\mathcal{Q} \leq f(n)) \leq \exp(f(n)) \cdot 5 \beta K \cdot \E_{\Phi} \tilde{l}(\hat{\sigma}(A) , \sigma^*) + \frac{1}{2},
\]
so setting $f(n) = \log \frac{1}{20 \beta K  \E_{\Phi} \tilde{l}(\hat{\sigma}(A), \sigma^*)}$, we have
\[
P_{\Psi}\left(\mathcal{Q} \leq \log \frac{1}{20 \beta K \E_{\Phi} l(\hat{\sigma}, \sigma^*)} \right) \leq \frac{3}{4}.
\]
By Chebyshev's inequality, we also have
\[
P_{\Psi}\left( \mathcal{Q} \leq \E_{\Psi} \mathcal{Q} + \sqrt{ 5 V_{\Psi}(\mathcal{Q})} \right) \geq 4/5,
\]
where $V_\Psi(Q)$ is the variance of $Q$ under $\Psi$. Hence, $\log \frac{1}{20 \beta K \E_{\Phi} \tilde{l}(\hat{\sigma}(A),\sigma_0)} \leq \E_{\Psi} \mathcal{Q} + \sqrt{ 5 V_{\Psi}(\mathcal{Q})}$, or equivalently,
\[
\E_{\Phi} \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq \frac{1}{20 \beta K} \exp\Big( - (\E_{\Psi} \mathcal{Q} + \sqrt{ 5 V_{\Psi}(\mathcal{Q})} ) \Big).
\]
We now compute $\E_{\Psi} \mathcal{Q}$ and $V_{\Psi}(\mathcal{Q})$. Note that
\[
\E_{\Psi} \mathcal{Q} = \frac{1}{2} \E_{\Psi} [ \mathcal{Q} \given \sigma^* = \sigma_0^1 ] + 
              \frac{1}{2} \E_{\Psi} [ \mathcal{Q} \given \sigma^* = \sigma_0^2 ].
\]
Furthermore, by Lemma \ref{lem:information_equivalence}, we have
\begin{align*}
\E_{\Psi} [\mathcal{Q} \given \sigma^* = \sigma_0^1] &= \E_{\Psi} \left[\sum_{u: \, u \neq 1,\, \sigma_0^1(u) = 1} \log \frac{Y(A_{u, 1})}{P(A_{u, 1})} + \sum_{u :\, \sigma_0^1(u) = 2} \log \frac{Y(A_{u, 1})}{Q(A_{u, 1})}\right]  \\
     &= \frac{n}{\beta K}  \int \log \frac{dY}{dP} dY + \frac{n}{\beta K} \int \log \frac{dY}{dQ} dY \\
     &= \frac{n}{\beta K} 2 D = \frac{n}{\beta K} I.
\end{align*}
Similarly, we have $\E_{\Psi}[ \mathcal{Q} \given \sigma^* = \sigma_0^2] = \frac{n I}{\beta K}$, so $\E_{\Psi} \mathcal{Q} = \frac{n I}{\beta K}$. We show in Lemma \ref{lemma: attack variance} that the following bound holds for the variance:
$$\sqrt{5 V_{\Psi} (\mathcal{Q})} \leq C \sqrt{ \frac{n I}{\beta K}}.$$

Now note that if $\frac{nI}{\beta K} \rightarrow \infty$, we have $\sqrt{\frac{nI}{\beta K}} = o \left( \frac{nI}{\beta K} \right)$, so $\sqrt{ 5 V_{\Psi}(\mathcal{Q})} = o \left( \frac{nI}{\beta K} \right)$. Therefore,
\begin{equation*}
\E_{\Phi} \tilde{l}(\hat{\sigma}(A), \sigma^*) \geq C\exp \left( - (1 + o(1)) \frac{nI}{\beta K} \right).
\end{equation*}
If instead $\frac{nI}{K} \rightarrow c < \infty$, then $\E_{\Psi} \mathcal{Q} = c (1 + o(1))$ and $\sqrt{ 5 V_{\Psi}(\mathcal{Q})} \leq C ( 1 + o(1))$, so
\begin{equation*}
\E_{\Phi} \tilde{l}(\hat{\sigma}(A), \sigma^*) \ge c' > 0,
\end{equation*}
for some constant $c'$.

Now define two measures $P_1, P_2$ on $(A, \hat{\sigma}(A))$, as follows:
\begin{align*}
P_1(A, \hat{\sigma}(A)) &= 
    P_{SBM}( A \given \sigma_0^1) P_{alg}( \hat{\sigma}(A) \given A), \\
P_2(A, \hat{\sigma}(A)) &= P_{SBM}( A \given \sigma_0^2) P_{alg}( \hat{\sigma}(A) \given A).
\end{align*}
Note that $\E_{\Phi}[ \tilde{l}(\hat{\sigma}, \sigma^*) \given \sigma^* = \sigma_0^1] = \E_1 \tilde{l}(\hat{\sigma}, \sigma_0^1) $ and $\E_{\Phi}[ \tilde{l}(\hat{\sigma}, \sigma^*) \given \sigma^* = \sigma_0^2] = \E_2 \tilde{l}(\hat{\sigma}, \sigma_0^2)$, where $\E_1$ and $\E_2$ are expectations taken with respect to $P_1$ and $P_2$, respectively. We claim that $\E_1 \tilde{l}(\hat{\sigma}, \sigma_0^1) = \E_2 \tilde{l}(\hat{\sigma}, \sigma_0^2)$, in which case $\E_{\Phi} l(\hat{\sigma}, \sigma^*) = \E_1 \tilde{l}(\hat{\sigma}, \sigma^1_0) = \E \tilde{l}(\hat{\sigma}, \sigma_0)$ and the claims follow.
%Thus, we devote the remainder of this section to proving the claim.

Define a permutation $\pi \in S_n$ that swaps $\{2, \dots, \frac{n}{\beta K} + 1\}$ with $\{ \frac{n}{\beta K} + 2, \dots, 2 \frac{n}{\beta K} + 1\}$ and satisfies $\pi(u) = u$ for $u = 1$ and $u \geq 2 \frac{n}{\beta K} + 2$. Clearly, $\sigma_0^2 = \tau \circ \sigma_0^1 \circ \pi^{-1}$, where $\tau \in S_K$ swaps cluster labels 1 and 2. Now let $A$ be fixed and let $\rho \in S_K$ be arbitrary. We have
\begin{align*}
d_H( \rho \circ \hat{\sigma}(A), \sigma_0^1) &= 
  d_H( \rho \circ \hat{\sigma}(A), \tau^{-1} \circ \sigma_0^2 \circ \pi)\\
  &= d_H( \rho \circ \hat{\sigma}(A) \circ \pi^{-1}, 
                     \tau^{-1} \circ \sigma_0^2) \\
  &= d_H( \rho \circ \xi^{-1} \circ \hat{\sigma}(\pi A), 
              \tau^{-1} \circ \sigma_0^2) \\
  &= d_H( \tau \circ \rho \circ \xi^{-1} \circ \hat{\sigma}(\pi A), \sigma_0^2).
\end{align*}
Therefore, $\rho \mapsto \tau \circ \rho \circ \xi^{-1}$ is a bijection between $S_K[\hat{\sigma}(A), \sigma_0^1]$ and $S_K[\hat{\sigma}(\pi A), \sigma_0^2]$. Furthermore, if $v$ is a node such that $(\rho \circ \hat{\sigma}(A))(v) \neq \sigma_0^1(v)$, we equivalently have
\begin{align*}
(\rho \circ \hat{\sigma}(A))(v) &\neq (\tau^{-1} \circ \sigma_0^2 \circ \pi) (v) \iff (\rho \circ \hat{\sigma}(A) \circ \pi^{-1} )(u) &\neq (\tau^{-1} \circ \sigma_0^2 )(u), \quad \textrm{where $\pi(v) = u$},
\end{align*}
so $(\tau \circ \rho \circ \xi^{-1} \circ \hat{\sigma}(\pi A)) (u) \neq
 \sigma_0^2(u)$. Thus, $v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0^1]$ if and only if $\pi(v) \in \mathcal{E}[\hat{\sigma}(\pi A), \sigma_0^2]$.  Finally, we conclude that
\begin{align*}
\E_1 \tilde{l}(\hat{\sigma}(A), \sigma_0^1) &= 
      \frac{1}{n} \sum_{v = 1}^n P_1 ( v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0^1]) \\
   &= \frac{1}{n} \sum_{v=1}^n  P_1 ( \pi(v) \in \mathcal{E}[\hat{\sigma}(\pi A), \sigma_0^2]) \\
   &\stackrel{(a)} = \frac{1}{n} \sum_{v=1}^n P_2( \pi(v) \in  \mathcal{E}[\hat{\sigma}(A), \sigma_0^2] ) \\
   &= \E_2 \tilde{l} (\hat{\sigma}(A), \sigma_0^2),
\end{align*}
where $(a)$ follows because $[\pi A]_{ij} = A_{\pi^{-1}(i), \pi^{-1}(j)}$, implying that if $A$ is distributed according to $P_{SBM}(A \given \sigma_0^1)$, then $\pi A$ is distributed according to $P_{SBM}(A \given \sigma_0^2)$. This concludes the proof.

%%%%%

\subsection{Properties of permutation-equivariant estimators}

Permutation-equivariant estimators possess symmetry properties. The following lemma formalizes one symmetry property useful for the proof of Theorem~\ref{thm:lower_bound}: 
\begin{lemma}
\label{lem:permutation_equivariance_symmetry}
Let the true clustering $\sigma_0$ be arbitrary. Suppose the weight matrix $A$ is drawn from an arbitrary probability measure and $\hat{\sigma}$ is any permutation-equivariant estimator. Let $u$ and $v$ be two nodes such that there exists $\pi \in S_n$ satisfying
\begin{itemize}
\item[(1)] $\pi(u) = v$,
\item[(2)] $\pi$ is measure-preserving; i.e., $A \stackrel{d}{=} \pi A$, and
\item[(3)] $\pi$ preserves the true clustering; i.e., there exists $\tau \in S_K$ such that $\tau \circ \sigma_0 \circ \pi^{-1} = \sigma_0$.
\end{itemize}
Then
\[
P( u \in \mathcal{E}[\hat{\sigma}(A), \sigma_0]) = P( v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0] ).
\]
\end{lemma}

\begin{proof}
Since $\hat{\sigma}(A) \stackrel{d}{=} \hat{\sigma}(\pi A)$, we have
%  \textcolor{red}{Don't we need a lemma which says that for any fixed $A$, the event $v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0 ]$ has the same probability as the event $v \in \mathcal{E}[\hat{\sigma}(\pi (A)), \sigma_0]$?} 
\[
P \Big(v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0 ] \Big) = 
       P \Big(v \in \mathcal{E}[\hat{\sigma}(\pi (A)), \sigma_0] \Big).
\]
We claim that $u \in \mathcal{E}[\hat{\sigma}(A), \sigma_0]$ if and only if $v \in \mathcal{E}[\hat{\sigma}(\pi A), \sigma_0]$, implying the desired result:
\begin{align*}
P\Big( u \in \mathcal{E}[\hat{\sigma}(A), \sigma_0]\Big) &= P\Big(v \in \mathcal{E}[ \hat{\sigma}(\pi A), \sigma_0]\Big) = P\Big(v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0]\Big).
\end{align*}

Consider a fixed matrix $A$, and let $\tau \in S_K$ satisfy $\tau \circ \sigma_0 \circ \pi^{-1} = \sigma_0$. Let $\xi \in S_K$ be the permutation such that $\hat{\sigma}(\pi A) = \xi \circ \hat{\sigma}(A) \circ \pi^{-1}$. For any $\rho \in S_K$, we have
\begin{align*}
d_H(\rho \circ \hat{\sigma}(A), \; \sigma_0) &= 
      d_H( \tau \circ \rho \circ \xi^{-1} \circ \xi \circ \hat{\sigma}(A) \circ \pi^{-1}, \; \tau \circ \sigma_0 \circ \pi^{-1} ) \\
  &= d_H( \tau \circ \rho \circ \xi^{-1} \circ \hat{\sigma}(\pi A), \; \sigma_0).
\end{align*}
%where the second equality follows from the assumption that $\hat{\sigma}$ is permutation equivariant.
Therefore, $\rho \in S_K[\hat{\sigma}(A), \sigma_0]$ if and only if $\tau \circ \rho \circ \xi^{-1} \in S_K[\hat{\sigma}(\pi(A)), \sigma_0]$. In particular, if $v \in \mathcal{E}[\hat{\sigma}(\pi A), \sigma_0]$, we have $\tau \circ \rho \circ \xi^{-1} \circ \hat{\sigma}(\pi A)(v) \neq \sigma_0(v)$ for some $\rho \in S_K[\hat{\sigma}(A), \sigma_0]$. Then
\begin{align*}
\hat{\sigma}(A)(u) &= \hat{\sigma}(A)(\pi^{-1}(v)) = \xi^{-1} \circ \xi \circ \hat{\sigma}(A) \circ \pi^{-1} (v) = \xi^{-1} \circ \hat{\sigma}( \pi A) (v) \\
   &\neq \rho^{-1} \circ \tau^{-1} \circ \sigma_0(v) = \rho^{-1} \circ \tau^{-1} \circ \sigma_0( \pi( u )) = \rho^{-1} (\sigma_0(u)).
\end{align*}
%where $(a)$ follows from the assumption that $\hat{\sigma}$ is permutation-equivariant, and $(b)$ follows because $\tau \circ \sigma_0 \circ \pi^{-1} = \sigma_0$ implies that $\sigma_0 = \tau^{-1} \circ \sigma_0 \circ \pi$. 
%Thus, $v \in \mathcal{E}[ \hat{\sigma}(\pi A), \sigma_0]$ implies that $\rho^* \circ \hat{\sigma}(A)(u) \neq \sigma_0(u)$ for some $\rho^* \in S_K[\hat{\sigma}(A), \sigma_0]$, which means that 
Thus, $u \in \mathcal{E}[ \hat{\sigma}(A), \sigma_0]$. Similar reasoning shows that if $u \in \mathcal{E}[ \hat{\sigma}(A), \sigma_0]$, then $v \in \mathcal{E}[\hat{\sigma}(\pi A), \sigma_0]$.
\end{proof}

\begin{corollary}
\label{cor:permutation_equivariance_symmetry_sbm}
Let the true clustering $\sigma_0$ be arbitrary. Suppose the weight matrix $A$ is drawn from an arbitrary probability measure and $\hat{\sigma}$ is any permutation-equivariant estimator.  Let $u$ and $v$ be two nodes lying in equally-sized clusters. Then
\[
P\Big( u \in \mathcal{E}[\hat{\sigma}(A), \sigma_0]\Big) = P\Big( v \in \mathcal{E}[\hat{\sigma}(A), \sigma_0] \Big).
\]
\end{corollary}

\begin{proof}
By Lemma~\ref{lem:permutation_equivariance_symmetry}, it suffices to construct a permutation $\pi \in S_n$ satisfying conditions (1)--(3).

First suppose $u$ and $v$ lie in the same cluster. It is easy to see that the conditions are satisfied when $\pi$ is the permutation that transposes $u$ and $v$ and $\tau$ is the identity.

If $u$ and $v$ lie in different clusters, suppose without loss of generality that $u$ is in cluster 1 and $v$ is in cluster 2, where clusters 1 and 2 have the same size. Let $\pi$ be the permutation that exchanges all nodes in cluster 1 with all nodes in cluster 2. The conditions are satisfied when $\tau$ is the permutation that transposes cluster labels 1 and 2. 
\end{proof}

\subsection{Properties of Renyi divergence}

We first state a lemma that provides an alternative characterization of the Renyi divergence: 

\begin{lemma}
\label{lem:information_equivalence}
Let $P$ and $Q$ be two probability measures on $\R$ that are absolutely continuous with respect to each other, with respective point masses $P_0$ and $Q_0$ at zero. The Renyi divergence satisfies $I = 2D$, where
%\[
%I = - 2 \log \int \left( \frac{dP}{dQ} \right)^{1/2} dQ, \qquad  \text{and} \quad
\begin{equation*}
D := \inf_{Y \in \mathcal{P}} \max \left\{ \int \log \frac{dY}{dP} dY, \, \int \log \frac{dY}{dQ} dY \right\},
\end{equation*}
and $\mathcal{P}$ denotes the set of probability measures absolutely continuous with respect to both $P$ and $Q$.
\end{lemma}

\begin{proof} 
First, note that $D$ is finite by making the choice $Y = P$. We claim that
\begin{align}
\label{EqnMeteor}
D = \inf_{Y \in \mathcal{P}} \left\{\int \log \frac{dY}{dP} dY: \; \int \log \frac{dP}{dQ} dY = 0\right\}.
\end{align}
This is because for any $Y \in \mathcal{P}$ such that $\int \log \frac{dP}{dQ} dY \neq 0$, 
we have $\int \log \frac{dY}{dP} dY \neq \int \log \frac{dY}{dQ} dY$. 
Suppose without loss of generality that the first quantity is larger. Then it is possible to take $\tilde{Y} = (1 - \epsilon) Y + \epsilon P$ for $\epsilon$ small enough such that $ \max \left\{ \int \log \frac{d\tilde{Y}}{dP} d\tilde{Y}, \, \int \log \frac{d\tilde{Y}}{dQ} d\tilde{Y} \right\}$ strictly decreases, so the infimum in the definition of $D$ could not have been achieved.

Since the new formulation~\eqref{EqnMeteor} is convex in $Y$, we may solve to obtain the optimal $Y^* \in \mathcal{P}$, defined by $Y_0^* = \frac{P_0^{1/2} Q_0^{1/2}}{Z}$ and $(1-Y_0^*) y^*(x) = \frac{((1-P_0) \cdot p(x))^{1/2} ((1-Q_0) \cdot q(x))^{1/2}}{Z}$. The quantity $Z$ is the normalization term: $Z = P_0^{1/2} Q_0^{1/2} + \int \sqrt{ (1-P_0) p(x) (1-Q_0) q(x)} dx$. 
Then
\begin{align*}
\int \log \frac{dY^*}{dP} dY^*  &= \log \frac{1}{Z} \left\{ 
           \left( \frac{Q_0}{P_0} \right)^{1/2}  Y_0^* + \int \left(\frac{(1-P_0) p(x)}{(1-Q_0)q(x)} \right)^{1/2} (1-Y_0^*) y^*(x) dx \right\} \\
  &= \log \frac{dP}{dQ} dY^* - \log Z = - \log Z. 
\end{align*}
It is straightforward to verify that $-2 \log Z = I$. 
\end{proof}

\subsection{Bounding the variance}
\begin{lemma}\label{lemma: attack variance}
For a suitable constant $C$, we have $\sqrt{5 V_{\Psi} (\mathcal{Q})} \leq C \sqrt{ \frac{n I}{\beta K}}$.
\end{lemma}
\begin{proof}
We begin with the decomposition
\begin{align*}
V_{\Psi}(\mathcal{Q}) &= V( \E_{\Psi}[ \mathcal{Q} \given \sigma^* ] ) + 
              E[ V_{\Psi}( \mathcal{Q} \given \sigma^* ) ] \\
      &=   E[ V_{\Psi}( \mathcal{Q} \given \sigma^* ) ] \\
      &=  \frac{1}{2} V_{\Psi}( \mathcal{Q} \given \sigma^* = \sigma_0^1) + 
          \frac{1}{2} V_{\Psi}( \mathcal{Q} \given \sigma^* = \sigma_0^2).
          \end{align*}
Then
\begin{align*}
V_{\Psi}(\mathcal{Q} \given \sigma^* = \sigma_0^1) &= \sum_{u:\, u \neq 1, \sigma_0^1(u) = 1 } 
         V_{\Psi} \left( \log \frac{Y(A_{v_1 u})}{P(A_{v_1 u})} \right) +
                        \sum_{u :\, \sigma_0^1(u) = 2} V_{\Psi} \left( \log \frac{Y(A_{v_1 u})}{Q(A_{v_1 u})} \right)  \\
   &\leq \frac{n}{\beta K}  \E_{\Psi} \left( \log \frac{Y(A_{v_1 u})}{P(A_{v_1 u})} \right)^2 + 
         \frac{n}{\beta K} \E_{\Psi} \left( \log \frac{Y(A_{v_1 u})}{Q(A_{v_1 u})} \right)^2. 
\end{align*}

We will show that $\E_{\Psi} \left( \log \frac{Y(A_{v_1 u})}{P(A_{v_1 u})} \right)^2$ is bounded by $C I$, so $\sqrt{ 5 V_{\Psi}(\mathcal{Q})} \leq C \sqrt{\frac{nI}{\beta K}} $. We have
\begin{align}
\E_{\Psi} \left( \log \frac{Y(A_{uv^*})}{P(A_{uv^*})} \right)^2 &= 
    \int \left( \log \frac{dY}{dP} \right)^2 dY \nonumber \\
  &= Y_0 \log^2 \frac{Y_0}{P_0} + (1-Y_0) \int y(x) \log^2 \frac{(1-Y_0) y(x)}{(1-P_0)p(x)} dx. \label{eqn:lower_bound_var_terms}
\end{align}
To bound the first term, we write
\begin{align*}
\left| \log \frac{Y_0}{P_0} \right|&= \left| \frac{1}{2} \log \frac{Q_0}{P_0} - \log Z \right| \\
    &\leq \frac{1}{2} \left| \log \left( 1 - \frac{P_0 - Q_0}{P_0} \right) \right| + \frac{I}{2} \\ 
   & \stackrel{(a)} \leq \frac{1}{2} \left| \frac{P_0 - Q_0 }{P_0} \right| + \left| \frac{P_0 - Q_0}{P_0} \right|^2 C + \frac{I}{2} \\   
   & \stackrel{(b)} \leq C \left| \frac{P_0 - Q_0}{P_0} \right| + C I,
\end{align*}
where $(a)$ follows from Lemma~\ref{lem:log_linearize} and the fact that $\frac{Q_0}{P_0}$ is bounded, and $(b)$ follows from the fact that $\left| \frac{ P_0 - Q_0}{P_0} \right| = \left| 1 - \frac{Q_0}{P_0} \right| \leq 1 + \left| \frac{Q_0}{P_0} \right|$ is bounded. Therefore,
\begin{align*}
Y_0 \log^2 \frac{Y_0}{P_0} &\leq Y_0 \left( C \frac{ |P_0 - Q_0| }{P_0} + C I \right)^2 \\
     &\stackrel{(a)} \leq Y_0 \frac{|P_0 - Q_0|^2}{P_0^2 } C + Y_0 I^2 C \\
     &\stackrel{(b)} \leq \frac{|P_0 - Q_0|^2}{P_0} C + I^2 C,
\end{align*}
where $(a)$ follows because $(x + y)^2 \leq 2x^2 + 2y^2$, and $(b)$ follows because $Y_0 = \frac{\sqrt{P_0 Q_0}}{Z} = (1+o(1)) C P_0$. 

Since $I = o(1)$, we have $I^2 \leq I$. Also,
\begin{equation*}
I \geq (1+o(1)) ( \sqrt{P_0} - \sqrt{Q_0})^2 = (1+o(1)) \frac{ (P_0 - Q_0)^2}{(\sqrt{P_0} + \sqrt{Q_0})^2} = C (1 + o(1)) \frac{ (P_0 - Q_0)^2 }{P_0},
\end{equation*}
from which we conclude that $Y_0 \log^2 \frac{Y_0}{P_0} \leq C I$.

Now we turn our attention to the second term in equation~\eqref{eqn:lower_bound_var_terms}. We have
\begin{align*}
\left| \log \frac{(1-Y_0) y(x)}{(1-P_0)p(x)} \right| &\leq 
              \frac{1}{2} \left| \log \frac{ (1-Q_0) q(x)}{(1-P_0)p(x)} - \log Z \right| \\
%
    &\leq \frac{1}{2} \left| \log \frac{1 - Q_0}{1-P_0} \right| + 
               \frac{1}{2} \left| \log \frac{q(x)}{p(x)} \right| + \frac{I}{2}.
\end{align*}
Therefore,
\begin{align}
\label{EqnNoodle}
(1 - Y_0) \int y(x) \left( \log \frac{1 - Y_0}{1 - P_0} \frac{y(x)}{p(x)} \right)^2 dx &\leq 
   (1 - Y_0) \int y(x) \left\{ \frac{1}{2} \left| \log \frac{1-Q_0}{1-P_0} \right| 
              +  \frac{1}{2} \left| \log \frac{q(x)}{p(x)}\right| + \frac{I}{2}\right\}^2 dx \notag \\
  &\leq (1 - Y_0) \int y(x) \left\{ C \left| \log \frac{1-Q_0}{1-P_0} \right|^2 + 
                   C \left| \log \frac{q(x)}{p(x)}\right|^2 + C I^2 \right\} dx,
\end{align}
where we have used the fact that $(x + y + z)^2 \leq 9x^2 + 9y^2 + 9z^2$ in the last inequality.
Define
\begin{align*}
\mathcal{A} & := (1 - Y_0) \int y(x) \left| \log \frac{1-Q_0}{1-P_0} \right|^2 dx, \\
% 
\mathcal{B} & := (1 - Y_0) \int y(x) \left| \log \frac{q(x)}{p(x)}\right|^2 dx, \\
%
\mathcal{C} & := (1 - Y_0) \int y(x) I^2  dx.
\end{align*}
We bound each term separately, beginning with $\mathcal{A}$. Note that
\begin{align*}
\left| \log \frac{1 - Q_0}{1- P_0} \right| &= \left| \log \left( 1 - \frac{Q_0 - P_0}{1 - P_0} \right) \right| \stackrel{(a)} \leq \left| \frac{Q_0 - P_0}{1 - P_0} \right| + C \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 \stackrel{(b)} \leq C \left| \frac{Q_0 - P_0}{1 - P_0} \right|,
\end{align*}
where $(a)$ follows from Lemma~\ref{lem:log_linearize} and the fact that $\frac{1 - Q_0}{1 - P_0}$ is bounded, and $(b)$ follows from the fact that $\left| \frac{Q_0 - P_0}{1 - P_0} \right| = \left| 1 - \frac{1 - Q_0}{1 - P_0} \right| \leq 1 + \left| \frac{1 - Q_0}{1 - P_0} \right| \leq C$. Therefore,
\begin{align*} 
\mathcal{A} & \le C(1 - Y_0) \int y(x) \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 dx \\
%
    &= C \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 \int \frac{\sqrt{(1-P_0)p(x) (1-Q_0)q(x)}}{Z} dx \\
   &\stackrel{(a)} \leq C (1+o(1)) \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 \int \sqrt{\frac{ (1-Q_0 )q(x)}{(1-P_0) p(x)} } (1 - P_0) p(x) dx \\ 
   &\stackrel{(b)} \leq C (1+o(1)) \left( \frac{Q_0 - P_0}{1 - P_0} \right)^2 (1 - P_0) \\
   &\leq C (1 + o(1)) \frac{ (Q_0 - P_0)^2}{1 - P_0},
\end{align*}
where in $(a)$, we use the fact that $\frac{1}{Z} = (1 + o(1))$ since $Z \rightarrow 1$, and in $(b)$, we use the fact that $\frac{1-Q_0}{1-P_0}$ and $\frac{q(x)}{p(x)}$ are bounded. Note that 
\begin{equation*}
I \geq (1+o(1)) (\sqrt{1 - P_0} - \sqrt{1- Q_0})^2 =  (1+o(1)) \frac{ (P_0 - Q_0)^2}{(\sqrt{1 - P_0} + \sqrt{1 - Q_0})^2} = C (1 + o(1)) \frac{ (P_0 - Q_0)^2}{1 - P_0},
\end{equation*}
implying that $\mathcal{A} \leq C ( 1 + o(1)) I \leq C I$.

Moving onto $\mathcal{B}$, first suppose $H = \Theta(1)$ and $\max\left\{\int p(x) \left| \log \frac{q(x)}{p(x)} \right|^2 dx, \; \int q(x) \left| \log \frac{q(x)}{p(x)} \right|^2 dx\right\} < \infty$. We have
\begin{align*}
\mathcal{B} & \le C \int \frac{\sqrt{(1-P_0) p(x) (1-Q_0) q(x)}}{Z} \left| \log \frac{q(x)}{p(x)} \right|^2 dx\\
  &\stackrel{(a)} \leq C \sqrt{(1 - P_0)(1-Q_0)} \int \sqrt{p(x) q(x)}  \left| \log \frac{q(x)}{p(x)} \right|^2 dx\\
  & \leq C \sqrt{(1 - P_0)(1-Q_0)} \int (p(x) + q(x))  \left| \log \frac{q(x)}{p(x)} \right|^2 dx\\
  &\stackrel{(b)} \leq C \sqrt{(1 - P_0)(1-Q_0)} H \leq C I,
\end{align*}
where $(a)$ follows because $Z \rightarrow 1$ and $(b)$ follows because $H = \Theta(1)$. 

Next, we make no assumption on $H$ but assume $\left| \log \frac{q(x)}{p(x)} \right|$ is bounded by a constant. We have
\begin{align*}
\left| \log \frac{q(x)}{p(x)} \right| &= \left| \log \left( 1 - \frac{p(x) - q(x)}{p(x)} \right) \right| \\
    &\stackrel{(a)} \leq \left| \frac{p(x) - q(x)}{p(x)} \right| + \left( \frac{p(x) - q(x)}{p(x)} \right)^2 C \\
  &\stackrel{(b)} \leq C \left| \frac{p(x) - q(x)}{p(x)} \right|,
\end{align*}
where $(a)$ follows from Lemma~\ref{lem:log_linearize} and the fact that $\frac{q(x)}{p(x)}$ is bounded, and $(b)$ follows from the fact that $\left| \frac{p(x) - q(x)}{p(x)} \right| = \left| 1 - \frac{q(x)}{p(x)} \right| \leq 1 + \left| \frac{q(x)}{p(x)} \right| \leq C$. Then
\begin{align*}
\mathcal{B} & \le 
         \frac{C}{Z} \int \sqrt{ \frac{1 - Q_0}{1 - P_0} \frac{q(x)}{p(x)} } (1 - P_0) p(x) 
                 \left( \frac{p(x) - q(x)}{p(x)} \right)^2 dx  \\
   &\stackrel{(a)} \leq \frac{C}{Z} (1 - P_0) \int p(x) \left( \frac{p(x) - q(x)}{p(x)} \right)^2 dx,
\end{align*}
where in $(a)$, we use the facts that $\frac{1}{Z} = (1 + o(1))$ and $\frac{1- Q_0}{1-P_0}$, and $\frac{p(x)}{q(x)}$ are both bounded by assumption. 
Now, note that $H = \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx 
= \int \frac{ (p(x) - q(x))^2 }{(\sqrt{p(x)} + \sqrt{q(x)})^2 } dx = C \int \frac{(p(x) - q(x))^2}{p(x)} dx $. Therefore,
\begin{align*}
\mathcal{B} &\leq C ( 1 - P_0) H \leq C \sqrt{ (1 - P_0)(1 - Q_0)} H \leq C I.
\end{align*}

Finally, note that $\mathcal{C} = (1 - Y_0) C I^2 \leq C I$. Substituting back into inequality~\eqref{EqnNoodle}, we therefore obtain
\begin{align*}
(1 - Y_0) \int y(x) \left( \log \frac{1 - Y_0}{1 - P_0} \frac{y(x)}{p(x)} \right)^2 dx \leq C I,
\end{align*}
%Combining the above bound with
so substituting back into inequality~\eqref{eqn:lower_bound_var_terms}, we obtain the desired bound.
%we return to inequality~\ref{eqn:lower_bound_var_terms} to obtain
%\begin{align*}
%\E_{\Psi} \left( \log \frac{Y(A_{uv^*})}{P(A_{uv^*})} \right)^2 \leq C I.
%\end{align*}
%Hence, $\sqrt{5 V_{\Psi} (\mathcal{Q})} \leq C \sqrt{ \frac{n I}{\beta K}}$. 
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional useful lemmas}



\begin{lemma}
\label{lem:renyi_hellinger}
Let
\begin{align*}
I & = -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0) p(x) q(x)} dx \right), \\
%
I^{h} & = (\sqrt{P_0} - \sqrt{Q_0})^2 + \int \left( \sqrt{ (1-P_0) p(x)} - \sqrt{ (1-Q_0) q(x)} \right)^2 dx.
\end{align*}
If $I^h < 2 - 2\epsilon$, then $I = I^h(1+\eta)$, where $|\eta| \leq \frac{I^h}{2\epsilon}$. Thus, $I \rightarrow 0$ if and only if $I^h \rightarrow 0$, in which case $I = I^h(1+o(1))$.

\end{lemma}

\begin{proof}
We have
\begin{align*}
I &= -2 \log \left( \sqrt{P_0 Q_0} + \int \sqrt{(1-P_0)(1-Q_0) p(x) q(x)} dx \right) \\
  &= -2 \log \left( 1 - \frac{1}{2} \left( 
                (\sqrt{P_0} - \sqrt{Q_0})^2 + 
               \int (\sqrt{(1-P_0)p(x)} - \sqrt{(1-Q_0)q(x)} )^2 dx \right) \right)\\
 &= -2 \log \left(1 - \frac{1}{2} I^h \right) \\
  &= 2 \cdot \frac{1}{2} I^h (1 + \eta),
\end{align*}
where $|\eta| \leq \frac{I^h}{2 \epsilon}$. The last equality follows from Lemma~\ref{lem:log_linearize}.
\end{proof}

\begin{lemma}
\label{lem:log_linearize}
Suppose $0 < \epsilon \le 1$. For all $0 \leq x < 1-\epsilon$, we have $\log (1 - x) = - (1 + \eta) x$, where $| \eta| \leq \frac{x}{2 \epsilon}$
\end{lemma}

\begin{proof}
This follows by taking the Taylor expansion of $\log (1 - x)$ around $x = 0$.
\end{proof}

\begin{lemma}
\label{lem:sqrt_linearize}
Let $f(z) =  \frac{1 - \frac{z}{2} - \sqrt{ 1 - z}}{z}$, for $z \leq 1$ and $z \neq 0$, and define $f(0) = 0$. Then $\left| f(z)  \right| \leq |z|$, for all $z \leq 1$.
\end{lemma}

\begin{proof}
Note that $f$ is continuous, with derivative
\[
f'(z) = - \frac{1}{z^2} - \frac{z - 2}{2 z^2 \sqrt{1-z}}.
\]
It is straightforward to check that $f'(z) \geq 0$ for all $z < 1$, and we may define $f'(0) = \frac{1}{4}$ such that $f'(z)$ is continuous. Therefore, $f(z)$ is monotonic and maximized at $z = 1$, yielding $f(1) = \frac{1}{2}$, and minimized at $\lim_{z \rightarrow -\infty} f(z) = -\frac{1}{2}$. 

We now split into cases. If $z < -\frac{1}{2}$, then $|f(z)| \leq \frac{1}{2} < |z|$. If $-1/2 \leq z \leq 1/2$, a Taylor expansion gives
\[
\sqrt{1 - z} = 1 - \frac{1}{2} z - \frac{1}{8} z^2 - \frac{1}{16} z^3  - \cdots - \frac{(n+1)!!}{2^n n!} z^n - \cdots.
\]
Hence,
\begin{align*}
\left| \sqrt{1-z} - \left(1 - \frac{z}{2}\right) \right| &\leq
     \frac{1}{8} (|z|^2 + |z|^3 + \cdots ) \\
  &\leq \frac{1}{8} |z|^2 ( 1 + |z| + |z|^2 + \cdots) \\
  &\leq \frac{1}{8} |z|^2 \frac{1}{1 - |z|} \leq \frac{1}{4} |z|^2,
\end{align*}
implying that $|f(z)| \leq \frac{1}{4} |z|$. Finally, if $z > 1/2$, we have  $|f(z)| \leq \frac{1}{2} < z$.
\end{proof}

\end{document}
