\section{Proof of Theorem~\ref{thm:threshold_impossibility}}
\label{sec:threshold_proof}

We will follow the proof strategy of Abbe et al.~\cite{abbe2014exact}.

\subsection{Main argument}

We will show that if 
\[
 (\sqrt{a} - \sqrt{b})^2 + \sqrt{ab} \int (\sqrt{p(x)} - \sqrt{q(x)})^2 < K
\]
there with a probability of at least $1/3$, we can find nodes $i \in A$ and $j \in B$ such that exchanging their community assignments has a larger likelihood than the ground truth. This would establish that the maximum likelihood estimator fails with probability at least $1/3$. Although we will establish the proof for the case of two communities, we note that the proof below trivially extends to $K > 2$ communities each of size $n$, simply by taking $A$ and $B$ to be any two fixed communities from the $K$ communities. For the sake of convenience, we assume $n/K$ is an integer.  \\


Let $A = \{1, 2, \dots, n/K\}$ and $B = \{n/K+1, n+2, \dots, 2n/K\}$. For $u\neq v$, let $A_{uv}$ be the weight of the edge $(u,v)$ and let $A_{uv} = 0$ if there exists no edge between $u$ and $v$. Just as in the case of unlabeled edges, maximizing the likelihood in the labeled case may be interpreted as finding the min-cut for the stochastic block model, where the weight of an edge is $d_n(A_{uv})$ where we define $d_n(0) = \log \frac{P_0}{Q_0}$ and $d_n(x)$ for $x \neq 0$ to be
\[
d_n(x) = \log \left( \frac{a p(x)}{b q(x)} \right).
\]

 We may describe $d_n(0)$ explicitly as

\begin{equation}
\label{EqnDefnDn}
d_n(0) = \log \left\{ \frac{1 - a \log n/n}{1 - b \log n/n}\right\}
\end{equation}

Note that since $d_n(0) \to 0$ as $n \to \infty$, and since the likelihood ratio $p(x)/q(x)$ is assumed to be bounded, we may find a constant $\cM > 0$ that upper-bounds $d_n$ for all $n$. Thus, 
\begin{equation*}
\cM \geq \max_x d_n(x), \qquad \text{for all } n.
\end{equation*}

For any node $u$ and any subset of nodes $H$, denote 
$$\cS(u , H) = \sum_{v \in H, v \neq u} d_n(A_{uv}).$$
Using an argument along the lines of Lemma \ref{lemma: aww}, it is easy to check that if there exist nodes $u \in A$ and $v \in B$ such that

\begin{align}\label{eq: ml fail}
\cS(u, A\setminus \{u\}) + \cS(v, B\setminus \{v\}) < \cS(u, B\setminus \{v\}) + \cS(v, A\setminus \{u\}),
\end{align}

then the community assignment where $\sigma(u) = B$ and $\sigma(v) = A$ and every other assignment remains the same is more likely than the truth. Thus, the maximum likelihood estimator will fail if this happens. Define the following events:
\begin{align*}
F &= \text{ maximum likelihood fails},\\
F_A &= \exists u \in A ~:~ \cS(u, A \setminus \{i\}) < \cS(u, B) - \cM,\\
F_B &= \exists v \in B ~:~ \cS(v, B \setminus \{j\}) < \cS(u, A) - \cM.
\end{align*}
We have the following simple lemma:
\begin{lemma}\label{lemma: FA}
If $\mathbb P(F_A) \ge 2/3$, then $\mathbb P(F) \ge 1/3$.
\end{lemma}

\begin{proof}
By symmetry, we have $\mathbb P(F_B) \ge 2/3$, so by a union bound, $\mathbb P(F_A\cap F_B) \ge 1/3$. Thus, with probability at least $1/3$, there exist nodes $i\in A$ and $j \in B$ such that
\begin{align*}
\cS(i, A\setminus \{i\}) & < \cS(i, B) - \cM \leq \cS(i, B) - \cS(i,j) = \cS(i, B\setminus \{j\}), \qquad \text{and} \\
\cS(j, B\setminus \{j\}) & < \cS(j, A) - \cM \leq \cS(j, A) - \cS(i,j) = \cS(j, A\setminus \{j\}).
\end{align*}
This implies
\begin{equation*}
\cS(i, A\setminus \{i\})+\cS(j, B\setminus \{j\}) < \cS(i, B\setminus \{j\})+\cS(j, A\setminus \{j\}),
\end{equation*}
which from expression \eqref{eq: ml fail}, implies that the maximum likelihood estimator fails.
\end{proof}

We now define $\gamma(n)$ and $\delta(n)$ as follows:
\begin{align*}
\gamma(n) = (\log n)^{\log^{\frac{2}{3}} n}, \qquad \text{and} \qquad \delta(n) = \frac{\sqrt{\log n}}{\log \log n}.
\end{align*}
 Let $H$ be a fixed subset of $A$ of size $\frac{n}{\gamma(n)}$. We will take $\gamma(n) \asymp (\log n)^{\log^{\frac{2}{3}} n}$, such that $\frac{n}{\gamma(n)}$ is an integer. Define the event $\Delta$ as follows:
\begin{align*}
\Delta = \text{ for all nodes $i \in H$}, \quad  \cS(i, H) < \delta(n).
\end{align*}
We then have the following lemma:
\begin{lemma} \label{lemma: delta}
%[Proof in Appendix~\ref{AppLemDelta}]
$\mathbb P(\Delta) \geq \frac{9}{10}$.
\end{lemma}
\begin{proof}
%\subsection{Proof of Lemma~\ref{lemma: delta}}
\label{AppLemDelta}

Let $\Delta_i$ be the event  $\cS(i , H) < \delta(n).$ By a simple union bound calculation, we have
\begin{align*}
\mathbb P(\Delta) &=1- \mathbb P(\Delta^c) = 1 - \mathbb P \left( \cup_{i \in H} \Delta_i^c \right) \ge 1 - |H| \cdot \mathbb P(\Delta_i^c).
\end{align*}
We will show that $$|H| \cdot \mathbb P(\Delta_i^c) = o(1),$$ by showing that
$$\log |H| + \log \mathbb P(\Delta_i^c) \to -\infty,$$ as $n \to \infty$. 
Let the weights of the edges from $i$ to nodes within $H$ be the random variables $\{X_1, \dots, X_{|H| - 1}\}$. Note that the $X_i$'s are independent and identically distributed according to $p_n$. We have
\begin{align*}
\mathbb P(\Delta_i^c) &= \mathbb P\left( \cS(i, H) \geq \frac{\sqrt{\log n}}{\log \log n}\right) = \mathbb P\left( \sum_{j = 1}^ {|H|-1} d_n(X_i) \ge \frac{\sqrt{\log n}}{\log \log n}\right) \leq \inf_{t > 0} \left\{ \frac{\mathbb E\left[ e^{td_n(X_1)}\right]^{|H|-1}}{e^{\frac{t\sqrt{\log n}}{\log \log n}}}\right\},
\end{align*}
using a Chernoff bound in the last inequality. Thus, for $t > 0$, we have
\begin{align*}
\log |H| + \log \mathbb P(\Delta_i^c) &\leq \log \frac{n}{\gamma(n)} + \log \frac{\mathbb E\left[ e^{td_n(X_1)}\right]^{ \frac{n}{\gamma(n)}-1}}{e^{\frac{t\sqrt{\log n}}{\log \log n}}}\\
%
&= \log \frac{n}{\gamma(n)} + \left(\frac{n}{\gamma(n)}-1\right)\log \mathbb E\left[e^{td_n(X_1)} \right] - \frac{t\sqrt{\log n}}{\log \log n}.
\end{align*}
Picking $t = \sqrt{\log n}\log\log n$, the last expression simplifies to
\begin{align}\label{eq: master}
-\log \gamma(n) +  \left(\frac{n}{\gamma(n)}-1\right)\log \mathbb E\left[e^{\sqrt{\log n}(\log\log n) d_n(X_1)} \right].
\end{align}
We now analyze $\log \mathbb E\left[e^{\sqrt{\log n}(\log\log n) d_n(X_1)} \right]$ carefully. Note that
\begin{align*}
\log \mathbb E\left[e^{\sqrt{\log n}(\log\log n) d_n(X_1)} \right] &= \log \Bigg[\left( \frac{1-a\log n/n}{1 - b\log n/n}\right)^{\sqrt{\log n}\log \log n} \left(1-\frac{a\log n}{n}\right)\\
%
&\qquad \qquad \qquad \qquad \qquad + \int_{\mathbb R} \left(\frac{ap(x)}{bq(x)}\right)^{\sqrt{\log n}\log\log n} \frac{a\log n}{n}p(x)dx\Bigg]\\
%
&:= \log(1 + \mu_n + \nu_n),
\end{align*}
where
\begin{align*}
1 + \mu_n &= \left( \frac{1-a\log n/n}{1 - b\log n/n}\right)^{\sqrt{\log n}\log \log n} \left(1-\frac{a\log n}{n}\right), \quad \text{and} \\
\nu_n &= \int_{\mathbb R} \left(\frac{ap(x)}{bq(x)}\right)^{\sqrt{\log n}\log\log n} \frac{a\log n}{n}p(x)dx.
\end{align*}

The following bound holds for $\nu_n$:
\begin{equation*}
\nu_n  \leq C_1 \frac{(\log n)^{C_2\sqrt{\log n}}}{n},
\end{equation*}
for suitable constants $C_1, C_2$. For $\mu_n$, we have
\begin{align*}
\mu_n &= \left( \frac{1-a\log n/n}{1 - b\log n/n}\right)^{\sqrt{\log n}\log \log n} \left(1-\frac{a\log n}{n}\right) - 1\\
%
&= \left(\left( \frac{1-a\log n/n}{1 - b\log n/n}\right)^{n/\log n}\right)^{\frac{(\log n)^{3/2}\log \log n}{n} } \left(1-\frac{a\log n}{n}\right) - 1.
\end{align*}
The term $\left( \frac{1-a\log n/n}{1 - b\log n/n}\right)^{n/\log n}$ tends to a constant, $\exp(b-a)$. Thus, for large enough $n$, we may find constants $0 <c_1 < c_2$ such that $\left( \frac{1-a\log n/n}{1 - b\log n/n}\right)^{n/\log n} \in (c_1, c_2)$. Using the Taylor series approximation of $c_i^x$ near 0, we have
$$c_i^{\frac{(\log n)^{3/2}\log \log n}{n}} = 1 + \frac{(\log n)^{3/2}\log \log n}{n}\log c_i + O\left(\left(\frac{(\log n)^{3/2}\log \log n}{n}\right)^2\right),$$
so
\begin{multline*}
c_i^{\frac{(\log n)^{3/2}\log \log n}{n}}\left(1-\frac{a\log n}{n}\right) - 1= \frac{(\log n)^{3/2} \log\log n}{n}\log c_i + O\left(\left(\frac{(\log n)^{3/2}\log \log n}{n}\right)^2\right) \\
%
- \frac{a\log n}{n} \left(1 + \frac{(\log n)^{3/2} \log \log n}{n} \log c_i\right).
\end{multline*}
Thus, for large enough $n$, there exists a constant $C_3$ that satisfies
\begin{equation*}
|\mu_n| \leq \frac{C_3\log^2 n}{n}.
\end{equation*}
Using the bound
\begin{align*}
\log (1 + \mu_n + \nu_n) \leq |\mu_n| +|\nu_n|,
\end{align*}
we conclude that
\begin{equation*}
\log \mathbb E\left[e^{\sqrt{\log n}(\log\log n) d_n(X_1)} \right] \leq C'_1 \frac{(\log n)^{C'_2\sqrt{\log n}}}{n},
\end{equation*}
for a suitable constants $C'_1$ and $C'_2$. Returning to the expression~\eqref{eq: master}, we conclude that
\begin{multline*}
-\log \gamma(n) +  \left(\frac{n}{\gamma(n)}-1\right)\log \mathbb E\left[e^{\sqrt{\log n} (\log\log n) d_n(X_1)} \right] \\
%
\le -\log \gamma(n) + \left(\frac{n}{\gamma(n)}-1\right) C'_1 \frac{(\log n)^{C'_2\sqrt{\log n}}}{n}.
\end{multline*}
Substituting $\gamma(n) = (\log n)^{\log^{\frac{2}{3}} n}$, we arrive at the upper bound
\begin{align*}
-\log^{\frac{2}{3}}n(\log\log n) + \left(\frac{n}{(\log n)^{\log^{\frac{2}{3}} n}} - 1\right)C'_1 \frac{(\log n)^{C'_2\sqrt{\log n}}}{n}.
\end{align*}
It is easy to check that as $n \rightarrow \infty$, we have
$$\left(\frac{n}{(\log n)^{\log^{\frac{2}{3}} n}} - 1\right)C'_1 \frac{(\log n)^{C'_2\sqrt{\log n}}}{n} \to 0,$$
and
$$-\log^{\frac{2}{3}}n(\log\log n) \to -\infty.$$
This concludes the proof.
\end{proof}

Finally, define the events $F_H^{(i)}$ and $F_H$ as follows:
\begin{align*}
F_H^{(i)} &= \text{ node $i \in H$ satisfies } \cS(i, A \setminus H) + \delta(n) <  \cS(i, B) - \cM,\\
F_H &= \cup_{i \in H} F_H^{(i)},
\end{align*}
and define
\begin{align}
\label{EqnDefnRho}
\rho(n) = \mathbb P\Big(F_H^{(i)}\Big).
\end{align}
We have the following result:
\begin{lemma}\label{lemma: final blow}
If $\rho(n) > \frac{\gamma(n)\log 10}{n}$, then $\mathbb P(F) > 1/3$ for sufficiently large $n$.
\end{lemma}

\begin{proof}

We first show that $\mathbb P(F_H) > \frac{9}{10}$ for large enough $n$. 
Since the events $F_H^{(i)}$ are independent, we have
\begin{align*}
\mathbb P(F_H) = \mathbb P \left( \cup_{i \in H} F_H^{(i)} \right) = 1 - \mathbb P \left(  \cap_{i \in H} \left(F_H^{(i)}\right)^c \right) = 1 - (1-\rho(n))^{\frac{n}{\gamma(n)}}.
\end{align*}
Clearly, if $\rho(n)$ is not $o(1)$, then $\mathbb P(F)$ tends to 1 and we are done. If $\rho(n)$ is $o(1)$, then
\begin{align*}
\lim_{n \to \infty} (1-\rho(n))^{\frac{n}{\gamma(n)}} = \lim_{n \to \infty} (1-\rho(n))^{\frac{1}{\rho(n)} \frac{\rho(n)n}{\gamma(n)}} &= \lim_{n \to \infty} \exp \left(- \frac{\rho(n)n}{\gamma(n)} \right) < \frac{1}{10},
\end{align*}
where the last inequality used the fact that $\rho(n) > \frac{\gamma(n)\log 10}{n}$. Hence, $\mathbb P(F_H) > \frac{9}{10}$, as claimed.

Now note that $\Delta \cap F_H \subseteq F_A$. By Lemma~\ref{lemma: delta}, we also have $\mathbb P(\Delta) \ge \frac{9}{10}$. Hence,
$$\mathbb P(F_A) \geq \mathbb P(\Delta) + \mathbb P(F_H) - 1 \geq \frac{8}{10} > \frac{2}{3},$$
which combined with Lemma \ref{lemma: FA} implies the desired result.
\end{proof}

Let $\{X_i\}_{i \geq 1}$ be a sequence of i.i.d.\ random variables distributed according to $p_n$, and let $\{Y_i\}_{i \geq 1}$ be a sequence of i.i.d.\ random variables distributed according to $q_n$. From the definition~\eqref{EqnDefnRho} of $\rho(n)$, and using independence, we have
\begin{align}
\rho(n) &= \mathbb P \left( \sum_{i=1}^{\frac{n}{K}} d_n(Y_i) - \sum_{i=1}^{\frac{n}{K}-\frac{n}{\gamma(n)}} d_n(X_i) > \delta(n) + \cM \right) \notag \\
%
&\geq P \left( \sum_{i=1}^{\frac{n}{K}-\frac{n}{\gamma(n)}} d_n(Y_i) - \sum_{i=1}^{\frac{n}{K}-\frac{n}{\gamma(n)}} d_n(X_i) > \delta(n) + \cM - \hat \delta(n) \right)\times \mathbb P\left( \sum_{i= \frac{n}{K}-\frac{n}{\gamma(n)}+1}^{\frac{n}{K}} d_n(Y_i) \geq \hat \delta(n)\right), \label{eq: product}
\end{align}
for any $\hat \delta(n)$. We will choose a suitable $\hat \delta(n)$ so that
\begin{equation}
\label{EqnHari}
\mathbb P\left( \sum_{i= \frac{n}{K}-\frac{n}{\gamma(n)}+1}^{\frac{n}{K}} d_n(Y_i) \geq \hat \delta(n)\right) \longrightarrow 1.
\end{equation}
Note that $d_n(Y_i)$ is a random variable satisfying 
\begin{equation*}
\mathbb P \left( d_n(Y_i) = \log \left\{\frac{1-a\log n/n}{1 - b\log n/n}\right\}\right) = 1 - \frac{b\log n}{n}.
\end{equation*}
Thus, 
\begin{equation*}
\mathbb P \left(d_n(Y_i) = \log \left\{\frac{1-a\log n/n}{1 - b\log n/n}\right\}, \text{ for all } \frac{n}{K}-\frac{n}{\gamma(n)}-1 \leq i \leq \frac{n}{K} \right) = \left(1 - \frac{b\log n}{n}\right)^{\frac{n}{\gamma(n)}}.
\end{equation*}
We may check that
\begin{equation*}
\left(1 - \frac{b\log n}{n}\right)^{\frac{n}{\gamma(n)}} \longrightarrow 1, 
\end{equation*}
implying that
\begin{equation*}
\mathbb P \left(\sum_{i= \frac{n}{K}-\frac{n}{\gamma(n)}+1}^{\frac{n}{K}} d_n(Y_i) = \frac{n}{\gamma(n)} \cdot \log \left\{\frac{1-a\log n/n}{1-b\log n/n}\right\} \right) \longrightarrow 1.
\end{equation*}
Thus, equation~\eqref{EqnHari} holds with 
\begin{equation}
\hat\delta(n) = \Bigg|  \frac{n}{\gamma(n)} \cdot \log \left\{\frac{1-a\log n/n}{1-b\log n/n}\right\} \Bigg|.
\end{equation}
Since $$\hat \delta(n)  = O\left(\frac{\log n}{\gamma(n)}\right) = o(\sqrt{\log n}),$$ 
we have $\delta(n) + \cM - \hat\delta(n) = o(\sqrt{\log n})$.
Define
\begin{align}
\label{EqnT}
T(N, p_n, q_n, \epsilon) = \mathbb P \left(\sum_{i=1}^N \Big(d_n(Y_i)-d_n(X_i)\Big) \ge \epsilon \right).
\end{align}
Using this notation gives 
\begin{equation*}
T\left(\frac{n}{K}-\frac{n}{\gamma(n)}, p_n, q_n, \delta(n) + \cM -\hat \delta(n)\right) = \mathbb P \left( \sum_{i=1}^{\frac{n}{K}-\frac{n}{\gamma(n)}} d_n(Y_i) - \sum_{i=1}^{\frac{n}{K}-\frac{n}{\gamma(n)}} d_n(X_i) \geq \delta(n) + \cM - \hat \delta(n) \right),
\end{equation*}
and using Lemma~\ref{lemma: horrible}, we conclude that
\begin{equation}\label{eq: pumpkin}
 -\log T\left(\frac{n}{K}-\frac{n}{\gamma(n)}, p_n, q_n, \delta(n) + \cM -\hat \delta(n)\right) \leq  \left(\frac{1}{K}\int_{\mathbb R}  \left(\sqrt{ap(x)} - \sqrt{bq(x)}\right)^2 dx\right)\log n + o(\log n).
\end{equation}

Substituting the bounds~\eqref{EqnHari} and~\eqref{eq: pumpkin} into equation \eqref{eq: product}, we then conclude that
\begin{equation*}
-\log \rho(n) \leq \left(\frac{1}{K}\int_{\mathbb R}  \left(\sqrt{ap(x)} - \sqrt{bq(x)}\right)^2 dx\right)\log n + o(\log n).
\end{equation*}
In particular, when
$$\left(\frac{1}{K}\int_{\mathbb R}  \left(\sqrt{ap(x)} - \sqrt{bq(x)}\right)^2 dx\right) < 1,$$
we have
$$-\log \rho(n) \leq \log n -\log \gamma(n) - \log\log 10,$$
for sufficiently large $n$. Lemma~\ref{lemma: final blow} then implies that maximum likelihood fails with probability at least $\frac{1}{3}$, completing the proof of the theorem.








\subsection{Supporting lemmas}
\label{sec:threshold_lemmas}


\begin{lemma} 
\label{lemma: horrible}
Let $\omega(n) = o(\sqrt{\log n})$ and $N(n) = \frac{n}{K}(1 + o(1))$. Then
$$-\log T\left(N(n), p_n, q_n, \omega(n)\right) \leq  \left(\frac{1}{K}\int_{\mathbb R}  \left(\sqrt{ap(x)} - \sqrt{bq(x)}\right)^2 dx\right) \log n + o(\log n).$$
\end{lemma}

\begin{proof}
We will use the proof strategy found in Zhang and Zhou~\cite{zhangminimax}. Let 
$$Z = d_n(Y) - d_n(X),$$
where $X \sim p_n$ and $Y \sim q_n$. 
Let $M(t) = \mathbb E e^{tZ}$, and note that
\begin{align*}
t^\star & = \arg\min_{t > 0} M(t) = \frac{1}{2}, \\
M(t^\star) & = \left(\int_{\mathbb R} \sqrt{p_n(x)q_n(x)}dx\right)^2, \\
I & = -\log M(t^\star) = -2\log \left(\int_{\mathbb R} \sqrt{p_n(x)q_n(x)}dx\right).
\end{align*}
Let $S_N = \sum_{i=1}^{N(n)} Z_i$, where the $Z_i$'s are i.i.d.\ and distributed according to $Z$, and denote the distribution of $Z$ by $p_Z$. Define $$\eta(n) = \log^{\frac{3}{4}} n.$$ Then
\begin{align}
\mathbb P\left(S_N \geq \omega(n)\right) &\geq \sum_{z: S_N \in [\omega(n), \eta(n))} \prod_{i=1}^{N(n)} p_Z(z_i) \notag \\
%
&\geq \frac{M^{N(n)}(t^\star)}{e^{t^\star \eta(n)}} \sum_{z: S_N \in [\omega(n), \eta(n))} \prod_{i=1}^{N(n)} \frac{e^{t^\star z_i}p_Z(z_i)}{M(t^\star)} \notag \\
%
&=\exp\left(-N(n)I - \frac{\eta(n)}{2}\right)\sum_{z: S_N \in [\omega(n), \eta(n))} \prod_{i=1}^{N(n)} \frac{e^{t^\star z_i}p_Z(z_i)}{M(t^\star)}, \label{eq: bumbum}
\end{align}
where the second inequality uses the fact that $e^{t^\star \eta(n)} \geq e^{t^\star \sum_i z_i}$ when $\sum_{i=1}^{N(n)} z_i < \eta(n)$.

Now denote $r(w) = \frac{e^{t^\star w}p_Z(w)}{M(t^\star)}$, and note that $r$ defines a probability distribution. Defining $W_1, W_2, \dots, W_n$ to be i.i.d.\ random variables with probability mass function $r(w)$, we then have
\begin{equation}
\label{eq: dundun}
\sum_{z: S_N \in [\omega(n), \eta(n))} \prod_{i=1}^{N(n)} \frac{e^{t^*z_i} p_Z(z_i)}{M(t^*)} = \mathbb P\left(\omega(n) \le \sum_{i=1}^{N(n)} W_i < \eta(n)\right).
\end{equation}



By Lemma~\ref{lemma: clt}, it follows that
$$\frac{1}{\sqrt {\log N(n)}}\sum_{i=1}^{N(n)} W_i \stackrel{d}\to \cN(0, \nu^2),$$
for some constant $\nu > 0$. Furthermore, by our choices of $\omega(n)$, $N(n)$, and $\eta(n)$, we have 
\begin{align*}
\frac{\omega(n)}{\sqrt{\log N(n)}} \to 0, \qquad \text{and} \qquad \frac{\eta(n)}{\sqrt{\log N(n)}} \to +\infty.
\end{align*}
Thus, 
\begin{equation*}
 \mathbb P\left(\frac{\omega(n)}{\sqrt{\log N(n)}} \leq \frac{1}{\sqrt {\log N(n)}}\sum_{i=1}^{N(n)} W_i < \frac{\eta(n)}{\sqrt{\log N(n)}} \right) \to 1/2,
\end{equation*}
implying that the left-hand probability expression becomes larger that $1/4$ for all large enough $n$.
Combining this with the bounds~\eqref{eq: bumbum} and~\eqref{eq: dundun}, we then obtain
\begin{align*}
\mathbb P(S_N \geq \omega(n)) \geq \exp \left(-N(n)I - \frac{\log^{\frac{3}{4}} n}{2} - \log 4 \right).
\end{align*}

Using $N = \frac{n}{K}(1+o(1))$, we arrive at
\begin{align*}
-\log T\left(N(n), p_n, q_n, \omega(n)\right) &= -\log \mathbb P(S_N \geq \omega(n)) \leq \left(\frac{1}{K}\int_{\mathbb R}  \left(\sqrt{ap(x)} - \sqrt{bq(x)}\right)^2 dx\right)  \log n+ o(\log n).
\end{align*}
This concludes the proof.

\end{proof}



\begin{lemma}
\label{lemma: clt}
Let $\{W_i\}_{i \geq 1}$ be i.i.d.\ random variables distributed as $r(w)$. Then 
\begin{equation*}
\frac{\sum_{i=1}^n W_i}{\sqrt{\log n}} \stackrel{d}{\longrightarrow} \cN(0, \nu^2),
\end{equation*}
as $n \rightarrow \infty$, where $\nu > 0$ is a constant.
\end{lemma}

\begin{proof}

We show that the moment generating function of $\frac{\sum_{i=1}^n W_i}{\sqrt{\log n}}$ converges to that of a normal random variable. By a simple computation, we may check that $r$ has the following distribution:
\begin{equation*}
r(0) = 1 - \frac{C_0\log n}{n} + O\left(\frac{\log^2 n}{n^2}\right),
\end{equation*}
for some constant $C_0 > 0$, and $r(x) = \Theta(\log n/n) + O\left(\frac{\log^2 n}{n^2}\right)$ elsewhere. Since $W$ is a bounded and symmetric random variable, it is easy to see that its moment generating function is given by
\begin{equation}
\label{EqnMGF}
\mathbb E e^{tW} = 1 + \int_{\mathbb R} r(\hat w)\left(e^{t\hat w/2} - e^{-t\hat w/2}\right)^2 d\hat w,
\end{equation}
using the fact that $r(0) = 1 - \int_{\mathbb R} 2r(\hat w)d\hat w$. Using the expression~\eqref{EqnMGF}, the moment generating function of $W$ is then given by
\begin{multline*}
\mathbb E e^{tW} = 1 + \int_{\mathbb R} \left(\frac{C(\hat w) \log n}{n} + O\left(\frac{\log^2 n}{n^2}\right)\right)\left(e^{t\hat w/2} - e^{-t\hat w/2}\right)^2 d\hat w
\end{multline*}
Substituting $\frac{t}{\sqrt{\log n}}$ in place of $t$ and using the approximation $a^{x/2} - a^{-x/2} = x\log a + O(x^2\log^2 a)$ for $x = o(1)$, we arrive at
\begin{align*}
\mathbb E e^{tW/\sqrt{ \log n}} &=  1 + \int_{\mathbb R} \left(\frac{C(\hat w) \log n}{n} + O\left(\frac{\log^2 n}{n^2}\right)\right) \left(\frac{t\hat w}{\sqrt {\log n}} + O\left(\frac{1}{\log n}\right)\right)^2 d\hat w\\
%
& \qquad + \int_{\mathbb R} O\left(\frac{\log^2 n}{n^2} \right)\left(\frac{t\hat w}{\sqrt {\log n}} + O\left(\frac{1}{\log n}\right)\right)^2d\hat w\\
%
%& = 1 + \sum_{1 \leq j \leq N} \left(\frac{C_j \log n}{n} + O\left(\frac{\log^2 n}{n^2}\right)\right) \left(\frac{t w_j}{\sqrt {\log n}} + O\left(\frac{1}{\log n}\right)\right)^2\\
%
%& \qquad + \sum_{N+1 \leq j \leq R} O\left(\frac{\log^2 n}{n^2} \right)\left(\frac{t w_j}{\sqrt {\log n}} + O\left(\frac{1}{\log n}\right)\right)^2\\
%
&= 1 + \frac{Ct^2}{n} + o\left(\frac{1}{n}\right),
\end{align*}
for a suitable constant $C$. Hence, the moment generating function of $\frac{\sum_{i=1}^n W_i}{\sqrt{\log n}}$ is given by
\begin{align*}
\left(\mathbb E e^{tW/\sqrt{ \log n}}\right)^n &= \left(1 + \frac{Ct^2}{n} + o\left(\frac{1}{n}\right)\right)^n \longrightarrow e^{Ct^2},
\end{align*}
which is the moment generating function of $\cN(0, 2C)$. This completes the proof.

\end{proof}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End:
