\subsection{Proof of the Lower Bound (Theorem~\ref{thm:lower_bound}}
\label{sec:lower_bound_proof}

We first state a lemma that gives an alternative characterization of the Renyi divergence. 

\begin{lemma}
\label{lem:information_equivalence}

Let $P, Q$ be two probability measures on $\R$ absolutely continuous with respect to each other. Suppose that part of $P,Q$ singular to the Lebesgue measure is a point mass at zero, denoted $P_0, Q_0$. 

Define
\[
I = - 2 \log \int \left( \frac{dP}{dQ} \right)^{1/2} dQ \qquad 
D = \inf_{Y \in \mathcal{P}} \max \left\{ \int \log \frac{dY}{dP} dY, \, \int \log \frac{dY}{dQ} dY \right\}
\]
where we use $\mathcal{P}$ to denote probability measures absolutely continuous to $P$ (and thus $Q$).

Then, we have that
\[ 
I = 2 D 
\]
\end{lemma}


\begin{proof} 

First, we note that $D$ must be finite since we can substitute $Y = P$ or $Y= Q$. We claim that $D$ is equivalent to the following:
\begin{align*}
\inf_{Y \in \mathcal{P}} & \int \log \frac{dY}{dP} dY \\
  & \int \log \frac{dP}{dQ} dY = 0
\end{align*}

This is because for any $Y \in \mathcal{P}$ such that $\int \log \frac{dP}{dQ} dY \neq 0$, 
we have that $\int \log \frac{dY}{dP} dY \neq \int \log \frac{dY}{dQ} dY$. 
Suppose without loss of generality that the former quantity is larger. 
Therefore, it is possible to take $\tilde{Y} = (1 - \epsilon) Y + \epsilon P$ for $\epsilon$ small enough such that 
$ \max \left\{ \int \log \frac{d\tilde{Y}}{dP} d\tilde{Y}, \, \int \log \frac{d\tilde{Y}}{dQ} d\tilde{Y} \right\}$ strictly decreases. 

Since the new optimization is convex in $Y$, we can solve and get $Y_0 = \frac{P_0^{1/2} Q_0^{1/2}}{Z}$ and $(1-Y_0) y(x) = \frac{((1-P_0) \cdot p(x))^{1/2} ((1-Q_0) \cdot q(x))^{1/2}}{Z}$. Here, we denote by $(1-Y_0) y(x), (1-P_0)p(x), (1-Q_0)q(x)$ the Radon-Nikodym derivative of the continuous part of $Y, P, Q$ with respect to the Lebesgue measure. $Z$ is the normalization term: $Z = P_0^{1/2} Q_0^{1/2} + \int \sqrt{ (1-P_0) p(x) (1-Q_0) q(x)} dx$. 

\begin{align*}
\int \log \frac{dY}{dP} dY  &= \log \frac{1}{Z} \left\{ 
           \left( \frac{Q_0}{P_0} \right)^{1/2}  Y_0 + \int \left(\frac{(1-P_0) p(x)}{(1-Q_0)q(x)} \right)^{1/2} (1-Y_0) y(x) dx \right\} \\
  &= \log \frac{dP}{dQ} dY - \log Z \\
  &= - \log Z 
\end{align*}

It is straightforward to verify that $-2 \log Z = I$. 

\end{proof}





\begin{proof}(of Theorem~\ref{thm:lower_bound})\\

Without loss of generality, let us suppose that node 1 was placed in cluster 1, i.e, $\sigma_0(1) = 1$. 

Let $\Phi$ denote the measure on the graph described by the colored SBM. Let $\Psi$ denote a measure on the graph defined as follows:

\begin{enumerate}
\item If $u,v \neq 1$, then $A_{uv}$ is distributed just as in $\Phi$. 
\item If $u = 1$ and $v \notin C_1 \cup C_2$, then $A_{1v}$ is distributed just as in $\Phi$.
\item If $u = 1$ and $v \in C_1 \cup C_2$, then $A_{1v}$ is distributed as $Y$, where $Y$ is the distribution that minimizes $D$ in lemma~\ref{lem:information_equivalence}. ($Y_0 \propto (P_0 Q_0)^{1/2}$ and $(1-Y_0) y(x) \propto \sqrt{(1-P_0)p(x)(1-Q_0) q(x)}$)
\end{enumerate}

The log-likelihood ratio $\log \frac{d P_{\Psi} }{d P_{\Phi}}$ is 
\[
\mathcal{Q} = \sum_{v \neq 1,\, v \in C_1} \log \frac{Y(A_{1v})}{P(A_{1v})} + \sum_{v \neq 1,\, v \in C_2} \log \frac{Y(A_{1v})}{Q(A_{1v})} 
\]

where we use the notation $P(A_{1v}) = P_0$ if $A_{1v} = 0$ and $P(A_{1v}) = (1-P_0)p(A_{1v})$ if $A_{1v} \neq 0$.

Let $f(n)$ be an arbitrary function and $\hat{\sigma}$ be an arbitrary clustering algorithm. Let $\mathcal{E} = \mathcal{E}(\hat{\sigma}(G))$ and let $v_1$ denote node $1$. 

\[
P_{\Psi}( \mathcal{Q} \leq f(n) ) = P_{\Psi}( \mathcal{Q} \leq f(n), v_1 \in \mathcal{E}) +
 P_{\Psi}( \mathcal{Q} \leq f(n), v_1 \notin \mathcal{E} )
\]

The first term can be bounded.

\begin{align*}
P_{\Psi}( \mathcal{Q} \leq f(n), v_1 \in \mathcal{E}) &= \int_{\mathcal{Q} \leq f(n), v_1 \in \mathcal{E}} d P_{\Psi} \\
    &= \int_{\mathcal{Q} \leq f(n), v_1 \in \mathcal{E}} \exp(\mathcal{Q}) d P_{\Phi} \\
    &\leq \exp(f(n)) P_{\Phi}( \mathcal{Q} \leq f(n), v_1 \in \mathcal{E}) \\
    &\leq \exp(f(n)) P_{\Phi}(v_1 \in \mathcal{E}) \\
    &\leq \exp(f(n)) \E_{\Phi}  l(\hat{\sigma}, \sigma_0)
\end{align*}
The last inequality follows because  $\E l(\hat{\sigma}(G), \sigma_0) = \frac{1}{n} \sum_{v=1}^n P( v \in \mathcal{E}(\hat{\sigma}(G)) ) = P(v_1 \in \mathcal{E}(\hat{\sigma}(G))$ since the nodes are exchangeable when $\sigma_0$ is drawn uniformly at random. 


To bound the second term, note that under $P_{\Psi}$, any clustering algorithm has at most $\frac{1}{K}$ chance of clustering $v_1$ correctly. Thus
\[
P_{\Psi}(\mathcal{Q} \leq f(n), v_1 \notin \mathcal{E}) \leq P_{\Psi}(v_1 \notin \mathcal{E}) \leq \frac{1}{2}
\]

Combining these two bounds, we have that
\[
P_{\Psi}(\mathcal{Q} \leq f(n)) \leq \exp(f(n)) \E_{\Phi} l(\hat{\sigma}, \sigma_0) + \frac{1}{2}
\]

Let $f(n) = \log \frac{1}{4 \E_{\Phi} l(\hat{\sigma}, \sigma_0)}$, then 

\[
P_{\Psi}\left(\mathcal{Q} \leq \log \frac{1}{4 \E_{\Phi} l(\hat{\sigma}, \sigma_0)} \right) \leq \frac{3}{4}
\]

From Chebyshev's inequality, we also have that
\[
P_{\Psi}\left( \mathcal{Q} \leq \E_{\Psi} \mathcal{Q} + \sqrt{ 5 V_{\Psi}(\mathcal{Q})} \right) \geq 4/5
\]

Hence, we get that $\log \frac{1}{4 \E_{\Phi} l(\hat{\sigma},\sigma_0)} \leq \E_{\Psi} \mathcal{Q} + \sqrt{ 5 V_{\Psi}(\mathcal{Q})}$.
\[
\E_{\Phi} l(\hat{\sigma}, \sigma_0) \geq \frac{1}{4} \exp\Big( - (\E_{\Psi} \mathcal{Q} + \sqrt{ 5 V_{\Psi}(\mathcal{Q})} ) \Big)
\]

We now just need to compute $\E_{\Psi} \mathcal{Q}$ and $V_{\Psi}(\mathcal{Q})$.

\begin{align*}
\E_{\Psi} \mathcal{Q} &= \E_{\Psi}  \sum_{v \neq 1,\, v \in C_1} \log \frac{Y(A_{1v})}{P(A_{1v})} + \sum_{v \neq 1,\, v \in C_2} \log \frac{Y(A_{1v})}{Q(A_{1v})}  \\
     &= (n/K - 1) \int \log \frac{dY}{dP} dY + (n/K) \int \log \frac{dY}{dQ} dY \\
     &= (n/K - 1/2) 2 D\\
     &= (n/K - 1/2) I \\
     &= \frac{nI}{K} (1 + o(1)) 
\end{align*}

Moving onto the variance term:

\begin{align*}
V_{\Psi}(\mathcal{Q}) &= \sum_{v \neq 1, v\in C_1} V_{\Psi} \left( \log \frac{Y(A_{1v})}{P(A_{1v})} \right) +
                        \sum_{v \neq 1, v\in C_2} V_{\Psi} \left( \log \frac{Y(A_{1v})}{Q(A_{1v})} \right)  \\
   &\leq (n/K - 1) \E_{\Psi} \left( \log \frac{Y(A_{1v})}{P(A_{1v})} \right)^2 + 
        (n/K) \E_{\Psi} \left( \log \frac{Y(A_{1v})}{Q(A_{1v})} \right)^2 
\end{align*}

Note that 
\[
\left| \log \frac{Y(A_{1v})}{Q(A_{1v})} \right| = 
  \left| \log \frac{P^{1/2}(A_{1v})}{Q^{1/2}(A_{1v})} - \log Z \right| \leq 
  \frac{C + I}{2}
\]

Thus, 
\begin{align*}
V_{\Psi}(\mathcal{Q}) \leq \left( \frac{C+I}{2} \right) 
       (n/K - 1/2) I
\end{align*}

If $nI/K \rightarrow \infty$, then $\sqrt{ 5 V_{\Psi}(\mathcal{Q})}$ is $o( n I/ K)$. If $nI/K \rightarrow c < \infty$, then
$\E_{\Psi} l(\hat{\sigma}, \sigma_0) > c' > 0$. 

\end{proof}










%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End:

