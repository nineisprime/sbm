\documentclass{article}
\usepackage{minx_math}
\usepackage[top=3.5cm, bottom=3.5cm, left=3.5cm, right=3.5cm]{geometry}
\begin{document}

\title{Detecting Communities on Colourful and Weighted Graphs}
\maketitle

\begin{abstract}
\centering
\noindent 
A graph is known, edges and nodes, \\
communities hide within its folds. \\
Stochastic blocks the model is, \\
weighted edges a novel goal.
\end{abstract}

%%%%%%%%%%
%%%%%%%%%%
%%%%
%%%% Deprecated

\begin{comment}
\section{General Discrete Likelihood}

Let the parameter space be the following:

\[
\Theta(\sigma, n, K, P, Q) = \{
   \sigma: [n] \rightarrow K, \, P, Q \textrm{ distributions} \}
\]

The graph adjacency matrix is generated as such:
\[
\begin{array}{cc}
A_{ij} \sim P & \trm{if } \sigma(i) = \sigma(j) \\
A_{ij} \sim Q & \trm{if } \sigma(i) \neq \sigma(j) 
\end{array}
\]

Suppose $P, Q$ are known, the likelihood function becomes:
\[
L(\sigma) = \sum_{i<j} \log P(A_{ij}) \mathbf{1}_{\sigma(i) = \sigma(j)} + 
  \sum_{i<j} \log Q(A_{ij}) \mathbf{1}_{\sigma(i) \neq \sigma(j)}
\]
Since $1 - \mathbf{1}_{sigma(i) \neq \sigma(j)} = \mathbf{1}_{\sigma(i) = \sigma(j)}$, we have that
\begin{align*}
L(\sigma) = \sum_{i<j} \log \frac{P(A_{ij})}{Q(A_{ij})} \mathbf{1}_{\sigma(i) = \sigma(j)} + \sum_{i<j} \log Q(A_{ij})
\end{align*}
We can safely discard the second term since it is a constant for any $\sigma$. We turn to the case where $P,Q$ are general discrete distributions with domain size $L$. It is straightforward to see that
\[
\begin{array}{cc}
\log P(A_{ij}) = & \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log P_l \\
\log Q(A_{ij}) = & \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log Q_l
\end{array}
\]
The same summation form similarly holds for $\log \frac{P(A_{ij})}{Q(A_{ij})}$. 

Thus, the likelihood function is
\begin{shaded}
\begin{align}
L(\sigma) = \sum_{i<j} \mathbf{1}_{\sigma(i) = \sigma(j)} \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log \frac{P_l}{Q_l} \label{eqn:likelihood}
\end{align}
\end{shaded}

Taking a brief detour, we note that if $0$ is in the domain of $P,Q$, such as the case with Bernoulli distributions, we can rewrite the likelihood function as
\[
L(\sigma) = \sum_{i<j} \mathbf{1}_{\sigma(i) = \sigma(j)} \sum_{l\neq 0} \mathbf{1}_{A_{ij} =l} \log \frac{P_k Q_0}{Q_k P_0} + \sum_{i<j} \mathbf{1}_{\sigma(i) = \sigma(j)} \log \frac{P_0}{Q_0}
\]
Assuming that $P_0 < Q_0$, the second term can be interpreted as the term that wants to put every node into a single cluster. This rewrite simply uses the fact that $1 - \mathbf{1}_{A_{ij} = 0} = \sum_{l\neq 0} \mathbf{1}_{A_{ij} = l}$. \\

\subsection{Oracle Setting Misclassification Bound}
Now we go back to using expression~\ref{eqn:likelihood}. We ask the question, in the oracle setting, what is the probability of the MLE making a mistake? Let $\sigma_0$ be the true cluster assignment and $\sigma'$ be a different one, the differences between the two likelihoods are:

\begin{align*}
L(\sigma_0) - L(\sigma') = &
   \sum_{i<j} \mathbf{1}_{\sigma_0(i) = \sigma_0(j) \wedge \sigma'(i) \neq \sigma'(j)} \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log \frac{P_l}{Q_l} \\
    &- \sum_{i<j} \mathbf{1}_{\sigma_0(i) \neq \sigma_0(j) \wedge \sigma'(i) = \sigma'(j)} \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log \frac{P_l}{Q_l} 
\end{align*}

Every pair of $(i,j)$ in the first and the second terms are independent. For each $(i,j)$ in the first term, $X = \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log \frac{P_l}{Q_l}$ is a random variable drawn from a discrete distribution:
\begin{equation}
X = \log \frac{P_l}{Q_l} \trm{ w.p. } P_l \label{eqn:edge_discrete_rv}
\end{equation}
For each $(i,j)$ in the second term, $Y = \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log \frac{P_l}{Q_l}$ is a random variable drawn from:
\begin{equation}
Y = \log \frac{P_l}{Q_l} \trm{ w.p. } Q_l \label{eqn:edge_discrete_rv2}
\end{equation}


Let $\alpha = |\{ (i,j) \,:\, \sigma_0(i) = \sigma_0(j) \wedge \sigma'(i) \neq \sigma'(j) \}|$ and $\gamma = | \{(i,j) \,:\, \sigma_0(i) \neq \sigma_0(j) \wedge \sigma'(i) = \sigma'(j) \}|$. We have that 

$$L(\sigma_0) - L(\sigma') < 0 \trm{ iff } \sum_{i=1}^\alpha X_i - \sum_{i=1}^\gamma Y_i < 0$$

Thus, for an assignment $\sigma'$ such that $d_H(\sigma', \sigma_0) \leq \alpha + \gamma$, we can bound the probability of MLE mistakingly selecting $\sigma'$ by bounding the probability that $\sum_{i=1}^\alpha X_i \leq \sum_{i=1}^\gamma Y_i$. 

\subsubsection{Upper Bound}

We upper bound the probability of error that $$ \sum_{i=1}^\alpha X_i < \sum_{i=1}^\gamma Y_i $$ where $X_i$ and $Y_i$'s are defined as in equations~\ref{eqn:edge_discrete_rv}, ~\ref{eqn:edge_discrete_rv2}. 

\begin{align*}
 \sum_{i=1}^\gamma Y_i - \sum_{i=1}^\alpha X_i &> 0  \quad (\Leftrightarrow) \\
e^{- t \sum_{i=1}^\alpha X_i} e^{t \sum_{i=1}^\gamma Y_i} &> 1
\end{align*}
where $t$ is an arbitrary real number since $X_i$ and $Y_i$ are both bounded. 

\begin{align*}
P( \sum_{i=1}^\gamma Y_i - \sum_{i=1}^\alpha X_i > 0 ) &\leq 
  \E \left( e^{- t \sum_{i=1}^\alpha X_i} e^{t \sum_{i=1}^\gamma Y_i} \right) \\
 & \leq  \E \left( e^{ - t X_1 \alpha} e^{ t Y_1 \gamma} \right ) \\
 &\leq \left( \E e^{ - t X_i} \E e^{t Y_1} \right)^{(1-w)\alpha + w \gamma} 
       \frac{ \left( \E e^{t Y_1} \right)^{(1-w)(\gamma - \alpha)} }
            { \left( \E e^{-t X_1} \right)^{w (\gamma - \alpha)} }
\end{align*}

We will show that when $t=1/2$ and $w=1/2$, the fraction term equals $1$ and the first term equals $\exp\left( - (1/2 \alpha + 1/2 \gamma) I \right)$. 

Note that 
\begin{align*}
\E e^{ -t X_1} &= \sum_l P_l e^{ - t \log \frac{P_l}{Q_l}} \\
   &= \sum_l P_l \left( \frac{Q_l}{P_l} \right)^t \\ 
   &= \sum_l \sqrt{P_l Q_l} \quad \trm{(if $t=1/2$)}
\end{align*}

\begin{align*}
\E e^{ t Y_1} &= \sum_l Q_l e^{ t \log \frac{P_l}{Q_l} } \\
   &= \sum_l Q_l \left( \frac{P_l}{Q_l} \right)^t \\
   &= \sum_l \sqrt{P_l Q_l} \quad \trm{if $t=1/2$}
\end{align*}

From Lemma~\ref{lem:affinity_renyi}, we arrive at the conclusion that 
\[
P( \sum_{i=1}^\gamma Y_i - \sum_{i=1}^\alpha X_i > 0) \leq 
    \exp\left( - \frac{(\alpha+\gamma)}{2}  I \right)
\]
   

\subsubsection{Neyman-Pearson Lower Bound}

We observe two random vectors $(X_1,...,X_n)$ and $(Y_1,...,Y_n)$. Consider a simple hypothesis:

\[
H_0: X_i \sim P,\, Y_i \sim Q \qquad H_1: X_i \sim Q,\, Y_i \sim P
\]

The likelihood ratio test is 
\begin{align*}
\sum_{i=1}^n \log q(X_i) + \log p(Y_i) - \log p(X_i) - \log q(Y_i) &\geq 0 \\
\sum_{i=1}^n \log \frac{p(X_i)}{q(X_i)} - \log \frac{p(Y_i)}{q(Y_i)} &\leq 0 \\
\sum_{i=1}^n \log \frac{p(X_i)}{q(X_i)} &\leq \log \frac{p(Y_i)}{q(Y_i)} \\ 
\end{align*}

The probability of error is:
\begin{align*}
P^n_0(A) + 1 - P^n_1(A) &= 1 - (P^n_1(A) - P^n_0(A)) \\
               &= 1 - TV(P_0^n, P_1^n) \\
            &= \sum_{K^n} \min(P_0^n, P_1^n) \\
            &\geq \frac{1}{2} A(P_0, P_1)^{2n} = \frac{1}{2} \left(
               \sum_{x,y} \sqrt{P_0(x,y) P_1(x,y)} \right)^{2n} \\
            &=  \frac{1}{2} \left( \sum_{x,y} \sqrt{p(x)q(y) q(x)p(y)}  \right)^{2n}\\
            &= \frac{1}{2} \left( \sum_x \sqrt{ p(x) q(x)} \right)^{4n}\\
            &= \frac{1}{2} \exp( - n 2 D_{1/2}(P \| Q)) 
\end{align*}

The last step is used often and should be a lemma in its own right.

\begin{lemma}
\label{lem:affinity_renyi}
$$ D_{1/2}(P \| Q) = - 2 \log \sum_x \sqrt{p(x)q(x)}$$ 
and so 
$$ \exp(- D_{1/2}(P \| Q) ) = \left(\sum_x \sqrt{p(x)q(x)} \right)^2$$.
\end{lemma}


Under $H_1$, $A^c$ is the event that $\sum_{i=1}^n \log \frac{p(X_i)}{q(X_i)} \geq \log \frac{p(Y_i)}{q(Y_i)}$ where $X_i \sim Q$ and $Y_i \sim P$. Therefore, $P_0(A) = P_1(A^c)$ and $P_0(A) \geq \frac{1}{4} \exp( - n 2 D_{1/2} (P \| Q))$.

A question arises on whether we can get rid of the 2 in the exponential. 

\end{comment}
%%%% End deprecation
%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%



\newpage
\section{Preliminary}

Suppose we have a graph of $n$ nodes and each edge may take on any of $L$ colors. The graph is generated by a stochastic block model with $K$ clusters. Each within-cluster edge takes on color $l \in \{1,...,L\}$ with probability $P_l$ and each between-cluster edge with probability $Q_l$. We suppose that $P_l, Q_l \rightarrow 0$ so that the graph is sparse. Let $n_k$ be the size of cluster $k$, we will suppose that $\frac{n}{\beta K} \leq n_k \leq \frac{\beta n}{K}$ for some $\beta \geq 1$. \\

The goal is to recover the clustering. We want to estimate $\hat{\sigma} : \{1,...,n\} \rightarrow \{1,...,K\}$ such that $\min_{\tau \in S_K} d_H( \hat{\sigma}, \tau \circ \sigma)$ is small where $S_K$ is the permutation group over $K$ elements.\\

We suppose that the number of colors $L$ is finite and that the probabilities $P_l, Q_l$ are unknown to us. We refer to the setting where $P_l, Q_l$ are known as the \emph{oracle setting}. 

We define the Renyi divergence between $P,Q$ as
\[
I_{tot} = -2 \log \sum_l \sqrt{P_l Q_l}
\]

\textbf{Our goal is to show that all results (i.e., weak consistency rates, strong consistency thresholds) that hold under the oracle setting also hold when the distribution $\{P_l, Q_l\}$ is unknown. }

\section{Weak Recovery Under the Oracle Setting}

\cite{zhangminimax} characterizes the minimax rate of weak recovery for Bernoulli $P,Q$. Their results and proofs can be extended in a straightforward manner to general discrete $P,Q$ under the oracle setting. 

\begin{proposition} 
\label{prop:weak_recovery_oracle}
(Oracle Setting Upper bound)
Assume $\frac{n I_{tot}}{K \log K} \rightarrow \infty$. The maximum likelihood estimator $\hat{\sigma}$ in the oracle setting achieves:
\[
\sup_{\Theta(n, K, \beta, P, Q)} \E r(\hat{\sigma}, \sigma) \leq \left\{ 
    \begin{array}{cc} 
   \exp \left( - (1 + o(1)) \frac{nI_{tot}}{2} \right ), \, & K=2, \\
   \exp \left( - (1 + o(1)) \frac{nI_{tot}}{\beta K} \right ), \,& K\geq 3
  \end{array} \right. 
\]   
\end{proposition}

\begin{proof}
Proof of this proposition follows that of Theorem 3.2 in~\cite{zhangminimax}. We describe only the parts that need to be modified. 

Because we have a different likelihood function, our $T(\sigma)$ takes on a different form from that of~\cite{zhangminimax} at the bottom of page 8:
\[
T(\sigma) = \sum_{i<j} \mathbf{1}_{\sigma(i) = \sigma(j)} \sum_{l=0}^L \log \frac{P_l}{Q_l} \mathbf{1}_{A_{ij} = l} 
\]

Let $\sigma_0$ denote the true community assignment. We make a mistake if for some other community assignment $\sigma$, we get $T(\sigma) > T(\sigma_0)$. The key part of the proof is to bound

$$
P_m \equiv P\left(\exists \sigma \,:\,  T(\sigma) > T(\sigma_0),\, d_H(\sigma, \sigma_0) = m \right)
$$

To that end, we bound the probability of error of a fixed $\sigma$ $m$-distant from $\sigma_0$ in Hamming distance. We will prove an analogue of Proposition 5.1 in~\cite{zhangminimax}:

Let $\sigma$ be an arbitrary assignment satisfying $d(\sigma, \sigma_0) = m$. Let $X_i, Y_i$ be random variables such that
\[
X_i = \log \frac{P_l}{Q_l} \trm{ w.p. $P_l$} \qquad 
Y_i = \log \frac{P_l}{Q_l} \trm{ w.p. $Q_l$} 
\]
and $\alpha, \gamma$ be integers where
\[
\alpha = | \{ (i,j) \,:\, \sigma_0(i) = \sigma_0(j) \wedge \sigma(i) \neq \sigma(j) \} | 
\quad
\gamma = | \{ (i,j) \,:\, \sigma_0(i) \neq \sigma_0(j) \wedge \sigma(i) = \sigma(j) \} | 
\]

Then
\begin{align}
P( T(\sigma) \geq T(\sigma_0) ) \leq 
  P\left( \sum_{i=1}^\alpha X_i - \sum_{i=1}^\gamma Y_i < 0 \right) \leq 
  \exp( - \frac{\gamma + \alpha}{2} I) \label{eqn:new_prop51}
\end{align}

Lemma 5.3 from~\cite{zhangminimax} bounds $\alpha, \gamma$ and Proposition 5.2 bounds the number of $\sigma$'s (up to equivalent classes) such that $d_H(\sigma, \sigma_0) = m$. These pieces together bounds $P_m$. The rest of the proof follows~\cite{zhangminimax} exactly starting from Page 16. \\

We devote the rest of the proof toward proving equation~\ref{eqn:new_prop51}.

\begin{align*}
T(\sigma_0) - T(\sigma') = &
   \sum_{i<j} \mathbf{1}_{\sigma_0(i) = \sigma_0(j) \wedge \sigma'(i) \neq \sigma'(j)} \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log \frac{P_l}{Q_l} \\
    &- \sum_{i<j} \mathbf{1}_{\sigma_0(i) \neq \sigma_0(j) \wedge \sigma'(i) = \sigma'(j)} \sum_{l=1}^L \mathbf{1}_{A_{ij} = l} \log \frac{P_l}{Q_l} \\
 &=  \sum_{i=1}^\alpha X_i - \sum_{i=1}^\gamma Y_i 
\end{align*}


\begin{align*}
P( \sum_{i=1}^\gamma Y_i - \sum_{i=1}^\alpha X_i > 0 ) &\leq 
  \E \left( e^{- t \sum_{i=1}^\alpha X_i} e^{t \sum_{i=1}^\gamma Y_i} \right) \\
 & \leq  \E \left( e^{ - t X_1 \alpha} e^{ t Y_1 \gamma} \right ) \\
 &\leq \left( \E e^{ - t X_i} \E e^{t Y_1} \right)^{(1-w)\alpha + w \gamma} 
       \frac{ \left( \E e^{t Y_1} \right)^{(1-w)(\gamma - \alpha)} }
            { \left( \E e^{-t X_1} \right)^{w (\gamma - \alpha)} }
\end{align*}

We will show that when $t=1/2$ and $w=1/2$, the fraction term equals $1$ and the first term equals $\exp\left( - (1/2 \alpha + 1/2 \gamma) I \right)$. 

Note that 
\begin{align*}
\E e^{ -t X_1} &= \sum_l P_l e^{ - t \log \frac{P_l}{Q_l}} \\
   &= \sum_l P_l \left( \frac{Q_l}{P_l} \right)^t \\ 
   &= \sum_l \sqrt{P_l Q_l} \quad \trm{(if $t=1/2$)}
\end{align*}

\begin{align*}
\E e^{ t Y_1} &= \sum_l Q_l e^{ t \log \frac{P_l}{Q_l} } \\
   &= \sum_l Q_l \left( \frac{P_l}{Q_l} \right)^t \\
   &= \sum_l \sqrt{P_l Q_l} \quad \trm{if $t=1/2$}
\end{align*}

\begin{align*}
P( \sum_{i=1}^\gamma Y_i - \sum_{i=1}^\alpha X_i > 0) &\leq 
    \left( \sum_l \sqrt{P_lQ_l} \right)^{\alpha + \gamma} \\
   &\leq  \exp( - \frac{1}{2} I)^{\alpha + \gamma} \\
   &\leq \exp\left( - \frac{(\alpha+\gamma)}{2}  I \right)
\end{align*}
   

\end{proof}

\section{Weak Recovery in the General Setting}

\begin{proposition}
\label{prop:consistency_possible}
 If weak recovery (consistency) is possible under the oracle setting, then it is possible when $P,Q$ are unknown.
\end{proposition}

\begin{proof}
From proposition~\ref{prop:weak_recovery_oracle}, we know that weak recovery is possible iff $\frac{n I_{tot}}{K \log K} \rightarrow \infty$. 

From Lemma~\ref{lem:simplify_renyi}, we have
\begin{align*}
I_{tot} &= (1+o(1))\sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2 
\end{align*}


Since $I_{tot} = \omega( \frac{K \log K}{n} )$ by hypothesis, it must be that, for some $l$, $(\sqrt{P_l} - \sqrt{Q_l})^2 = \omega( \frac{K \log K}{n} )$. 

We choose such an $l$ and consider an estimator $\hat{\sigma}_l$ that uses only the information $\mathbf{1}_{A_{ij} = l}$. 

Since the Renyi-divergence $I_l$ of $Ber(P_l)$ and $Ber(Q_l)$ is
\[
I_l = (1+o(1)) \left( (\sqrt{P_l} - \sqrt{Q_l})^2 + (\sqrt{1-P_l} - \sqrt{1-Q_l})^2 \right)
\]
We have that $\frac{ n I_l}{K \log K} \rightarrow \infty$ and weak consistency is thus achievable with $\hat{\sigma}_l$. 

\end{proof}

Although weak consistency is achievable with the estimator $\hat{\sigma}_l$ that considers only $\mathbf{1}_{A_{ij} = l}$, the estimator converges at $\exp( - \frac{n I_l}{\beta K})$ and therefore does not converge at the same rate. It is easy to see that $I_l \leq I_{tot} \leq L I_l (1 - o(1))$ where the second inequality holds as equality under some cases. 

\section{Rate Optimal Recovery}

We propose the following algorithm to recover the clusters in the general setting. We proceed in two stages: first, we identify a color $l$ that can provide consistent recovery by itself (such $l$ must exist by proposition~\ref{prop:consistency_possible}), and second, we use $\sigma^l$ -- a clustering based on $l$ -- to estimate $\{ \hat{P}_l, \hat{Q}_l \}$ and then use the estimates to refine the clustering. The second stage closely follows the algorithm from \cite{gao2015achieving}. \\

\textbf{Stage 1. Identify a consistent color}
\begin{enumerate}
\item For each color $l$:
  \begin{enumerate}
   \item Perform spectral clustering on $\tilde{A}_{ij} \equiv \mathbf{1}(A_{ij} = l)$ to get $\sigma^l$. 
   \item Estimate $\hat{P}_l, \hat{Q}_l$ via counts from $\sigma^l$. 
   \item Estimate $\sqrt{I_l}$ via 
  $\sqrt{ \hat{I}_l } \equiv \frac{| \hat{P}_l - \hat{Q}_l |}{\sqrt{ \hat{P}_l \vee \hat{Q}_l}}$. 
   \end{enumerate}
\item Discard from $L$ all colors $l$ such that $\sqrt{\hat{I}_l} \leq \sqrt{ \frac{1}{n}}$.
\item Output $l^*$ for which $\sqrt{\hat{I}}$ is maximized
\end{enumerate}

 \textbf{Stage 2. Refine clusters}
\begin{enumerate}
\item For each node $u$:
  \begin{enumerate}
  \item Use previously computed $l^*$ to perform spectral clustering on $\mathcal{G}_{-u}$, get $\sigma_u$.
   \item Use $\sigma_u$ to estimate $\hat{P}_l, \hat{Q}_l$.
   \item Assign $\hat{\sigma}(u) = \arg\max_k \sum_{v \,:\, \sigma_u(v) = k} \sum_l 
                  \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) $. 
   \end{enumerate}
\item Run consensus. Output $\hat{\sigma}$. 

\begin{shaded}
\begin{proposition}
\label{prop:rate_optimal}
Suppose $K$ is fixed and that $n \frac{I_{tot}}{K} \rightarrow \infty$. Suppose that $P_l \asymp Q_l$ for all $l$. Then the above procedure has error rate satisfying
\[
\lim_{n \rightarrow \infty}  \sup_{\sigma_0, \{P_l, Q_l\}} P \left( l(\hat{\sigma}, \sigma_0) \geq \exp\left( - (1 - \eta) \frac{ n I_{tot}}{\beta K} \right) \right) = 0
\]
\end{proposition}
\end{shaded}

In particular, this shows that the threshold behavior (for the symmetric $K=2$ case and finite colors $L$) that \cite{jog2015information} demonstrates hold even if the $P_l, Q_l$'s are not known. 

The rest of the write-up constitute the proof of proposition~\ref{prop:rate_optimal}. The proof proceeds in three steps. In the first step (section~\ref{sec:estimation}, we prove a general result that controls the estimation quality $| \hat{P}_l - P_l |$ and $|\hat{Q}_l - Q_l|$ where $\hat{P}_l, \hat{Q}_l$ are constructed from a consistent clustering algorithm. In the second step (section~\ref{sec:initial_clustering}), we provide guarantee for the first stage of the our proposed algorithm. In the third step, we analyze the second stage of the proposed algorithm.

\end{enumerate}

\section{Estimation}
\label{sec:estimation}

Let $\sigma_0$ be the true clustering and $\hat{\sigma}$ be a clustering algorithm.


\begin{shaded}
\begin{proposition}
\label{prop:estimation_consistency}
Let $\sigma = \hat{\sigma}(G)$ be a clustering of the graph with error rate $\gamma$. That is, $d_H(\sigma, \sigma_0) = \gamma n$. Let $\Delta_l = | P_l - Q_l |$. Suppose 
\begin{align}
\gamma K \log K \rightarrow 0. 
\end{align}
Let $\hat{P}_l = \frac{\sum_{u,v \,:\, \sigma(u)=\sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {\sum_{u,v \,:\, \sigma(u) = \sigma(v)} 1}$ and
    $\hat{Q}_l = \frac{\sum_{u,v \,:\, \sigma(u) \neq \sigma(v)} \mathbf{1}(A_{uv} = l) }
                      {\sum_{u,v \,:\, \sigma(u) \neq \sigma(v)} 1}$ be the MLE of $P_l$ and $Q_l$ based on $\sigma$. 
Then, uniformly for all $l$ satisfying $ n \frac{\Delta_l^2}{(P_l \vee Q_l)} \rightarrow \infty$ (i.e., consistency), we have that
\begin{align}
| \hat{P}_l - P_l | &= \eta \Delta_l \\
| \hat{Q}_l - Q_l | &= \eta \Delta_l
\end{align}
with probability at least $1 - Ln^{-(3+\delta)}$ and for some $\eta = o(1)$ 
\end{proposition}
\end{shaded}

\begin{proposition}
\label{prop:strong_estimation_consistency}
Suppose
\begin{align}
\gamma K \log K &= o \left( \frac{\Delta_l}{P_l \vee Q_l} \right)  
   \label{eqn:rate_assumption}\\
\frac{n \Delta_l^3}{ (P_l \vee Q_l)^2 } &\rightarrow 0
    \label{eqn:gamma_assumption}
\end{align}
or, alternatively, suppose that
\begin{align}
\gamma K \log K &= o \left( \frac{\Delta_l}{P_l \vee Q_l} \right)^2 
    \label{eqn:stronger_gamma_assumption}
\end{align}

Then, we have that, uniformly for all $l$ satisfying the above assumptions,  
\begin{align}
| \hat{P}_l - P_l | &= \eta \frac{\Delta_l^2}{P_l \vee Q_l}  \\
| \hat{Q}_l - Q_l | &= \eta \frac{\Delta_l^2}{P_l \vee Q_l} 
\end{align}
with probability at least $1- Ln^{-(3+\delta)}$ and for some $\eta = o(1)$. 
\end{proposition}

Note that \ref{eqn:stronger_gamma_assumption} implies \ref{eqn:gamma_assumption} since
\[
\frac{ (\sqrt{P_l} - \sqrt{Q_l})^2}{\Delta_l} \leq 
 \frac{ \Delta_l}{P_l \vee Q_l} \leq 1
\]


Special attention has to be paid to $P_0, Q_0$. In this case, we will apply our assumptions on $1-P_0, 1-Q_0$ because $(\sqrt{P_0} - \sqrt{Q_0})^2 = o(\sqrt{1-P_0} + \sqrt{1-Q_0})^2$. 
Clearly, any analysis on the MLE's of $1-P_0, 1-Q_0$ automatically applies to the MLE's of $P_0, Q_0$. 

\subsection{Proof} 

We want to use a rough clustering $\sigma$ to estimate the parameters. $\sigma$ itself is a random variable dependent on the edge variables but we will analyze a fixed $\sigma$ and then take the union bound.

Let $\sigma_0$ be the true clustering and $\sigma$ be a rough one. 
Suppose that $d_H(\sigma, \sigma_0) = \gamma n$.

There are at most $\binom{n}{\gamma n}$ possible assignment $\sigma$'s that satisfy the distance constraint. 

\[
\log \binom{n}{\gamma n} \leq 
  \log \left( \frac{ n(n-1) ...(n-\gamma n+1) }{(\gamma n)!} \right) \leq
  \log \left( \frac{ n^{\gamma n} e^{\gamma n} }
     { (\gamma n)^{\gamma n} } \frac{1}{\sqrt{2\pi \gamma n}} \right)
 \leq C \gamma n \log \frac{1}{\gamma} 
\]



\subsubsection{Bias of $\hat{P}_l$}


Our estimator of $P_L$ is 
\[
\hat{P_l} = \frac{ \sum_{i,j \,:\, \sigma(i) = \sigma(j)} \mathbf{1}(A_{ij} = l) }{
                   \sum_{i,j \,:\, \sigma(i) = \sigma(j)} }
\]

Because $\sigma$ is an imperfect clustering, $\hat{P}_l$ will be biased. 
\[
\E \hat{P_l} = 
   \frac{ \sum_{i,j \,:\, \sigma(i) = \sigma(j)} \mathbf{1}(\sigma_0(i) = \sigma_0(j) ) P_l + 
               \mathbf{1}(\sigma_0(i) \neq \sigma_0(j)) Q_l }{
                   \sum_{i,j \,:\, \sigma(i) = \sigma(j)} }
\]

Thus, we have that, for any $\gamma$,
\begin{align}
P_l \wedge Q_l &\leq \E \hat{P}_l \leq P_l \vee Q_l \label{eqn:bias_simple_bound} \\
P_l \wedge Q_l &\leq \E \hat{Q}_l \leq P_l \vee Q_l \nonumber
\end{align}

We will assume that $P_l \geq Q_l$ first. Then, $\E \hat{P}_l \leq P_l$. 

To get a lower bound of the bias, observe that
\begin{align*}
\frac{\sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j)) \mathbf{1}(\sigma_0(i) \neq \sigma_0(j)) }{\sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j)) } &= 
   \frac{\sum_k \sum_{i,j \,:\, \sigma(i)=\sigma(j)=k} \mathbf{1}(\sigma_0(i) \neq \sigma_0(j))}{\sum_k \hat{n}_k (\hat{n}_k-1)} \\
  &\leq \frac{ \sum_k \gamma_k n \hat{n}_k }{\sum_k \hat{n}_k(\hat{n}_k - 1)} \quad
  \left(\trm{for $\sum_k \gamma_k = \gamma$}  \right) \\
  &\leq \frac{\max_k \hat{n}_k \sum_k \gamma_k n }{\min_k \hat{n}_k \sum_k (\hat{n}_k - 1)} \\
 &\leq \frac{\max_k \hat{n}_k }{\min_k \hat{n}_k} \gamma
\end{align*}

We define $\hat{n}_k = \sum_i \mathbf{1}(\sigma(i) = k)$. We can bound the ratio term as follows:
\begin{align*}
\frac{\max_k \hat{n}_k}{\min_k \hat{n}_k} &\leq
   \frac{ \frac{\beta n}{k} + \gamma n}{\frac{n}{\beta k} - \gamma n} \\
  &\leq \frac{\beta^2 + k \gamma \beta}{1 - k \gamma \beta} \\
 &\leq (\beta^2 + k\gamma \beta) (1 + o(1)) \quad \trm{
         (assuming $\gamma \rightarrow 0$)}
\end{align*}


Therefore, 
\[
P_l - \beta^2 \gamma (P_l - Q_l) (1 + o(1))  \leq \E \hat{P}_l \leq P_l
\]

In the event that $Q_l \geq P_l$, it is straightforward to check that
\[
P_l \leq \E \hat{P}_l \leq P_l + \beta^2 \gamma (Q_l - P_l)(1+o(1))
\]

In any case, we have the following bound:
\[
| \E \hat{P}_l - P_l |\leq \beta^2 \gamma \Delta_l (1 + o(1)) 
\]
where $\Delta_l  = |Q_l - P_l|$. 

Under the assumption that $\gamma K \log K \rightarrow 0$, we immediately have that 
\begin{align}
| \E \hat{P}_l - P_l | = o( \Delta_l ).
\end{align} 

Using our stronger assumption on $\gamma$ (equation~\ref{eqn:gamma_assumption}), we have that

\begin{align}
| \E \hat{P}_l - P_l | = o\left( \frac{\Delta_l^2}{P_l \vee Q_l} \right) 
\label{eqn:Pl_bias}
\end{align}


\subsubsection{Variance of $\hat{P}_l$}

Having handled the bias, we now bound the deviation.

Let $\tilde{A}_{ij} = \mathbf{1}(A_{ij} = l)$. Then, by Bernstein's inequality,
\[
P\left( \left| \sum_{i,j\,:\, \sigma(i) = \sigma(j)} (\tilde{A}_{ij} - \E \tilde{A}_{ij} ) \right|  > t 
 \right) \leq 2 \exp\left( 
    - \frac{t^2}{ 2 \sum_{i,j \, \sigma(i) = \sigma(j)} \E \tilde{A}_{ij}  + \frac{2}{3}t } 
\right)
\]

We first bound $\sum_{i,j \, \sigma(i) = \sigma(j)} \E \tilde{A}_{ij}$:
\begin{align*}
\sum_{i,j \, \sigma(i) = \sigma(j)} \E \tilde{A}_{ij} &=
  \sum_k \hat{n}_k (\hat{n}_k - 1) \E \hat{P}_l \\
 &\leq (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) \quad 
  \trm{(by Equation~\ref{eqn:bias_simple_bound})}
\end{align*}

Therefore,
\[
P\left( \left| \sum_{i,j\,:\, \sigma(i) = \sigma(j)} (\tilde{A}_{ij} - \E \tilde{A}_{ij} ) \right|  > t 
 \right) \leq 2 \exp\left( 
    - \frac{t^2}{ 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1)  + \frac{2}{3}t } 
\right)
\]

We want to bound the probability by $\exp( - C_1 \gamma n \log \frac{1}{\gamma} - (3+\delta) \log n )$. Assuming that $\gamma \geq \frac{1}{n}$, we have that $\gamma \log \gamma^{-1} \geq \frac{1}{n} \log n$. 

We choose $t$ such that
\begin{align*}
t^2 &= 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) \left( 
    C_1 \gamma n \log \frac{1}{\gamma} + (3+\delta) \log n \right) \vee 
   \left( C_1 \gamma n \log \frac{1}{\gamma} + (3 + \gamma) \log n \right)^2  \\
 &\leq 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) C_{\delta} \gamma n \log \frac{1}{\gamma} \vee \left( C_\delta \gamma n \log \frac{1}{\gamma} \right)^2  \\
 &\leq \left( \sqrt{  2 (P_l \vee Q_l)  \sum_k \hat{n}_k (\hat{n}_k - 1) C_\delta \gamma n \log \frac{1}{\gamma} } + C_\delta \gamma n \log \frac{1}{\gamma} \right)^2
\end{align*}

It easy to check that, regardless of which among $\{ 2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) ,\, C_\delta \gamma n \log \frac{1}{\gamma} \}$ is larger, the probability term is at most $\exp( - C_1 \gamma n \log \frac{1}{\gamma} - (3+\delta) \log n )$. Thus, with at most that probability:

\begin{align*}
| \hat{P}_l - \E \hat{P}_l | =
\frac{\sum_{i,j \, \sigma(i) = \sigma(j)} (\tilde{A}_{ij} - \E \tilde{A}_{ij} ) }{
  \sum_{i,j} \mathbf{1}( \sigma(i) = \sigma(j) ) } &> 
  \frac{t}{\sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j)) } \\
\end{align*}

Substituting in the previous bound we had of $t$:

\begin{align*}
 \frac{t}{\sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j)) } &\leq
   \frac{  \sqrt{  2 (P_l \vee Q_l) \sum_k \hat{n}_k (\hat{n}_k - 1) C_\delta \gamma n \log \frac{1}{\gamma} } + C_\delta \gamma n \log \frac{1}{\gamma} }
        { \sum_{i,j} \mathbf{1}(\sigma(i) = \sigma(j) ) }  \\
  &\leq \frac{\sqrt{2 (P_l \vee Q_l) C_\delta \gamma n \log \frac{1}{\gamma}}}
             {\sqrt{ \sum_k \hat{n}_k (\hat{n}_k - 1)}} + 
          \frac{C_\delta \gamma n \log \frac{1}{\gamma}}
             {\sum_k \hat{n}_k (\hat{n}_k - 1)} \\
 &\leq 2 \frac{ \sqrt{P_l\vee Q_l} \sqrt{ C_\delta \gamma \log \frac{1}{\gamma}} }
           {\sqrt{ \min_k \hat{n}_k}} + 
       \frac{C_\delta \gamma \log \frac{1}{\delta}}{\min_k \hat{n}_k} \\
 &\leq 2 \sqrt{ \frac{P_l \vee Q_l}{n} } \sqrt{C_\delta \beta k \gamma \log \frac{1}{\gamma}} + 
       \frac{C_\delta \beta k \gamma \log \frac{1}{\gamma}}{n} 
\end{align*}

Under the assumption that $\frac{ n \Delta_l^2}{P_l \vee Q_l} \rightarrow \infty$, we have that $\Delta_l \geq \sqrt{ \frac{P_l \vee Q_l}{n}}$. Under the same assumption, it must be that $P_l \vee Q_l \geq \frac{1}{n}$. If we further apply the assumption that $\gamma K \log K \rightarrow 0$, we have that, with probability $1 - L n^{-(3+\delta)}$, for some $\eta = o(1)$, uniformly for all colors $l$ satisfying $\frac{n \Delta_l^2}{P_l \vee Q_l} \rightarrow \infty$, 

\begin{align*}
| \hat{P}_l - \E \hat{P}_l | = \eta \Delta_l
\end{align*}

Now we apply the stronger assumptions of proposition~\ref{prop:strong_estimation_consistency} to get the stronger result. 

We note that $k \gamma \log \frac{1}{\gamma} = o\left( \frac{(\sqrt{P}_l - \sqrt{Q}_l)^2}{\Delta_l} \right)$ under assumption~\ref{eqn:gamma_assumption}. 

The first term is
\begin{align*}
& 2 \sqrt{ \frac{P_l \vee Q_l}{n} } \sqrt{ C_\delta \beta k \gamma \log \frac{1}{\gamma}} \\
&\leq 2 \sqrt{ \frac{P_l \vee Q_l}{n} } \sqrt{ \frac{\Delta_l}{ P_l \vee Q_l }} \quad
\trm{
(by assumption~\ref{eqn:gamma_assumption})
 } \\
&\leq 2 \sqrt{ \frac{\Delta_l}{n} } \\
& = o\left( \frac{\Delta_l^2}{P_l \vee Q_l} \right) \quad \trm{
   (assuming that $\frac{(P_l \vee Q_l)^2}{n \Delta_l^3} \rightarrow 0$, 
     (\ref{eqn:rate_assumption})) 
    } \\
&= o \left( \sqrt{P_l} - \sqrt{Q_l} \right)^2 
\end{align*}

We can repeat the above derivation using assumption~\ref{eqn:stronger_gamma_assumption} only instead of using both ~\ref{eqn:gamma_assumption} and \ref{eqn:rate_assumption}.
\begin{align*}
& 2 \sqrt{ \frac{P_l \vee Q_l}{n} } \sqrt{ C_\delta \beta k \gamma \log \frac{1}{\gamma}} \\
&\leq 2 \sqrt{ \frac{P_l \vee Q_l}{n} }  \frac{\Delta_l}{ P_l \vee Q_l } \quad
\trm{ 
(by assumption~\ref{eqn:stronger_gamma_assumption})
 } \\
&\leq 2 \frac{\Delta_l}{\sqrt{ n P_l \vee Q_l}} \\
&= o \left( \sqrt{P_l} - \sqrt{Q_l} \right)^2 \quad \trm{(since 
$\frac{1}{\sqrt{n}} = o\left( \frac{\Delta_l}{\sqrt{P_l \vee Q_l}} \right)$)}
\end{align*}


Likewise, we have, for the second term
\begin{align*}
  \frac{C_\delta \beta k \gamma \log \frac{1}{\gamma}}{n}  &\leq
 \frac{\Delta_l}{n (P_l \vee Q_l)} = o \left( \frac{\Delta_l^2}{P_l \vee Q_l} \right)
\end{align*}

Therefore, we have that
\begin{align}
| \hat{P}_l - \E \hat{P}_l | = o \left( \sqrt{P}_l - \sqrt{Q_l} \right)^2
\label{eqn:Pl_variance}
\end{align}
with probability at least $1 - L n^{-(3+\delta)}$ uniformly for all $\sigma$ (satisfying $d_H(\sigma, \sigma_0) = \gamma n$) and for all colors $l$ satisfying our two assumptions.

\newpage
\section{Controlling probability of misclassification}
\label{sec:misclassify}

Now that we have control over $\hat{P}_l$, we study the effect of plugging in these estimates into the refinement stage.

\begin{shaded}
In this section, we will first make the simplifying assumption that all colors $l$ satisfy assumptions~\ref{eqn:rate_assumption} and \ref{eqn:gamma_assumption} and furthermore, 
\begin{align}
P_l \asymp Q_l \label{eqn:Pl_Ql_equiv}
\end{align}
\end{shaded}



Let $\sigma_u$ be a clustering for all the nodes except $u$. Suppose that $d_H(\sigma_u, \sigma_0) = \gamma n$. 


We assign $u$ based on the criterion:
\[
\argmax_k \sum_{v\, \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
\]

Define $m_k' = \{ v \,:\, \sigma_u(v) = k,\, \sigma_0(v) = k \}$, $m_1' = \{ v \,:\, \sigma_u(v) = 1 ,\, \sigma_0(v) = 1\}$ as the points correctly clustered by $\sigma_u$. 

\begin{shaded}
\begin{proposition}
Suppose that $\sigma_u$ is a clustering of all nodes except for $u$ with error rate $\gamma$, that is, $d_H(\sigma_u, \sigma_0) = \gamma n$. Suppose that statements of proposition~\ref{prop:estimation_consistency} holds. 

Then, we have that, with probability at least $1 - \exp \left( - (1 - o(1)) \frac{n}{K} I^* \right)$, 
\[
\sigma_0(u) = \argmax_k \sum_{v\, \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
\]

\end{proposition}
\end{shaded}

\subsection{Proof}

Suppose without the loss of generality that $\sigma_0(u) = 1$.  We want to then control the probability that for some cluster $k$, 
\begin{align*}
\sum_{v\,:\, \sigma_u(v)=k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
&\geq 
 \sum_{v\,:\, \sigma_u(v)=1} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) 
  \quad \trm{(iff)} \\
\sum_{v \,:\, \sigma_u(v) = k} \bar{A}_{uv} - \sum_{v\,:\, \sigma_u(v) = 1} \bar{A}_{uv} 
&\geq 0 \quad \trm{(a.e. upper bounded by)} \\
\left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m'_k} \tilde{X}_i \right) - 
\left( \sum_{i=1}^{m_1'} \tilde{X}_i + \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) &\geq 0  \quad \trm{(iff)} \\
\exp( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i - 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) ) &\geq 1 
\end{align*}

where $\bar{A}_{uv} \equiv \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l)$ and $\tilde{X}_i = \log \frac{\hat{P}_l}{\hat{Q}_l}$ with probability $P_l$ and $\tilde{Y}_i = \log \frac{\hat{P}_l}{\hat{Q}_l}$ with probability $Q_l$. 



\begin{align*}
& P \left( \exp( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i- 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) ) \geq 1 \right) \\ 
&\leq \E \left( 
\exp( t \left( \sum_{i=1}^{m_k'} \tilde{Y}_i + \sum_{i=1}^{m_k - m_k'} \tilde{X}_i - 
     \sum_{i=1}^{m_1'}  \tilde{X}_i - \sum_{i=1}^{m_1 - m_1'} \tilde{Y}_i  \right) )
 \right) \\ 
&\leq \left( \E \exp( t \tilde{Y}_i ) \right)^{m_k'} \exp(t \tilde{X}_i )^{m_1 - m_1'}  
    \left( \E \exp( - t \tilde{X}_i) \right)^{m_1'} \exp( -t \tilde{Y}_i )^{m_k - m_k'} \\
&\leq \left( \sum_l e^{t \log \frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k'}  
      \left( \sum_l e^{t \log \frac{\hat{P}_l}{\hat{Q}_l}} P_l \right)^{m_k - m_k'} 
      \left( \sum_l e^{- t \log \frac{\hat{P}_l}{\hat{Q_l}} } P_l \right)^{m_1'}
     \left( \sum_l e^{-t \log \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{m_1 - m_1'}
\end{align*}

We will set $t = \frac{1}{2}$, in which case, we have:
\begin{align}
=& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k'}
 \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l \right)^{m_k - m_k'}
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l \right)^{m_1 - m_1'}
       \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1'} \nonumber \\
=&  \left( \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right)^{m_k - m_k'}
 \left( \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right)^{m_1 - m_1'}  
   \label{eqn:excess_error_term} \\
 & \left( \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{m_k' - (m_1 - m_1')} 
    \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1' - (m_k - m_k')}
   \label{eqn:Ihat_error_term} 
\end{align} 

We will bound term~\ref{eqn:excess_error_term} and \ref{eqn:Ihat_error_term} separately. 

\textbf{Bound for Term~\ref{eqn:excess_error_term}.} 

First, note that 
\begin{align*}
 \frac{P_l}{Q_l} - 1 &= \frac{P_l - Q_l}{Q_l}  
\end{align*}

We will show that $\frac{\hat{P}}{\hat{Q}}$ behaves similarly. 
\begin{align*}
\frac{\hat{P}_l}{\hat{Q}_l} - 1 &= 
     \frac{ \hat{P}_l - P_l + P_l }{ \hat{Q}_l - Q_l + Q_l} -1  \\
  &=  \frac{  \frac{\hat{P}_l - P_l}{Q_l} + \frac{P_l}{Q_l}}
       { \frac{\hat{Q}_l - Q_l}{Q_l} + 1} - 1 \\
 &= \left( \frac{P_l}{Q_l} + \frac{\hat{P}_l - P_l}{Q_l} \right)
    \left( 1 - \frac{\hat{Q}_l - Q_l}{Q_l} (1+o(1)) \right) -1  \\
 &= \frac{P_l-Q_l}{Q_l} + o\left( \frac{\Delta_l}{Q_l} \right) \quad \highlight{\trm{(assuming $|\hat{P}_l - P_l| = o (\Delta_l)$)}}
\end{align*}

Therefore, 
\begin{align*}
\sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} - 1 &= 
   \sqrt{ 1 + \frac{P_l - Q_l}{Q_l} + o\left( \frac{\Delta_l}{Q_l} \right)}  - 1\\
  & \left\{ \begin{array}{cc}
      \leq \frac{P_l - Q_l}{Q_l} (1+o(1)) & \trm{(if $P_l \geq Q_l$)} \\
      \geq \frac{P_l - Q_l}{Q_l} (1+o(1)) & \trm{(if $P_l < Q_l$)} 
     \end{array} \right.
\\
\end{align*}

Symmetry yields that 

\begin{align*}
\sqrt{ \frac{\hat{Q}_l}{\hat{P}_l} } - 1  &
   \left\{ \begin{array}{cc}
      \leq \frac{Q_l - P_l}{P_l} (1+o(1)) & \trm{(if $Q_l \geq P_l$)} \\
      \geq \frac{Q_l - P_l}{P_l} (1+o(1)) & \trm{(if $Q_l < P_l$)} 
     \end{array} \right. \\
\end{align*}

Now, we can bound term~\ref{eqn:excess_error_term}:
\begin{align*}
\left| 1 -  \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right|
 &= \frac{ \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} (P_l - Q_l) }
     { \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l } \\
&= \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} }(P_l - Q_l) \\
&= \sum_{l} \left( \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right) (P_l - Q_l)  \\
&\leq \sum_l \frac{\Delta^2_l}{Q_l}(1+o(1)) = O( I^* ) \quad 
   \highlight{\trm{(assuming that $P_l \asymp Q_l$)}}
\end{align*}

Identical analysis shows that
\[
\left| 1 - \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right| 
= O(I^*) 
\]

Therefore, term~\ref{eqn:excess_error_term} can be bounded as
\begin{align*}
&  \left( \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } P_l}
                {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l}  \right)^{m_k - m_k'}
 \left( \frac{ \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } Q_l}
             { \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l} } P_l} \right)^{m_1 - m_1'}  \\
&\leq \exp( O(I^*) (m_k - m_k' + m_1 - m_1') ) \\
&\leq \exp( O(I^*) \gamma n) \\
&\leq \exp\left( \frac{n}{k} o(I^*) \right) \quad \trm{(since $\gamma k \rightarrow 0$)}
\end{align*}

\textbf{Bound for Term~\ref{eqn:Ihat_error_term}.}

First, note that $ (m'_k - (m_1 - m_1')) - (m_1' - (m_k - m_k')) = m_k - m_1$.

Define 
$\hat{I} = - \log \left( \sum_l \frac{\hat{P}_l}{\hat{Q}_l} Q_l \right) \left( \sum_l \frac{\hat{Q}_l}{\hat{P}_l} P_l \right) $. 
With this definition, 

\begin{align*}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k' - (m_1 - m_1')} 
       \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1' - (m_k - m_k')} \\
&= \exp( - \hat{I} )^{\frac{(m'_k - (m_1 - m_1')) + (m_1' - (m_k - m_k'))}{2}}  \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} 
\end{align*}

We claim that the following three statements are true. 
\begin{enumerate}
\item[Claim 1] $m_1' - (m_k - m_k') \geq n_1 - 2 \gamma n$ and likewise for $m_k' - (m_1 - m_1')$
\item[Claim 2] $\hat{I} - I^* \geq - o(1) I^*$
\item[Claim 3] $\left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} = \exp(\frac{n}{k}  o(I^*)  $
\end{enumerate}

Let us first suppose that these statements are true and see that term~\ref{eqn:Ihat_error_term} can be bounded. 


\begin{align*}
& \exp( - \hat{I} )^{\frac{(m'_k - (m_1 - m_1')) + (m_1' - (m_k - m_k'))}{2}}  \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}}  \\
&\leq  \exp( - (I^* + (\hat{I} - I^*) )^{\frac{(m'_k - (m_1 - m_1')) + (m_1' - (m_k - m_k'))}{2}}  
 \left( \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l}
             {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l} \right)^{\frac{m_1 - m_k}{2}}  
  \\
&\leq \exp \left( - (1-o(1)) I^* (n_1 - \gamma n) \right) 
   \left( \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l}
             {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l} \right)^{\frac{m_1 - m_k}{2}}  
   \quad \trm{(by claim 1 and 2)}\\
&\leq \exp \left( - (1-o(1)) \frac{n}{\beta k} I^*  \right) 
   \quad \trm{(by claim 3)}
\end{align*}

The last inequality holds because $\gamma = o\left( \frac{1}{k \log k} \right)$. We will prove each of the three claims in the remainder of the proof.

\textbf{Claim 1:} This is straightforward. $\sigma_u$ has at most $\gamma n$ errors and therefore, $m_1' \geq n_1 - \gamma n$ and $m_k - m_k' \leq \gamma n$. 

\textbf{Claim 2:} We show that the estimation error of $\hat{P}_l, \hat{Q}_l$ does not make $\hat{I}$ too small.

\begin{align}
\hat{I} - I^* &= - \log \frac{ 
     \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)
     \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)}{ 
          \left( \sum_l \sqrt{P_l Q_l} \right)^2 } \label{eqn:Ihat_Istar}
\end{align}

Let us consider the numerator.
\begin{align*}
& \left( \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)
\left( \sum_l \sqrt{ \frac{\hat{Q}_l}{\hat{P}_l}} P_l \right) \\
&= \left( \sum_l \sqrt{ P_l Q_l} \sqrt{ \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l}} \right) 
     \left( \sum_l \sqrt{P_l Q_l} \sqrt{ \frac{P_l}{\hat{P}_l} \frac{\hat{Q}_l}{ Q_l}} \right) \\
&= \sum_l P_l Q_l + \sum_{l, l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} + 
   \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} \left( \sqrt{T} + \frac{1}{\sqrt{T}} - 2 \right) \\
&= \left( \sum_l \sqrt{P_l Q_l} \right)^2 + \sum_{l < l'} 
                  \sqrt{P_l Q_l P_{l'} Q_{l'}} \left( \sqrt{T} + \frac{1}{\sqrt{T}} - 2 \right) 
\end{align*}

where we define $T = \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} 
      \frac{P_{l'}}{\hat{P}_{l'}} \frac{\hat{Q}_{l'}}{Q_{l'}}  $. It will be later shown that $T \rightarrow 1$ and thus, continuing equation~\ref{eqn:Ihat_Istar},

\begin{align}
\hat{I} - I^* &= - \log \left( 1 + \frac{ \sum_{l<l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T} + \frac{1}{\sqrt{T}} - 2 \right)}
    { \left( \sum_l \sqrt{ P_l Q_l} \right)^2 }  \right) \nonumber \\
     &  \geq  - \log \left( 1 + 4 \sum_{l<l'} \sqrt{P_l Q_l P_{l'} Q_{l'}}  
    \left( \sqrt{T} + \frac{1}{\sqrt{T}} - 2 \right)  \right) 
  \quad \trm{(assuming that $\sum_l \sqrt{P_l Q_l} \geq 1/2$)} \nonumber \\
   & \geq - 4 \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T} + \frac{1}{\sqrt{T}} - 2 \right) \label{eqn:Ihat_Istar2}
\end{align}

We proceed by first bounding $|T - 1|$ and then taking the second order approximation of $\left( \sqrt{T} + \frac{1}{\sqrt{T}} - 2 \right)$ around $1$. 

\begin{align*}
|T - 1| &= \left| \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} 
      \frac{P_{l'}}{\hat{P}_{l'}} \frac{\hat{Q}_{l'}}{Q_{l'}} - 1 \right| \\
 &= \left| \left( 1 - \frac{P_l - \hat{P}_l}{P_l} \right)
    \left( 1 - \frac{\hat{Q}_l - Q_l}{\hat{Q}_l} \right)
   \left( 1- \frac{\hat{P}_{l'} - P_{l'}}{\hat{P}_{l'}}\right)
   \left( 1 -  \frac{Q_{l'}- \hat{Q}_{l'}}{Q_{l'}} \right) -1 \right| \\
&\leq \left( \frac{|P_l - \hat{P}_l|}{P_l} +  \frac{|\hat{Q}_l - Q_l|}{\hat{Q}_l}
           +   \frac{| \hat{P}_{l'} - P_{l'}|}{\hat{P}_{l'}} +
               \frac{| Q_{l'} - \hat{Q}_{l'} | }{Q_{l'}} \right) 
\end{align*}
\highlight{We use the assumption that $P_l \asymp Q_l$ and that $| \hat{P}_l - P_l | = \eta \Delta_l$ for some $\eta = o(1)$. }

\highlight{We will also assume that $\frac{1}{2} P_l \leq \hat{P}_l \leq 2 P_l$. }

Then, we have that
\begin{align*}
|T - 1| \leq \eta \left( 3 \frac{\Delta_l}{P_l \vee Q_l} + 3 \frac{\Delta_{l'}}{P_{l'} \vee Q_{l'}} \right) 
\end{align*}

The Taylor approximation of $\sqrt{T} + \frac{1}{\sqrt{T}} - 2$ around $T=1$ is:
\begin{align*}
\sqrt{T} + \frac{1}{\sqrt{T}} -2  &\leq 
  \frac{1}{4} (T - 1)^2 + O (T-1)^3 \\
 &\leq \frac{1}{4} \eta \left( 3 \frac{\Delta_l}{P_l \vee Q_l} + 3 \frac{\Delta_{l'}}{P_{l'} \vee Q_{l'}} \right)^2 
\end{align*}

Continuing on from equation~\ref{eqn:Ihat_Istar2}, we have that
\begin{align*}
\hat{I} - I^* &\geq - 4 \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \left( \sqrt{T} + \frac{1}{\sqrt{T}} - 2 \right) \\
  &\geq - \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
    \eta \left( 3 \frac{\Delta_l}{P_l \vee Q_l} + 3 \frac{\Delta_{l'} }{P_{l'} \vee Q_{l'}} 
     \right)^2 \\
 &\geq - \sum_{l < l'} \sqrt{P_l Q_l P_{l'} Q_{l'}} 
           36 \eta \left( \frac{\Delta_l}{ P_l \vee Q_l} \right)^2 
    \quad \trm{(assuming $\frac{\Delta_l}{P_l \vee Q_l} \geq 
                 \frac{\Delta_{l'}}{P_{l'} \vee Q_{l'}}$ )} \\
 &\geq - \sum_l \sum_{l'} 36 \eta \sqrt{P_{l'}Q_{l'}} \frac{\Delta_l^2}{P_l \vee Q_l} \\
 &\geq - \eta \left( \sum_l \frac{\Delta_l^2}{P_l \vee Q_l} \right)
         \left( \sum_{l'} 36 \sqrt{P_{l'}Q_{l'}} \right) \\
 &\geq - o(I^*)
\end{align*}

The last inequality follows because $\sum_{l'} \sqrt{P_{l'} Q_{l'}} \leq 1$. This proves claim 2.

\textbf{Claim 3.} 

\begin{align*}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} \\
&= \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{\frac{m_k - m_1}{2}} 
 \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{\frac{m_1 - m_k}{2}} 
   \left( \frac{\sum_l \sqrt{\hat{P}_l \hat{Q}_l}}{\sum_l \sqrt{\hat{P}_l \hat{Q}_l}} \right)^{\frac{m_1 - m_k}{2}} \\
&=  \left( 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} \hat{P_l} } \right)^{\frac{m_1 - m_k}{2}} 
\end{align*}

Assume that $m_k \geq m_1$. The reverse case can be analyzed in identical manners. Then,
\begin{align*}
&= \left( 1 + 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l)}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( 1+ \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} (\hat{P}_l - P_l)}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l } \right)^{\frac{m_k - m_1}{2}} 
   \\
\end{align*}

The term $\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l = \sum_l \sqrt{\hat{P}_l \hat{Q}_l} = \Theta( \sqrt{\hat{P}_0 \hat{Q}_0} ) =  \Theta(1)$ since $|\hat{P}_0 - P_0| = o( \sum_{l\neq 0} |P_l - Q_l|) \rightarrow 0$ and $P_0 \rightarrow 1$. Likewise, 
$\sum_l \sqrt{ \frac{\hat{Q}_l}{\hat{P}_l}} P_l 
  = \Theta \left( \sqrt{ \frac{\hat{Q}_0}{\hat{P}_0}} P_0 \right) =
  \Theta(1)$. 

To bound the numerator term, we first note that 
\[
\left| \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } - 1 \right| \leq \frac{\Delta_l}{Q_l}
\]

Therefore, we have that
\begin{align*}
\left| \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l) \right|  &= 
  \left|  \sum_l \left( \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} -1 \right) (Q_l - \hat{Q}_l) 
 \right| \\
 &\leq o\left( \sum_l \frac{\Delta^2_l}{Q_l} \right) \\
& \leq o( I^*) 
\end{align*}

\begin{align*}
& \left( 1 + 
   \frac{\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} (Q_l - \hat{Q}_l)}
        {\sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} \hat{Q}_l} 
     \right)^{\frac{m_k - m_1}{2}} 
   \left( 1+ \frac{\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} (\hat{P}_l - P_l)}
         {\sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l } \right)^{\frac{m_k - m_1}{2}} 
\\
&\leq \exp\left( (m_k - m_1) \log(1 + o(I^*) ) \right) \\
&\leq \exp \left( \frac{n}{k} o(I^*) \right) 
\end{align*}

This proves claim 3. 

Multiplying the bounds for term~\ref{eqn:Ihat_error_term} and \ref{eqn:excess_error_term} completes the proof. 


%%% DEPRECATED %%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%
%%%%%%%%%%

\begin{comment}
We will focus on the term $\left( \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)$ right now.

\begin{align*}
 \sum_l \sqrt{ \frac{\hat{P}_l}{\hat{Q}_l} } Q_l &= 
   \sum_l \sqrt{ P_l Q_l } \sqrt{ \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} } \\
 &= \sum_l \sqrt{P_l Q_l} \sqrt{ \left( 1 - \frac{P_l - \hat{P}_l}{P_l} \right)
                \left( 1 - \frac{\hat{Q}_l - Q_l}{ \hat{Q}_l } \right) } \\
   &\leq \sum_l \sqrt{P_l Q_l} ( 1 + \kappa_l ) \quad  \left( \trm{$\kappa_l \equiv 
               2 \left| \frac{\hat{P}_l - P_l}{P_l} \right| \vee \left| \frac{\hat{Q}_l - Q_l}{\hat{Q}_l} \right|$} \right)
\end{align*}

Let $\hat{I}= - 2 \log \sum_l \sqrt{ P_l Q_l } \sqrt{ \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} }$. 
\begin{align}
\hat{I} - I^* &= -2 \log \frac{  \sum_l \sqrt{ P_l Q_l } \sqrt{ \frac{\hat{P}_l}{P_l} \frac{Q_l}{\hat{Q}_l} } }{ \sum_l \sqrt{P_l Q_l} } \\
  &\geq -2 \log \left( 1 + \frac{\sum_l \sqrt{P_l Q_l} \kappa_l }{\sum_l \sqrt{P_l Q_l} }
    \right) \quad \trm{(by previous calculation)}\\
   &\geq - 2 \frac{\sum_l \sqrt{P_l Q_l} \kappa_l }{\sum_l \sqrt{P_l Q_l} } \\
   &\geq - 4 \sum_l \sqrt{P_l Q_l} \kappa_l \quad \trm{(assuming $\sqrt{P_0Q_0} \geq \frac{1}{2}$)} \label{eqn:Ihat_bound}
\end{align} 

Working through the $\sum_l \sqrt{ \frac{\hat{Q}_l}{\hat{P}_l} } P_l$ term, we get a similar bound:
\begin{align*}
\hat{I}' & \equiv \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \\
        &= \sum_l \sqrt{ P_l Q_l} \sqrt{ \frac{\hat{Q_l}}{Q_l} \frac{P_l}{\hat{P}_l}} 
\end{align*}

\begin{align}
\hat{I}' - I^* &= -2 \log \frac{ \sum_l \sqrt{ P_l Q_l} \sqrt{ \frac{\hat{Q_l}}{Q_l} \frac{P_l}{\hat{P}_l}} }
  { \sum_l \sqrt{P_l Q_l}}  \\
  &\geq -2 \log \left( 1 + \frac{ \sum_l \sqrt{P_l Q_l} \kappa_l' }{\sum_l \sqrt{P_l Q_l}} \right) \\
  &\geq -2 \frac{\sum_l \sqrt{P_l Q_l} \kappa'_l }{\sum_l \sqrt{P_l Q_l} } 
   \quad \left( \kappa \equiv 2 \left| \frac{\hat{P}_l - P_l}{\hat{P}_l} \right| \vee 
               \left| \frac{\hat{Q}_l - Q_l}{Q_l} \right| \right) \\
 &\geq -4 \sum_l \sqrt{P_l Q_l} \kappa'_l \quad 
   \left( \trm{ assuming $\sqrt{P_0 Q_0} > 1/2$} \right) \label{eqn:Ihatprime_bound}
\end{align}

Let us abuse notation for a second and let 
\[
\kappa_l = 2 \left| \frac{ \hat{P}_l - P_l}{P_l} \right| \vee 
    \left| \frac{ \hat{P}_l - P_l}{\hat{P}_l} \right| \vee
   \left| \frac{ \hat{Q}_l - Q_l}{\hat{Q}_l} \right| \vee 
\left| \frac{ \hat{Q}_l - Q_l}{Q_l} \right| 
\]

Assume for now that $\sqrt{P_lQ_l} \kappa_l = o \left( \sqrt{P_l} - \sqrt{Q_l} \right)^2$. (We will prove this later)

Then, returning to equations~\ref{eqn:Ihat_bound} and~\ref{eqn:Ihatprime_bound}, we have that
\begin{align*}
\hat{I} - I^* &\geq -4 \sum_l \sqrt{P_l Q_l} \kappa_l \\
   &\geq - o(1) \cdot \sum_l \left( \sqrt{P_l} - \sqrt{Q_l} \right)^2 \\
   &\geq - o(1) \cdot I^*
\end{align*}

The same conclusion holds in the same fashion for $\hat{I}'$. 

We left off bounding the error of misclassification at equation~\ref{eqn:Ihat_error_term}. Let us return to it. 

\begin{align*}
& \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l} } Q_l \right)^{m_k'} 
       \left( \sum_l \sqrt{\frac{\hat{Q_l}}{\hat{P}_l} } P_l \right)^{m_1' - (m_k - m_k')} \\
&\leq \left( \exp( - \frac{1}{2} \hat{I} ) \right)^{m_k'} 
    \left( \exp( - \frac{1}{2} \hat{I}') \right)^{m'_1 - (m_k - m_k')}  \\
&\leq \left( \exp( - \frac{1}{2} (1-o(1)) I^* )^{m_k'} \right) 
      \left( \exp( - \frac{1}{2} (1-o(1)) I^* )^{m'_1 - (m_k - m_k') } \right)  \\
&\leq  \exp\left( - \left( \frac{m_k' + m'_1 - (m_k - m_k')}{2}\right)  (1-o(1)) I^* \right) 
\end{align*}

Since we assume that $\sigma$ has error $\gamma n$, we can bound $m_k', m_1'$ and $m_k - m_k'$ as follows:
\[
m_k' \geq n_k - \gamma n \quad 
m_1' \geq n_1 - \gamma n \quad
m_k - m_k' \leq \gamma n
\]

\begin{align*}
m_k' + m_1' - (m_k - m_k') &\geq n_k + n_1 - 3 \gamma n \\
  &\geq 2 \frac{n}{\beta k} - 3 \gamma n \\
  &\geq 2 \frac{n}{\beta k} (1 - o(1)) \quad \trm{(assume $\gamma k \rightarrow 0$)}
\end{align*}

Therefore, we have that the error probability is bounded by 
\begin{shaded}
\begin{align*}
& P\left( \sum_{v,\, \sigma_u(v) = k} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) \geq \sum_{v,\, \sigma_u(v) = 1} \sum_l \log \frac{\hat{P}_l}{\hat{Q}_l} \mathbf{1}(A_{uv} = l) \right) \\
&\leq \left( \sum_l \sqrt{\frac{\hat{P}_l}{\hat{Q}_l}} Q_l \right)^{m'_k}
       \left( \sum_l \sqrt{\frac{\hat{Q}_l}{\hat{P}_l}} P_l \right)^{m_1' - (m_k - m_k')}\\
& \leq \exp\left( - (1- o(1)) \frac{n}{\beta k} I^* \right) 
\end{align*}
\end{shaded}

on event $\mathcal{E}_u$. Because event $\mathcal{E}_u$ essentially ``removes'' the randomness in $\hat{P}_l$ and $\hat{Q}_l$, the same bound holds uniformly for all clusters $k$. \\

We now have to prove that $\sqrt{P_l Q_l} \kappa_l = o\left( \sqrt{P_l} - \sqrt{Q_l} \right)^2$.

Under assumption~\ref{eqn:Pl_Ql_equiv}, this bound holds easily for 
$\left| \frac{\hat{P}_l - P_l}{P_l} \right|$ and 
$\left| \frac{\hat{Q}_l - Q_l}{Q_l} \right|$ by bounds~\ref{eqn:Pl_bias} and \ref{eqn:Pl_variance}.

We thus turn our attention to $\left| \frac{\hat{P}_l - P_l}{\hat{P}_l} \right|$. The analysis for $\left| \frac{\hat{Q}_l - Q_l}{\hat{Q}_l} \right|$ is identical. 

\[
\frac{\hat{P}_l - P_l}{\hat{P}_l} = \frac{\hat{P}_l - P_l}{P_l} \frac{P_l}{\hat{P}_l} 
\]

Since we knows that
\begin{align*}
1 - \frac{\hat{P}_l - P_l}{P_l} &\leq \frac{\hat{P}_l}{P_l} \leq 1 + 
  \frac{\hat{P}_l - P_l}{P_l} \\
\frac{1}{ 1 + \frac{\hat{P}_l - P_l}{P_l}} &\leq \frac{P_l}{\hat{P}_l} \leq 
   \frac{1}{1 - \frac{\hat{P}_l - P_l}{P_l}} 
\end{align*}

Since $\frac{\hat{P}_l - P_l}{P_l} \rightarrow 0$, we have that
\begin{align*}
1 - \frac{ \hat{P}_l - P_l}{P_l} (1+o(1)) \leq \frac{P_l}{\hat{P}_l} \leq
  1 + \frac{ \hat{P}_l - P_l}{P_1} (1-o(1))
\end{align*}

Putting this back, we have that
\begin{align*}
\left| \frac{\hat{P}_l - P_l}{\hat{P}_l} \right| \leq 
\left| \frac{\hat{P}_l - P_l}{P_l} \right| (1+o(1))
\end{align*}

This completes the analysis. 

\end{comment}
%%% End deprecated Section 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{comment}
\subsection{Further Discussion on Assumptions~\ref{eqn:gamma_assumption}, \ref{eqn:rate_assumption}}

We start with the assumption that consistency is possible under the oracle setting:
\[
\frac{n I^*}{K \log K} = \frac{n}{K\log K} \sum_{l\neq 0} (\sqrt{P_l} - \sqrt{Q_l})^2 \rightarrow 0
\]

Let $\tilde{L}$ be the set of colors $l$ such that
\[
\tilde{L} = \left\{ l \,:\, \frac{(\sqrt{P_l} - \sqrt{Q_l})^2 }{ I^* } \rightarrow c > 0 \right\}
\]

Since the set of colors $L$ is finite, $\tilde{L}$ must be non-empty. Furthermore, for any two colors $l, l' \in \tilde{L}$, we have that $(\sqrt{P}_l - \sqrt{Q}_l)^2 \asymp (\sqrt{P_{l'}} - \sqrt{Q_{l'}})^2$. 

\begin{shaded}
We will assume that the error $\gamma$ of the rough clustering satisfies
\begin{align}
\gamma = O\left( \exp\left( - \frac{n I_{\tilde{l}}}{K \log K}\right) \right) \quad
   \trm{for some $\tilde{l} \in \tilde{L}$} \label{eqn:gamma_rate}
\end{align}
\end{shaded}

\begin{proposition}
Suppose \ref{eqn:gamma_rate} is true. Let $l \in \tilde{L}$ be fixed, possibly different from the one in \ref{eqn:gamma_rate}, and suppose that for some positive integer $r$,
\begin{align}
\frac{ (K \log K)^{r+1} (P_l \vee Q_l)^{r+1}}{n^r \Delta_l^{2r+1}} \rightarrow 0
\label{eqn:weak_rate_assumption}
\end{align}
then, ~\ref{eqn:gamma_assumption} holds, that is,
\[
K \log K \gamma = o\left( \frac{(\sqrt{P_l} - \sqrt{Q_l})^2}{\Delta_l} \right)
\]

Also, if for some positive integer $r$, we have that
\begin{align}
\frac{ (K \log K)^{r+2} (P_l \vee Q_l)^{r+2}}{n^r \Delta_l^{2r+2}} \rightarrow 0
\label{eqn:weak_rate_assumption2}
\end{align},

then, ~\ref{eqn:stronger_gamma_assumption} holds, that is
\[
K \log K \gamma = o\left( \frac{(\sqrt{P_l} - \sqrt{Q_l})^2}{\Delta_l} \right)^2
\]

\end{proposition}

It is helpful to compare \ref{eqn:rate_assumption}, \ref{eqn:weak_rate_assumption}, \ref{eqn:weak_rate_assumption2}, and the basic consistency assumption of $\frac{(K \log K) ( P_l \vee Q_l)}{n \Delta_l^2} \rightarrow 0$. For clarity, we will formulate them as conditions on $n$.
\begin{align*}
n &= \omega \left( \frac{(K\log K) (P_l \vee Q_l)}{\Delta_l^2} \right) 
  \quad \trm{(consistency)} \\
n &= \omega \left( \frac{ (K\log K)^{\frac{r+1}{r}}  (P_l \vee Q_l)^{\frac{r+1}{r}}  }{\Delta_l^{\frac{2r+1}{r}} } \right) 
  \quad \trm{(\ref{eqn:weak_rate_assumption})} \\
n &= \omega \left( \frac{ (K\log K)^{\frac{r+2}{r}}  (P_l \vee Q_l)^{\frac{r+2}{r}}  }{\Delta_l^{\frac{2r+2}{r}} } \right) 
  \quad \trm{(\ref{eqn:weak_rate_assumption2})} \\
n &= \omega \left( \frac{ (P_l \vee Q_l)^2 }{\Delta_l^3 } \right)
  \quad \trm{(\ref{eqn:rate_assumption})}
\end{align*}

So we see that, for a fixed $K$,  \ref{eqn:rate_assumption} is \ref{eqn:weak_rate_assumption} with $r=1$ or \ref{eqn:weak_rate_assumption2} with $r=2$, and \ref{eqn:weak_rate_assumption}, \ref{eqn:weak_rate_assumption2} are not much stronger than the consistency condition since we can set $r$ to be large. 

\begin{proof}

Suppose that $\gamma = O\left( \exp \left( - \frac{n I_{\tilde{l}}}{K \log K} \right) \right)$. Let $r$ be such that either \ref{eqn:weak_rate_assumption} or \ref{eqn:weak_rate_assumption2} are satisfied. 

We have then that
\begin{align*}
\gamma &= o \left( \frac{K \log K}{n I_{\tilde{l}} } \right)^r \\
   &= o \left( \frac{K \log K}{n I_l} \right)^r \quad 
     \trm{(since $l \in \tilde{L}$)} \\
 &= o\left( \frac{ (K \log K)^r (P_l \vee Q_l)^r}{n^r \Delta_l^{2r}} \right) 
\end{align*}
where the last equation comes from the fact that $I_l = \Theta\left( \frac{\Delta_l^2}{P_l \vee Q_l} \right)$. 

Therefore, we have that
\begin{align*}
(K \log K) \gamma \frac{P_l \vee Q_l}{\Delta_l} &= 
   o \left( \frac{ (K \log K)^{r+1} (P_l \vee Q_l)^{r+1}}{ n^r \Delta_l^{2r + 1}} \right) \\
  &\rightarrow 0 \quad \trm{(using assumption~\ref{eqn:weak_rate_assumption})}
\end{align*}

Similarly, 
\begin{align*}
(K \log K)^2 \gamma \left(\frac{P_l \vee Q_l}{\Delta_l} \right)^2 &= 
   o \left( \frac{ (K \log K)^{r+2} (P_l \vee Q_l)^{r+2}}{ n^r \Delta_l^{2r + 2}} \right) \\
  &\rightarrow 0 \quad \trm{(under assumption~\ref{eqn:weak_rate_assumption2})}
\end{align*}

\end{proof}
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% DEPRECATED

\begin{comment}
\newpage
\subsection{Addressing the colors where $P_l \gg Q_l$}

The bounds on the estimation error ~\ref{eqn:Pl_bias} and ~\ref{eqn:Pl_variance} are still valid. However, we can no longer show that
\[
\sqrt{P_l Q_l} \kappa_l = o (\sqrt{P_l} - \sqrt{Q_l})^2
\]
This is because
\begin{align*}
\sqrt{P_l Q_l} \kappa_l &\geq \sqrt{P_l Q_l} \frac{ |\hat{Q}_l - Q_l| }{Q_l} \\
  &\geq \sqrt{ \frac{P_l}{Q_l} } | \hat{Q}_l - Q_l |
\end{align*}

Since $\frac{P_l}{Q_l} \rightarrow \infty$, there is little we can say about the above expression.

We will divide our analysis into two cases. First, we will assume that $Q_l \geq \frac{1}{n}$. Second, we will assume that $Q_l \leq \frac{1}{n}$.

\subsubsection{First Case}

We assume in this section that $Q_l \geq \frac{1}{n}$.

We will perform a better analysis on the estimation error.

\subsubsection{Second Case}

We assume that $Q_l \leq \frac{1}{n}$.

$I^*$ is almost unaffected if we replace $Q_l$ with $\frac{1}{n}$. 

\subsection{Handling Weak Colors}
\end{comment}

%%%%% END deprecation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Choosing the initial clustering}
\label{sec:initial_clustering}

Recall that the stage 1 of our algorithm is as follows. Let $\tau$ be an input parameter. 

\begin{enumerate}
\item For each color $l$:
  \begin{enumerate}
   \item Perform spectral clustering on $\tilde{A}_{ij} \equiv \mathbf{1}(A_{ij} = l)$ to get $\sigma^l$. 
   \item Estimate $\hat{P}_l, \hat{Q}_l$ via counts from $\sigma^l$. 
   \item Estimate $\sqrt{I_l}$ via 
  $\sqrt{ \hat{I}_l } \equiv \frac{| \hat{P}_l - \hat{Q}_l |}{\sqrt{ \hat{P}_l \vee \hat{Q}_l}}$. 
   \end{enumerate}
\item Discard from $L$ all colors $l$ such that $\sqrt{\hat{I}_l} \leq \tau \sqrt{ \frac{1}{n}}$.
\item Output $l^*$ for which $\sqrt{\hat{I}}$ is maximized
\end{enumerate}

Assuming that $ \frac{n I_{tot}}{K} \rightarrow \infty$, we want to say that the $l^*$ we output satisfies $\frac{n I_{l^*}}{K} \rightarrow \infty $ and that the $l$'s we discard satisfy $\limsup_{n \rightarrow \infty} n \frac{I_l}{K} < \infty$. It must be noted that the output of the initialization algorithm depends on $n$; that is, $l^*$ and the discarded colors $l$ depend on $n$. We will omit the dependence in our notation. 


Both of these claims follow from proposition~\ref{prop:initial_guarantee} below. The second claim follows directly from the second statement of proposition~\ref{prop:initial_guarantee}. To see that the first claim is also true, note that there must exist an $l$ such that $\frac{n I_l}{K}$ by proposition~\ref{prop:consistency_possible} and furthermore, by the first statement of proposition~\ref{prop:initial_guarantee}, $\hat{I}_{l} \asymp I_{l}$. 

\begin{proposition}
\label{prop:initial_guarantee}
Suppose color $l$ satisfies 

$$\Delta_l = \omega \left( \sqrt{ \frac{K (P_l \vee Q_l)}{n}} \right)$$. 

Let $\sigma^l$ be a spectral clustering of the graph based on $\tilde{A}_{ij} = \mathbf{1}(A_{ij}) = l)$ and let $\hat{P}_l, \hat{Q}_l$ be estimates of $P_l, Q_l$ constructed from $\sigma^l$. Then, with probability at least $1 - 2n^{-3+\delta}$ for some $\delta > 0$, there is a sequence $\eta \rightarrow 0$ such that

\[
\frac{ | P_l - Q_l |}{\sqrt{P_l \vee Q_l}} (\frac{1}{\sqrt{2}} - \eta) \leq \frac{ | \hat{P}_l - \hat{Q}_l| }{\sqrt{ \hat{P}_l \vee \hat{Q}_l }} \leq  
 \frac{ | P_l - Q_l | }{\sqrt{ P_l \vee Q_l}} (\sqrt{2} + \eta)
\]

On the other hand, supposing $K$ is fixed and consider a color $l$ such that 

$$\Delta_l = O \left( \sqrt{ \frac{P_l \vee Q_l}{n}} \right)$$ 

(which implies that $\sigma^l$ is inconsistent), then, with probability at least $1 - n^{-3+\delta}$, there exists $\eta \rightarrow 0$ such that 
\[
\frac{\hat{P}_l - \hat{Q}_l}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} \leq  C' \frac{1}{\sqrt{n}} (1 + \eta) 
\]
where $C' \equiv \sqrt{2} (\limsup_{n \rightarrow} \sqrt{n I_l} + 1)$ is a constant. 
\end{proposition}



\begin{proof}

Let color $l$ be fixed and let $\gamma$ be the error rate of the spectral clustering $\sigma^l$. 

From the proof of estimation error of $\hat{P}_l$ (\ref{prop:estimation_consistency})
\[
|\hat{P}_l - P_l| \leq \beta^2 \gamma \Delta_l (1+o(1)) + 2 \sqrt{\frac{P_l \vee Q_l}{n} } \sqrt{ C K \gamma \log \frac{1}{\gamma}} + \frac{K \gamma \log \frac{1}{\gamma}}{n}
\]
with probability at least $1 - n^{-(3+\delta)}$  and likewise for $\hat{Q}_l - Q_l$. 

First, suppose $\Delta = \omega \left( \sqrt{ \frac{ K(P_l \vee Q_l)}{n}} \right)$. Then, by Theorem~\ref{thm:spectral_rate}, we know that, with probability at least $1 - n^{-4}$, $\gamma K \log K \rightarrow 0$. We also assume without loss of generality that $P_l \geq Q_l$. 


In this case, $\sqrt{ \frac{P_l \vee Q_l}{n}} = o( \Delta_l )$. Also, $\Delta_l \geq \frac{1}{n}$ and so we have that $| \hat{P}_l - P_l | = o(\Delta_l)$. Since $\Delta_l \leq P_l$, we also have that $ \frac{1}{2} P_l \leq \hat{P}_l  \leq 2 P_l$. Likewise, we have that $|\hat{Q}_l - Q_l| = o(\Delta_l)$ and $\hat{Q}_l \leq 2 P_l$. 

\begin{align*}
\frac{ | \hat{P}_l - \hat{Q}_l |}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} &\geq 
          \frac{| P_l - Q_l | - o(\Delta_l)}{ \sqrt{2 P_l} } \\
  &\geq \frac{\Delta_l}{\sqrt{2 P_l} } (1 - o(1)) 
\end{align*}

\begin{align*}
\frac{ | \hat{P}_l - \hat{Q}_l |}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} &\leq 
          \frac{| P_l - Q_l | + o(\Delta_l)}{ \sqrt{\frac{1}{2} P_l} } \\
  &\leq \frac{\sqrt{2} \Delta_l}{\sqrt{P_l} } (1 + o(1)) 
\end{align*}

Now, we turn to the second statement of the proposition and suppose  $\Delta_l = O\left( 
   \sqrt{\frac{P_l \vee Q_l}{n} } \right)$. With no assumption on $\gamma$, we must carefully examine the proof of proposition~\ref{prop:estimation_consistency}.

From the proof, it is clear that the bias $|\E \hat{P}_l - P_l| \leq \Delta_l = O\left( 
\sqrt{ \frac{P_l \vee Q_l}{n}} \right)$ and that the variance satisfies 
\begin{align*}
|\hat{P}_l - \E \hat{P}_l | &\leq \frac{\sqrt{2 (P_l \vee Q_l) C_\delta \gamma n \log \frac{1}{\gamma}}}
             {\sqrt{ \sum_k \hat{n}_k (\hat{n}_k - 1)}} + 
          \frac{C_\delta \gamma n \log \frac{1}{\gamma}}
             {\sum_k \hat{n}_k (\hat{n}_k - 1)} \\
  &\leq \frac{\sqrt{2 (P_l \vee Q_l) C_\delta \gamma K \log \frac{1}{\gamma}}}
             {\sqrt{ \max_k \hat{n}_k}}  + 
          \frac{C_\delta \gamma K \log \frac{1}{\gamma}}
             {\max_k \hat{n}_k } \\
  &\leq \sqrt{ \frac{ P_l \vee Q_l}{n}} \sqrt{ C_\delta \gamma K^2 \log \frac{1}{\gamma}} + 
    \frac{C_\delta \gamma K^2 \log \frac{1}{\gamma}}{n} 
\end{align*}
 
Since $\frac{1}{n} \leq \sqrt{ \frac{P_l \vee Q_l}{n}}$ by our assumption that $P_l \vee Q_l = \omega \left(\frac{1}{n} \right)$, we have that

$$| \hat{P}_l - P_l | = O\left( \sqrt{\frac{P_l \vee Q_l}{n} } \right)$$ 

Since $\frac{ P_l \vee Q_l }{n} \rightarrow 0$, we also have that $\hat{P}_l \geq \frac{1}{2} P_l$. Therefore, 

\begin{align*}
\frac{ | \hat{P}_l - \hat{Q}_l |}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} &\leq 
          \frac{| P_l - Q_l | + O \left( \sqrt{ \frac{P_l \vee Q_l}{n}} \right)}{ \sqrt{\frac{1}{2} P_l} } \\
  &\leq \sqrt{2} \frac{|P_l - Q_l|}{\sqrt{P_l}} + \sqrt{2} \sqrt{ \frac{1}{n}} (1 + \eta) \\
  &\leq C' \sqrt{ \frac{1}{n} } (1 + \eta)
\end{align*}

\end{proof}

\begin{theorem}
Suppose $\frac{ n I_{tot}}{K} \rightarrow \infty$ and let $L'$ be the set of colors such that $\limsup_{n \rightarrow \infty} \sqrt{n I_l} < \infty$. \\

Suppose the initialization algorithm is set with $\tau = \max_{l \in L'} \sqrt{2} 
( \limsup_{n \rightarrow \infty} n I_l  + 2)$. Then, the following holds with probability at least $1 - 2Ln^{-(3+\delta)}$.

\begin{enumerate}
\item 
Let $l^*$ be the color outputted by the initialization algorithm. Then, for all large enough $n$, we have that $l^*$ satisfies 
\[
\frac{n I_{l^*} }{K} \rightarrow \infty
\]
\item
A color $l$ is discarded by the initialization algorithm iff $l \in L'$. 
\end{enumerate}

\end{theorem}

\begin{proof}
By taking a union bound, we reason that conclusions of Proposition~\ref{prop:initial_guarantee} hold uniformly for all $l$ with probability at least $1 - 2L n^{-(3+\delta)}$.

By proposition~\ref{prop:consistency_possible}, there must exists a color $l$ such that $\frac{n I_l}{K} \rightarrow \infty$. By proposition~\ref{prop:initial_guarantee}, with probability at least $1 - 2 L n^{-(3 + \delta)}$, 

\begin{align*}
\frac{ |P_{l^*} - Q_{l^*}|}{\sqrt{ P_{l^*} \vee Q_{l^*}}} &\geq 
\frac{1}{\sqrt{2} + \eta} \frac{|\hat{P}_{l^*} - \hat{Q}_{l^*} | }{\sqrt{ \hat{P}_{l^*} \vee \hat{Q}_{l^*} }} \\
& \geq
\frac{1}{\sqrt{2} + \eta} \frac{|\hat{P}_l - \hat{Q}_l | }{\sqrt{ \hat{P}_l \vee \hat{Q}_l}} \\
 &\geq \frac{|P_l - Q_l|}{\sqrt{P_l \vee Q_l}} \frac{1/\sqrt{2} - \eta}{\sqrt{2} + \eta}  \\
\end{align*}

where we let $n$ be large enough such that $|\eta| < 1/\sqrt{2}$. Therefore, we see that $ \frac{n I_{l^*} }{K} \rightarrow \infty$. 

Let us turn to the second claim of the theorem. If $l \in L'$, then it is clear by proposition~\ref{prop:initial_guarantee} that $l$ will be discarded.

If $l \notin L'$, then, by proposition~\ref{prop:initial_guarantee}, we have that
\begin{align*}
\frac{|\hat{P}_l - \hat{Q}_l|}{\sqrt{\hat{P}_l \vee \hat{Q}_l}} &\geq 
   (\frac{1}{\sqrt{2}} - \eta) \frac{ | P_l - Q_l|}{\sqrt{P_l \vee Q_l}} 
\end{align*}

For large enough $n$, $\frac{|P_l - Q_l|}{\sqrt{P_l \vee Q_l}} > \tau \sqrt{ \frac{1}{n}}$ and so $l$ would not be discarded. 

\end{proof}

\section{Continuous Distributions}

In this section, we suppose that the weights of within-cluster edges are drawn from a density $p$ and that of between-cluster edges are drawn from a density $q$.
\begin{align*}
A_{ij} &\sim p  \quad \trm{if $\sigma_0(i) = \sigma_0(j)$} \\
A_{ij} &\sim q  \quad \trm{if $\sigma_0(i) = \sigma_0(j)$}
\end{align*}

In this case, the continuous Renyi divergence is 
\[
I = -2 \log \int \sqrt{p(x)q(x)} dx 
\]

Under the oracle setting, the rate of recovery is similar to that of proposition~\ref{prop:weak_recovery_oracle}. 
\begin{proposition} 
\label{prop:weak_recovery_oracle_continuous}
(Oracle Setting Upper Bound for Continuous Distributions)
Assume $\frac{n I}{K \log K} \rightarrow \infty$. The maximum likelihood estimator $\hat{\sigma}$ in the oracle setting achieves:
\[
\sup_{\Theta(n, K, \beta, P, Q)} \E r(\hat{\sigma}, \sigma) \leq \left\{ 
    \begin{array}{cc} 
   \exp \left( - (1 + o(1)) \frac{nI}{2} \right ), \, & K=2, \\
   \exp \left( - (1 + o(1)) \frac{nI}{\beta K} \right ), \,& K\geq 3
  \end{array} \right. 
\]   
\end{proposition}
The proof is identical to that of proposition~\ref{prop:weak_recovery_oracle}, replacing sums with integrals where needed. 


\subsection{Rate Optimal Recovery}

Let us go to the general setting. Our goal is to show that under certain conditions, rate-optimal recovery is possible for continuous distributions.

\subsubsection{Assumptions}
\label{sec:continuous_assumptions}

\begin{enumerate}
\item[A1] $p(x), q(x)$ are supported on $[0,1]$, and $p(x), q(x) \geq c > 0$. 
\item[A2] $p(x) - q(x) = \gamma(x) \alpha$ where $|\gamma(x)| \leq M$ for some constant $M$. $\alpha \rightarrow 0$.
\item[A3] $|p'(x)|, |q'(x)|, |\gamma'(x)| \leq M'$ for some constant $M'$. 
\end{enumerate}

\subsubsection{Continuous and Discretized Renyi Divergence}

First, we show that, under the assumptions we have listed, the continuous Renyi divergence scales as $\alpha^2$. 

\begin{proposition}
\label{prop:continuous_renyi_order}
Let $I = -2 \log \int \sqrt{p(x)q(x)} dx$ be the continuous Renyi divergence.

Suppose assumptions A1 and A2 are satisfied with constants $M, c$. 

We have that, with $d = \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx$,
\[
I = d \alpha^2 ( 1 + \eta )
\]
where, for any $\alpha < \frac{c}{4M}$, $|\eta| \leq \frac{12 M}{c} \alpha$. 

\end{proposition}

\begin{proof}

First, denote $H = \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx$ as the continuous Hellinger distance and we will first show that $c_1 \alpha^2 \leq H \leq c_2 \alpha^2$ for some constants $c_1, c_2$.\\

\begin{align*}
&\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \\
=& \int ( \sqrt{p(x)} - \sqrt{p(x) - \gamma(x) \alpha} )^2 dx \\
=& \int p(x) \left( 1 - \sqrt{ 1 - \frac{\gamma(x) \alpha}{p(x)}} \right)^2 dx 
\end{align*}

Since $|\gamma(x)| \leq M$ and $p(x) \geq c$, we have that, for any $|\alpha| \leq \frac{c}{2M}$, $\left| \frac{\gamma(x)}{p(x)} \alpha \right| \leq \frac{1}{2}$. Thus, we can take Taylor series expansion of $f(z) = \sqrt{1 - z}$ around 0 and plug in $\frac{\gamma(x) \alpha}{p(x)}$. \\


Therefore, there exists some function $\xi(\alpha, x)$ satisfying $|\xi(\alpha, x)| \leq 
\frac{4M}{c} \alpha$ such that

\begin{align*}
=& \int p(x) \left( 1 - (1 - \frac{1}{2} \frac{\gamma(x) \alpha}{p(x)} (1 + \xi(\alpha, x)) ) \right)^2 dx \\
=& \int p(x) \left( \frac{1}{2} \frac{\gamma(x) \alpha}{p(x)} (1 + \xi(\alpha, x)) \right)^2 dx\\
=& \alpha^2 \int p(x) \left( \frac{1}{2} \frac{\gamma(x) }{p(x)} \right)^2 (1+\xi(\alpha, x))^2 dx \\
=& \alpha^2 \left( \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx \right) (1 + \eta) 
\end{align*}

where $\eta = \frac{ \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 ( 2 \xi(x, \alpha) + \xi(x, \alpha)^2 )  dx }
             { \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx}$. Since $| \xi(\alpha, x) | \leq \frac{4M}{c} \alpha \leq 1$, 
\begin{align*}
|\eta| & 
\leq \frac{1}{ \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx } 
  \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 \frac{12 M}{c} \alpha dx \\
&\leq \frac{12 M}{c} \alpha
\end{align*} 

Now, it remains to bound $I$ in terms of $H$. Since
\begin{align*}
I =& -2 \log \int \sqrt{p(x)q(x)} dx \\
 =& -2 \log \left( 1 - \frac{1}{2} \int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx \right) \\
 =& -2 \log (1 - \frac{1}{2} H) 
\end{align*}

If $\alpha \leq \frac{c}{4M}$, then $H \leq \frac{1}{16}$, thus, we have that

\[
(1 + \eta') H \geq I \geq H
\]

where $|\eta'| \leq \frac{1}{4} H \leq \frac{1}{4} \alpha^2 d (1 + \eta)$.  

This completes the proof with $d = \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx$. 

\end{proof}


Next, we define the \textbf{discretized Renyi divergence}. Let the interval $[0,1]$ be divided into $L$ equally spaced sub-intervals. Let $l$ index each of these sub-intervals and let $B = 1/L$ be the length of each sub-interval.

Let $P_l = \int_{\trm{Bin}_l} p(x) dx$ and $Q_l = \int_{\trm{Bin}_l} q(x)dx$. We define the \emph{discretized Renyi divergence} as $\tilde{I} = -2 \log \sum_{l=1}^L \sqrt{P_l Q_l} $ and $\tilde{H} = \sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2$. 


\begin{proposition}
\label{prop:discrete_renyi_order}
Let $L \geq 1$ be arbitrary and let $\tilde{I}_L = -2 \log \sum_{l=1}^L \sqrt{P_l Q_l}$ be the discretized Renyi divergence. 

Suppose assumptions A1 and A2 hold. 

Let $\gamma_l = \int_{\trm{Bin}_l} \gamma(x) dx$ and $d_L = \sum_l P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 $

Then, we have that,
\[
 \tilde{I}_L = d_L \alpha^2 ( 1 + \eta_L )
\]
where, for any $\alpha < \frac{c}{2M}$, $|\eta_L| \leq \frac{8M}{c} \alpha$.

\end{proposition}

\begin{proof}

Again, we first analyze the discretized Hellinger distance. The discretized Renyi divergence can be bounded in terms of the discretized Hellinger distance in the same fashion as the continuous case. 

\begin{align*}
\tilde{H} &= \sum_{l=1}^L (\sqrt{P_l} - \sqrt{Q_l})^2 \\
   &= \sum_{l=1}^L P_l \left( 1 - \sqrt{Q_l}{P_l} \right)^2 \\
 &= \sum_{l=1}^L P_l \left( 1 - \sqrt{1 - \frac{P_l - Q_l}{P_l}} \right)^2 
\end{align*}

We simplify the term $P_l - Q_l$.
\begin{align*}
P_l - Q_l &= \int_{\trm{Bin}_l} p(x) - q(x) dx \\
  &= \int_{\trm{Bin}_l} \gamma(x) \alpha dx \\
  &= \gamma_l \alpha 
\end{align*}

where $|\gamma_l| \leq M B$. 

\[
P_l = \int_{\trm{Bin}_l} p(x) dx \leq c B
\]

For $\alpha < \frac{c}{2M}$, we have that $\frac{\gamma_l}{P_l} \alpha < \frac{1}{2}$ and thus, there exists $\eta_l$ satisfying $|\eta_l| \leq \frac{4M}{c} \alpha$ such that

\begin{align*}
\tilde{H} &= \sum_{l=1}^L P_l \left( 1 - \sqrt{ 1 - \frac{\gamma_l \alpha}{P_l}} \right)^2
\\
&= \sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l \alpha}{P_l} (1 + \eta_l) \right)^2 \\
&= \alpha^2 \sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 (1 + \eta_l)^2 \\
&= \alpha^2 \left( \sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 \right) ( 1 + \eta_L) 
\end{align*}

where $\eta_L = \frac{ 
  \sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 (2 \eta_l + \eta_l^2)}
{\sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2} 
$. And since $|\eta_l| \leq \alpha \frac{4M}{c} < 1$, we have that

\begin{align*}
|\eta_L| \leq \frac{12 M}{c} \alpha
\end{align*}

The claim thus follows. 

\end{proof}


Next, we will show that $\lim_{L \rightarrow \infty} d_L = d$. 

\begin{proposition}
\label{prop:convergence_discrete_continuous_renyi}

Let $d = \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx$ and $d_L = \sum_l P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2$. 

Suppose assumptions A1, A3 hold. Then, we have that
\[
\lim_{L \rightarrow \infty} d_L = d
\]

\end{proposition}


Note that $B \rightarrow 0$ is equivalent to $L \rightarrow \infty$ since $B = 1/L$. 

\begin{proof}

Let $\trm{Bin}_l = [a_l, b_l]$. 
\begin{align*}
P_l &= \int_{\trm{Bin}_l} p(x) dx \\
  &= \int_{a_l}^{b_l} p(x) dx \\
 &= \int_{a_l}^{b_l} p(a_l) + p'(c_x) (x - a_l) dx \quad \trm{for some $c_x \in [a_l, b_l]$}\\
 &= B p(a_l) + B^2 \xi_l \quad \trm{where $|\xi_l| \leq M'/2$}
\end{align*}

Likewise, we have that $\gamma_l = B \gamma(a_l) + B^2 \xi'_l$ for some $\xi'_l$. 

\begin{align*}
d_L &= \sum_{l=1}^L P_l \left( \frac{1}{2} \frac{\gamma_l}{P_l} \right)^2 \\
  &= \sum_{l=1}^L B \left(
     (p(a_l) + B \xi_l) \left( \frac{1}{2} \frac{\gamma(a_l) + B \xi'_l}{p(a_l) + B \xi_l} \right)^2 \right) 
\end{align*}

Since $p(a_l) \geq c > 0$ is bounded away from 0 and $|\xi_l|, |\xi'_l| \leq M'$ is bounded away from $\infty$, we have that

\begin{align*}
 &= \sum_{l=1}^L B p(a_l) \left( \frac{1}{2} \frac{\gamma(a_l)}{p(a_l)} \right)^2 (1 + O(B))
\end{align*}

Thus, nothing that $\sum_{l=1}^L B = 1$, we have that

\begin{align*}
\lim_{B \rightarrow 0} d_L &= \lim_{B \rightarrow 0} \sum_{l=1}^L B p(a_l) 
 \left( \frac{1}{2} \frac{\gamma(a_l)}{p(a_l)} \right)^2 \\
 &= \int p(x) \left( \frac{1}{2} \frac{\gamma(x)}{p(x)} \right)^2 dx
\end{align*} 
Since $ \frac{\gamma^2(x)}{p(x)}$ is Riemann integrable. 
 
\end{proof}


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\begin{comment}
Next, we will show that $\lim_{B \rightarrow 0} \tilde{I} = I$. 
\begin{proposition}
\label{prop:convergence_discrete_continuous_renyi}
Let $P_l, Q_l, \tilde{I}$ be defined as above and suppose assumption A1 and A3 are satisfied with constants $M', c$. 

Then,
\[
\lim_{B \rightarrow 0} \tilde{I} = I
\]
\end{proposition}

Note that $B \rightarrow 0$ is equivalent to $L \rightarrow \infty$ since $B = 1/L$. 

\begin{proof}

Let $\trm{Bin}_l = [a_l, b_l]$. 
\begin{align*}
P_l &= \int_{\trm{Bin}_l} p(x) dx \\
  &= \int_{a_l}^{b_l} p(x) dx \\
 &= \int_{a_l}^{b_l} p(a_l) + p'(c_x) (x - a_l) dx \quad \trm{for $c_x \in [a_l, b_l]$}\\
 &= B p(a_l) + B^2 \xi_l \quad \trm{where $|\xi_l| \leq M'/2$}
\end{align*}

Likewise, we have that $Q_l = B q(a_l) + B^2 \xi'_l$ for some $\xi'_l$. 

\begin{align*}
\tilde{I} &= -2 \log \sum_{l=1}^L \sqrt{ P_l Q_l} \\
 &= -2 \log \sum_{l=1}^L B \sqrt{ (p(a_l) + B\xi_l)(q(a_l) + B \xi'_l) } \\
 &= -2 \log \sum_{l=1}^L \left( B \sqrt{ p(a_l) q(a_l) } + \left(B^2 \frac{\xi_l }{p(a_l)} 
   + B^2 \frac{\xi'_l}{q(a_l)} \right)(1+\eta_l) \right)
\end{align*}
where $\eta_l \rightarrow 0$ as $B \rightarrow 0$. $p(a_l), q(a_l) \geq c$ and $|\xi_l|, |\xi'_l| \leq M'$. Let $c_l = \left( \frac{\xi_l}{p(a_l)} + \frac{\xi'_l}{q(a_l)} \right)$ and it is clear that $|c_l| \leq \frac{M}{c}$. 

Thus, nothing that $\sum_{l=1}^L B = 1$, we have that

\begin{align*}
\lim_{B \rightarrow 0} \tilde{I} &= \lim_{B \rightarrow 0} -2 \log 
  \left( \sum_{l=1}^L B \sqrt{p(a_l) q(a_l)} + \sum_{l=1}^L B^2 c_l (1 + \eta_l) \right) \\
 &= -2 \log \left( \int \sqrt{p(x) q(x)} dx \right) 
\end{align*} 
\end{proof}
\end{comment}
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%

These propositions together imply the following:
\begin{shaded}
\begin{theorem}
\label{thm:relative_convergence_discrete_continuous_renyi}
Suppose assumptions A1-A3 are satisfied. Let $\tilde{I}_L$ be the discretized Renyi divergence discretized at level $L$.

Let $n \rightarrow \infty$, then, we have that, for \textbf{any sequences} $L_n \rightarrow \infty, \alpha_n \rightarrow 0$,

\[
\lim_{n \rightarrow \infty} \left| \frac{\tilde{I}_L}{I} - 1 \right| \rightarrow 0
\]

\end{theorem}
\end{shaded}

\begin{proof}


By proposition~\ref{prop:continuous_renyi_order} and proposition~\ref{prop:discrete_renyi_order}, we have that, for all $\alpha < \frac{c}{4M}$,

\begin{align*}
| \tilde{I}_L - I | 
 & \leq \left| d_L \alpha^2 ( 1 + \eta_L) - d \alpha^2 ( 1 + \eta) \right| \\
  &\leq d\alpha^2 \left| \frac{d_L}{d} (1 + \eta_L) - (1+\eta) \right|  \\
 &\leq I \left| \frac{d_L}{d} \frac{(1 + \eta_L)}{(1+\eta)} - 1 \right| \\
& \Rightarrow \\
\left| \frac{\tilde{I}_L}{I} - 1 \right| &\leq 
\left | \frac{d_L}{d} \frac{(1+\eta_L)}{1 + \eta} - 1 \right|
\end{align*}

where $|\eta|, |\eta_L| \leq \frac{12M}{c} \alpha$ for all $L$ and $d_L, d$ do not depend on $\alpha$.

It is clear then that $\lim_{\alpha \rightarrow 0} \left| \frac{\tilde{I}_L}{I} - 1 \right| = \left| \frac{d_L}{d} - 1 \right|$ and that $\lim_{L \rightarrow \infty} \lim_{\alpha \rightarrow 0} 
\left| \frac{\tilde{I}_L}{I} - 1 \right| = 0$.  
We need to show additionally that the the convergence is uniform, that is, 
$\lim_{\alpha \rightarrow 0} \sup_L 
\left| \left| \frac{\tilde{I}_L}{I} - 1 \right| - \left|  \frac{d_L}{d} - 1  \right| \right| = 0$.

This is true since $\lim_{\alpha \rightarrow 0} \sup_L \eta_L = 0$ by proposition~\ref{prop:discrete_renyi_order}. The claim follows immediately.
\end{proof}

\subsubsection{Recovery Procedure}

\begin{enumerate}
\item For each edge $(i,j)$, set $\tilde{A}_{ij} = \mathbf{I}(A_{ij} < \tau)$ for some $\tau \in (0, 1)$. Use the $\tilde{A}_{ij}$ labels for the initial rough clustering.
\item Bin the interval $[0,1]$ into $L$ bins and estimate $P_l, Q_l$.
\item Refine clustering with MLE based on $\hat{P}_l, \hat{Q}_l$.
\end{enumerate}


The initial clustering is consistent if we assume that $\left| \int_0^{\tau} \gamma(x) dx \right| = c_1 > 0$. Let $\bar{P} = \int_0^{\tau} p(x) dx, \bar{Q} = \int_0^{\tau} q(x) dx$, $\bar{P} - \bar{Q} = c_1 \alpha$. Furthermore, $\bar{P}, \bar{Q} \geq c$. Thus, 
\[
\bar{I} = -2 \log \left( \sqrt{\bar{P} \bar{Q}} + \sqrt{(1-\bar{P})(1-\bar{Q})} \right) = \Theta( \alpha^2)
\]

The discretization step is fine so long as $L \rightarrow \infty$. 

\subsection{Generalization}

The assumptions listed in section~\ref{sec:continuous_assumptions} can be generalized in several ways. In the first way, $\gamma$ can be a function of $\alpha$ under weak assumptions.
\begin{enumerate}
\item[B1] $p(x), q(x)$ are supported on $[0,1]$, and $p(x), q(x) \geq c > 0$. 
\item[B2] $p(x) - q(x) = \gamma(x, \alpha) \alpha$ where $|\gamma(x, \alpha)| \leq M$ for some constant $M$. $\alpha \rightarrow 0$.
\item[B3] For all $\alpha$, $0 < c_1 \leq \int \frac{\gamma(x, \alpha)^2}{p(x)} dx \leq c_2 < \infty$. 
\item[B4] $|p'(x)|, |q'(x)|, |\partial_x \gamma(x,\alpha)| \leq M'$ for some constant $M'$. 
\end{enumerate}

Let us see a specific example in which these generalizations are relevant. Suppose that $f(x)$ is a density supported on $[0, 1-\alpha]$ and $g(x)$ is a density, bounded away from 0, and supported on $[0,1]$. We define $p(x), q(x)$ as mixtures.
\begin{align*}
p(x) &= \lambda f(x) + (1-\lambda) g(x)\\
q(x) &= \lambda f(x - \alpha) + (1-\lambda) g(x)
\end{align*}

Suppose that $|f'(x)|, |g'(x)| \leq M$, then, we have that
\begin{align*}
p(x) - q(x) &= \lambda (f(x) - f(x-\alpha)) \\
  &= \lambda f'(c_{x, \alpha}) \alpha \quad \trm{for some $c_{x, \alpha}$ in between $x, x-\alpha$}
\end{align*}

Since $|f'(c_{x, \alpha} )| \leq M$, condition $B2$ is satisfied. Condition $B3$ amounts to requiring that $\int \frac{(f'(c_{x, \alpha}))^2}{p(x)} dx $ be bounded away from 0 and $\infty$ for all $\alpha$. 


\newpage
\section{Technical Lemmas}

\begin{lemma}
\label{lem:simplify_renyi}
Let $P = \{ P_l \}_{l = 0,..., \infty}$ and $Q = \{ Q_l \}_{l=0,...,\infty}$ be two discrete distributions and suppose $P_0, Q_0 \rightarrow 1$. Let $I = - 2 \log \sum_l \sqrt{ P_l Q_l}$.

Then, we have that $I \rightarrow 0$ and 
\[
I = (1 + o(1)) \sum_{l = 1}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 
\]

\end{lemma}

\begin{proof}
First, it is clear that if $P_0, Q_0 \rightarrow 1$, then 
\begin{align*}
\sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 &= 
   (\sqrt{P_0} - \sqrt{Q_0})^2 + \sum_{l=1}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 \\
 &= (\sqrt{P_0} - \sqrt{Q_0})^2 + \sum_{l=1}^\infty P_l + \sum_{l=1}^\infty Q_l - 
  2 \sum_{l=1}^\infty \sqrt{P_l Q_l} \\
 &\leq  (\sqrt{P_0} - \sqrt{Q_0})^2 + \sum_{l=1}^\infty P_l + \sum_{l=1}^\infty Q_l \\
\end{align*}

Therefore, $\lim_{n\rightarrow \infty} \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 = 0$. 

\begin{align*}
I &= -2 \log \sum_{l=0}^\infty \sqrt{P_l Q_l} \\
  &= -2 \log \left( 1 - \frac{1}{2} \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 \right) \\ 
  &= (1 + o(1)) \sum_{l=0}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 \quad \trm{(since the sum tends to 0)}
\end{align*}

We will show that $(\sqrt{P_0} - \sqrt{Q_0})^2 = o \left( \sum_{l=1}^\infty (\sqrt{P_l} - \sqrt{Q_l} )^2 \right)$ and the result follows immediately.

Let $P' = 1 - P_0$ and $Q' = 1 - Q_0$. 
\begin{align*}
(\sqrt{P_0} - \sqrt{Q_0})^2 &= (\sqrt{1-P'} + \sqrt{1-Q'})^2 \\
  &= (1-P') \left( 1 - \sqrt{ \frac{1-Q'}{1-P'}} \right)^2 \\
 &= (1-P') \left( 1 - \sqrt{ 1 - \frac{Q' - P'}{1 - P'} } \right)^2 \\
 &\leq (1-P') \left( 1 - (1 - \frac{1}{2} \left( \frac{Q' - P'}{1 - P'} \right) (1+o(1)) ) \right)^2 \\
 &\leq (1 - P') \left( \frac{1}{2} \left( \frac{Q'-P'}{1-P'} \right) (1+o(1)) \right)^2 \\
 &\leq \frac{1}{4} \left( \frac{Q' - P'}{1 - P'} \right)^2 (1 + o(1)) \leq \frac{1}{4} ( Q' - P')^2 (1 + o(1)) 
\end{align*}

\begin{align*}
\sum_{l=1}^\infty (\sqrt{P_l}  - \sqrt{Q_l})^2 &= \sum_{l=1}^\infty P_l + Q_l - 2 \sqrt{P_lQ_l} \\
 &\geq P' + Q' - 2 \sqrt{ \left( \sum_{l=1}^\infty P_l \right) \left( \sum_{l=1}^\infty Q_l \right) } \\
 &= P' + Q' - 2\sqrt{P' Q'} \\
 &= (\sqrt{P'} - \sqrt{Q'})^2 \\
 &= P' \left( 1 - \sqrt{ \frac{Q'}{P'} } \right)^2 \\ 
 &= P' \left( 1 - \sqrt{ 1 - \frac{P' - Q'}{P'} } \right)^2 \\
 &\geq P' \left( 1 - ( 1 - \frac{1}{2} \frac{P' - Q'}{P'} (1 + o(1)) ) \right)^2 \\
 &\geq P' \left( \frac{1}{2} \frac{P' - Q'}{P'} (1 + o(1)) \right)^2 \\
 &\geq \frac{1}{4} \left( \frac{(P' - Q')^2}{P'} \right) (1 + o(1)) 
\end{align*}

Thus, we have shown that 
\begin{align*}
(\sqrt{P_0} - \sqrt{Q_0})^2& \leq \frac{1}{4} (Q' - P')^2 (1 + o(1)) \\
\sum_{l=1}^\infty (\sqrt{P_l} - \sqrt{Q_l})^2 & \geq \frac{1}{4} \frac{(P' - Q')^2}{P'} (1 + o(1))
\end{align*}

Since $P' \rightarrow 0$, the proof is complete.

\end{proof}

\section{Reference Results}

\subsection{Existing Results from Literature}

Let $\Theta_0(n, k, p, q, \beta)$ be the parameter space of homogeneous stochastic block model with $p$ as the within-cluster probability and $q$ as the between-cluster probability. The following theorem follows from Theorem 3 and Proposition 1 of \cite{gao2015achieving}. 

\begin{theorem}
\label{thm:spectral_rate}
Assume $p \leq C_1 q$ and that $p, q = \Omega( \frac{1}{n} )$ and suppose that $n \geq 2 \beta k$. Suppose there is some $c \in (0,1)$ such that 

$$ \frac{k^3 p}{(p-q)^2 n^2} \leq c$$. 

Suppose we apply Unnormalized-Spectral-Clustering with trim constant $\tau = C_2 \bar{d}$ and a sufficiently small post-processing constant $\mu > 0$. Then, for any constant $C'$, there exists some $C > 0$ dependent only on $C', C_1, C_2, \mu$ such that

\[
l(\hat{\sigma}, \sigma_0) \leq C \frac{\beta^2 k^2 p}{(p-q)^2 n }
\]

with probability at least $1 - n^{-C'}$. 

\end{theorem}

We note that $\frac{(p-q)^2}{p} = I$. Restated, this theorem says that if $p \asymp q$ and if $ \frac{k}{nI} \rightarrow 0$, then, the error rate $\gamma$ of spectral clustering goes to zero with probability $1 - n^{-C'}$ for any constant $C' > 0$. 

\bibliographystyle{plain}
\bibliography{note}

\end{document}
